[{"content":"1. Basics RLHF: Reinforcement Learning from Human Feedback SFT: Supervised Fine-Tuning RL trains neural networks through trial and error. When finetuning a language model with RLHF, the model produces some text then receives a score/reward from a human annotator that captures the quality of that text. Then, we use RL to finetune the language model to generate outputs with high scores.\nIn this case, we cannot apply a loss function that trains the language model to maximize human preferences with supervised learning. This is because there’s no easy way to explain the score human give or connect it mathematically to the output of the neural network. In other words, we cannot backpropagate a loss applied to this score through the rest of the neural network. This would require that we are able to differentiate (i.e., compute the gradient of) the system that generates the score, which is a human that subjectively evaluates the generated text.\nThe agent acts and receives rewards (and new states) from the environment. Problems that are solved via RL tend to be structured in a similar format. Namely, we have an agent that is interacting with an environment; see the figure above. The agent has a state in the environment and produces actions, which can modify the current state, as output. As the agent interacts with the environment, it can receive both positive and negative rewards for its actions. The agent’s goal is to maximize the rewards that it receives, but there is not a reward associated with every action taken by the agent. Rather, rewards may have a long horizon, meaning that it takes several correct, consecutive actions to generate any positive reward.\n2. Markov Decision Process (MDP) 2.1. Concepts and Definitions Markov Decision Process (MDP) is a way to formulate the system described above more formally and mathematically. Within an MDP, we have states, actions, rewards, transitions, and a policy, as shown in the equation below:\n$$ \\begin{cases} s \\in S \u0026 \\text{State} \\\\ a \\in A \u0026 \\text{Action} \\\\ r_s \\in \\mathbb{R} \u0026 \\text{Reward} \\\\ \\pi(a|s) \u0026 \\text{Policy} \\\\ T(s_{t+1}|s_{t},a_{t}) \u0026 \\text{Transition function} \\\\ \\end{cases} $$States and actions have discrete values, while rewards are real numbers.\nIn an MDP, we define two types of functions: transition and policy functions. The policy takes a state as input, then outputs a probability distribution over possible actions.\nNotably, the action that is chosen only depends on the current state and not any state history that precedes it. This is a key property of an MDP, which make the assumption that the next action only depends upon the current state.\nGiven this output, we can make a decision for the action to be taken from a current state, and the transition is then a function that outputs the next state based upon the prior state and chosen action. Using these components, the agent can interact with the environment in an iterative fashion, as the figure shown below.\nStructure of an MDP. The policy describes how the agent chooses its next action given the current state. The agent follows this strategy as it interacts with the environment. The goal is to learn a policy that maximizes the reward that the agent receives from the environment. As the agent interacts with the environment, we form a trajectory ($\\tau$) of states ($s$) and actions ($a$) that are chosen throughout this process. Then, given the reward ($r_s$) associated with each of these states, we get a total return ($R(\\tau)$) given by the equation below, where $\\gamma$ is the discount factor:\n$$ \\begin{cases} \\tau \u0026= \\{s_0, a_0, s_1, a_1, \\dots, s_t, a_t\\} \\quad \u0026\\text{(Trajectory)} \\\\ R(\\tau) \u0026= \\sum_t \\gamma^t r_{s_t} \\quad \u0026\\text{(Return)} \\end{cases} $$$R(\\tau)$ is the summed reward across the agent\u0026rsquo;s full trajectory, but rewards achieved at later time steps are exponentially discounted by the factor $\\gamma$ TL;DR. The fact that the discount rate is bounded to be smaller than 1 is a mathematical trick to make an infinite sum finite. This helps proving the convergence of certain algorithms. See Understanding the role of the discount factor in reinforcement learning. \u0026mdash; This means that current rewards are more valuable than later rewards, due to both uncertainty and the simple fact that waiting to receive a reward is less desirable.\nThe goal of RL is to train an agent that maximizes this return. As shown by the equation below, we can characterize this as finding a policy that maximizes the return over trajectories that are sampled from the final policy:\nNote that the policy is a probability distribution over actions at each time step given the current state, so the exact trajectory produced is not deterministic. Many different trajectories can be obtained depending upon how we sample actions from the policy.\n$$ \\max_{\\pi} ~ \\mathbb{E}_{\\tau \\sim P_{\\pi, T}} ~ R(\\tau) $$where:\n$\\max_{\\pi}$ means to find the policy that yields the maximum return. $\\mathbb{E}_{\\tau \\sim P_{\\pi, T}}$ means to take the expectation or average over trajectories randomly sampled from a certain policy $\\pi$ and transition function $T$. $R(\\tau)$ is the return of the trajectory $\\tau$. 2.2. A Classical Example A classical example of an MDP is a maze, where the agent is trying to find the optimal path to the goal.\nA simple traditional RL environment. The target is to train the agent to find the optimal (largest return) solution path. States: The positions in the $2 \\times 3$ maze. The positions can be represented as a one-hot vector. Actions: The possible moves (up, down, left, right) \u0026mdash; This is the agent\u0026rsquo;s (i.e., the model\u0026rsquo;s) each-step output. Rewards: The agent receives a reward of $+10$ for reaching the goal and $-10$ for reaching the trap. The agent receives a reward of $0$ for all other states. Transition function: The agent can move to an adjacent state based on the chosen action, but it cannot move through walls. The transition function defines how the agent moves from one state to another based on the action taken. Policy: The agent\u0026rsquo;s policy is a probability distribution over the possible actions given the current state. For example, if the agent is in the state (0, 0), it might have a policy that gives a high probability to moving right and a low probability to moving down. Trajectory: The sequence of states and actions taken by the agent as it navigates the maze. Like many problems that are solved with RL, this setup has an environment that is not differentiable (i.e., we can’t compute a gradient and train the model in a supervised fashion) and contains long-term dependencies, meaning that we might have to learn how to perform several sequential actions to get any reward.\n2.3. Taxonomy of modern RL algorithms Taxonomy of modern RL algorithms. 3. Deep Q-Leanring 3.1. Q-Learning Q-Learning is a model-free RL algorithm, meaning that we don’t have to learn a model for the environment with which the agent interacts. The goal of Q-Learning is to learn the value of any action at a particular state.\nThere are three key concepts to mention here:\nValue $Q(s, a)$ corresponds to choosing action $a$ at current state $s$. The value not only contains the determined reward by taking action $a$ at state $s$, but also contains a discounted and recursive future Q-value of the next state $s'$ after taking action $max_a$ (i.e., the best action at state $s'$). The higher a value $Q(s, a)$ is, the more valuable it is to take action $a$ at state $s$. A look-up table (Q-table) must be maintained to store the Q values for each state-action pair. The algorithm first initialize all Q values as zero and pick an initial state with which to start the learning process. Then, iterate over the following steps:\nPick an action to execute from the current state (using an $\\varepsilon$-Greedy Policy). Get a reward and next state from the (model-free) environment. Update the Q value in the Q-table based on the Bellman equation. Here we show a simplified update method which derives from the Bellman Optimality Equation and defines $Q(s_t, a_t)$ recursively:\n$$ Q(s_t, a_t) = r_t + \\gamma \\max_{a} Q(s_{t+1}, a) $$where:\n$Q(s_t, a_t)$ is the Q value of the current state $s_t$ and action $a_t$. $r_t$ is the reward received after taking action $a_t$ at state $s_t$. $\\gamma$ is the discount factor. $\\max_{a} Q(s_{t+1}, a)$ is the maximum Q value of the next state $s_{t+1}$ over all possible actions $a$. 3.2. Deep Q-Learning (DQL) In DQL, Q-table is replaced by a neural network.\nStructure of an DQL. In DQL, we have two neural networks: the Q network and the target network. These networks are identical, but the exact architecture they use depend upon the problem being solved7. To train these networks, we first gather data by interacting with the environment. This data is gathered using the current Q network with an ε-greedy policy. This process of gathering interaction data for training the Q network is referred to as experience replay; see above.\nFrom here, we use data that has been collected to train the Q network. During each training iteration, we sample a batch of data and pass it through both the Q network and the target network. The Q network takes the current state as input and predicts the Q value of the action that is taken (i.e., predicted Q value), while the target network takes the next state as input and predicts the Q value of the best action that can be taken from that state8 (i.e., target Q value).\nFrom here, we use the predicted Q value, the target Q value, and the observed reward to train the Q network with an MSE loss; see above. The target network is held fixed. Every several iterations, the weights of the Q network are copied to the target network, allowing this model to be updated as well. Then, we just repeat this process until the Q network converges. Notably, the dataset we obtain from experience replay is cumulative, meaning that we maintain all of the data we have observed from the environment throughout all iterations.\nWhy do we need the target network? The vanilla Q-learning framework leverages two Q values in its update rule: a (predicted) Q value for the current state-action pair and the (target) Q value of the best state-action pair for the next state. In DQL, we similarly have to generate both of these Q values. In theory, we could do this with a single neural network by making multiple passes through the Q network—one for the predicted Q value and one for the target Q value. However, the Q network’s weights are being updated at every training iteration, which would cause the target Q value to constantly fluctuate as the model is updated. To avoid this issue, we keep the target network separate and fixed, only updating its weights every several iterations to avoid creating a “moving target”.\nThis idea of using a separate network to produce a training target for another network—referred to as knowledge distillation [6]—is heavily utilized within deep learning. Furthermore, the idea of avoiding too much fluctuation in the weights of the teacher/target model has been addressed in this domain. For example, the mean teacher approach [7] updates the weights of the teacher model as an exponential moving average of the student network’s weights; see above. In this way, we can ensure a stable target is provided by the teacher during training.\n4. Policy Gradients In Policy Gradients, we will assume that our policy is a machine learning model (e.g., a deep neural network) with parameters $\\theta$. This policy takes a state as input and predicts some distribution over the action space. We use this output to decide what action should be taken next within the MDP:\n$$ \\begin{cases} s \\in S \u0026 \\text{State} \\\\ a \\in A \u0026 \\text{Action} \\\\ r_s \\in \\mathbb{R} \u0026 \\text{Reward} \\\\ \\pi_{\\theta}(a|s) \u0026 \\text{Policy} \\\\ T(s_{t+1}|s_{t},a_{t}) \u0026 \\text{Transition function} \\\\ \\end{cases} $$As our agent traverses the environment, it receives positive or negative reward signals for the actions it chooses and the states that it visits. Our goal is to learn a policy from these reward signals that maximizes total reward across an entire trajectory sampled from the policy. This idea is captured by the return, which sums the total rewards over an agent’s trajectory:\n$$ \\begin{cases} \\tau \u0026= \\{s_0, a_0, s_1, a_1, \\dots, s_t, a_t\\} \\quad \u0026\\text{(Trajectory)} \\\\ R(\\tau) \u0026= \\sum_t \\gamma^t r_{s_t} \\quad \u0026\\text{(Return)} \\end{cases} $$If $\\gamma \u003c 1$, then the return is Infinite-Horizon Discounted Return. If $\\gamma = 1$, then the return is Finite-Horizon Return.\n4.1. Value Functions and Advantage Functions One final concept that will be especially relevant is that value functions. In RL, there are four basic value functions, all of which assume the infinite-horizon discounted return:\n$$ \\begin{align*} V^{\\pi}(s) \u0026= \\mathbb{E}_{\\tau \\sim (\\pi,T)} [R(\\tau)|s_0 = s] \u0026\u0026 \\text{(On-Policy Value Function)} \\\\ Q^{\\pi}(s, a) \u0026= \\mathbb{E}_{\\tau \\sim (\\pi,T)} [R(\\tau)|s_0 = s, a_0 = a] \u0026\u0026 \\text{(On-Policy Action-Value Function)} \\\\ V^{*}(s) \u0026= \\max_{\\pi} \\mathbb{E}_{\\tau \\sim (\\pi,T)} [R(\\tau)|s_0 = s] \u0026\u0026 \\text{(Optimal Value Function)} \\\\ Q^{*}(s, a) \u0026= \\max_{\\pi} \\mathbb{E}_{\\tau \\sim (\\pi,T)} [R(\\tau)|s_0 = s, a_0 = a] \u0026\u0026 \\text{(Optimal Action-Value Function)} \\end{align*} $$ On-Policy Value Functio: expected return if starting in state $s$ and act according to policy $\\pi$ afterwards. On-Policy Action-Value Function: expected return if you start in state $s$, take some action $a$ (may not come from the current policy), and act according to policy $\\pi$ afterwards. Optimal Value Function: expected return if you start in state $s$ and always act according to the optimal policy afterwards. Optimal Action-Value Function: expected return if you start in state $s$, take some action a (may not come from the current policy), and act according to the optimal policy afterwards. There is an important connection between the optimal policy in an environment and the optimal action-value function. Namely, the optimal policy selects the action in state $s$ that maximizes the value of the optimal action-value function.\nUsing the value functions described above, we can define a special type of function called an advantage function, which is heavily used in RL algorithms based on policy gradients:\n$$ A^{\\pi}(s, a) = Q^{\\pi}(s, a) - V^{\\pi}(s) \\quad \\text{(Advantage Function)} $$ $Q^{\\pi}(s, a)$ on-policy action-value function. $V^{\\pi}(s)$ on-policy value function. Simply put, the advantage function characterizes how much better it is to take a certain action a relative to a randomly-selected action in state $s$ given a policy $\\pi$. Here, we should notice that the advantage function can be derived using the on-policy value and action-value functions defined before, as these functions assume that the agent acts according to a randomly-selected action from the policy $\\pi$.\nThe value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next. \u0026mdash;\u0026mdash; [3]\n4.2. Policy Optimization During the learning process, we aim to find parameters $\\theta$ for our policy that maximize the objective function below:\n$$ \\mathcal{J}(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim (\\pi_{\\theta},T)} [R(\\tau)] $$where:\n$\\pi_{\\theta}$ is the policy with network parameters $\\theta$. $\\tau$ is the trajectory sampled from the policy $\\pi_{\\theta}$ and transition function $T$. $T$ is the transition function that defines how the agent moves from one state to another based on the action taken. $R(\\tau)$ is the return of the trajectory $\\tau$. In words, this objective function measures the expected return of trajectories sampled from our policy within the specified environment.\nIf we want to find parameters $\\theta$ that maximize this objective function, one of the most fundamental techniques that we can use is gradient ascent, which iterates over parameters $\\theta$ using the update rule shown below:\n$$ \\theta_{t+1} = \\theta_t + \\alpha \\nabla_{\\theta} \\mathcal{J}(\\pi_{\\theta})|_{\\theta_t} $$Do a lot of math to compute $\\nabla_{\\theta} \\mathcal{J}(\\pi_{\\theta})$, and the final result of the basic policy gradient is:\n$$ \\nabla_{\\theta} \\mathcal{J}(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim (\\pi_{\\theta}, T)} \\left[ \\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) R(\\tau) \\right] $$Now, we have an actual expression for the gradient of our objective function that we can use in gradient ascent! Plus, this expression only depends on the return of a trajectory and the gradient of the log probability of an action given our current state. As long as we instantiate our policy such that the gradient of action probabilities is computable (e.g., this is pretty easy to do if our policy is implemented as a neural network), we can easily derive both of these quantities.\n4.3. Computing the Policy Gradient in Practice In practice, we can estimate the value of this expectation by sampling a fixed number of trajectories, by:\nSample several trajectories by letting the agent interact with the environment according to the current policy. Estimate the policy gradient using an average of relevant quantities over the fixed number of sample trajectories. Then given a set of sampled trajectories $\\mathcal{D} = \\{\\tau_0, \\tau_1, \\dots\\}$, we can estimate the policy gradient $\\overline{\\nabla_{\\theta} \\mathcal{J}(\\pi_{\\theta})}$ as follows:\n$$ \\overline{\\nabla_{\\theta} \\mathcal{J}(\\pi_{\\theta})} = \\frac{1}{|\\mathcal{D}|} \\sum_{\\tau \\in \\mathcal{D}} \\sum_{t=0}^{T} \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) R(\\tau) \\right] $$4.4. Vallina Poclicy Gradient (VPG) and Other Policy Gradients Given:\n$$ \\nabla_{\\theta} \\mathcal{J}(\\pi_{\\theta}) = \\mathbb{E} \\left[ \\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) \\Psi_t \\right] $$We have:\nBasic Policy Gradient: $$ \\Psi_t = R(\\tau) $$ Sum of all (potentially discounted) rewards obtained along the entire trajectory. Reward-to-Go: $$ \\Psi_t = \\sum_{i=t}^{T} r_{s_i, a_i} $$ Rewards after the current action. Reward-to-Go with Baseline: $$ \\Psi_t = \\sum_{i=t}^{T} r_{s_i, a_i} - b(s_i) $$ A baseline function to our expression that only depends on the current state. Vallina Policy Gradient (VPG): $$ \\Psi_t = A^{\\pi_{\\theta}}(s_t, a_t) = Q^{\\pi_{\\theta}}(s_t, a_t) - V^{\\pi_{\\theta}}(s_t) $$ where $A^{\\pi_{\\theta}}(s_t, a_t)$ is the advantage function, $Q^{\\pi_{\\theta}}(s_t, a_t)$ is the action-value function, and $V^{\\pi_{\\theta}}(s_t)$ is the value function. TODO: Why use VPG?\n5. Proximal Policy Optimization (PPO) DQL: can only be applied in relatively simple environments. VPG: has poor data efficiency and robustness, meaning that we must collect tons of data from our environment to eliminate noise within the policy gradient estimate. Motivation for TRPO and PPO:\nGenerally applicable (i.e., to both discrete and continuous problems) Data efficient Robust (i.e., works without too much tuning) Simple (i.e., not too difficult to understand/implement) TRPO satisfies the first two points outlined above, while PPO satisfies all four.\n5.1. Aligning LLMs with RL Basic procedure for aligning LLMs. After pre-training, the model perfroms next token prediction, but its output may be repetitive, uninteresting, or not useful. That\u0026rsquo;s the reason alignment is needed.\nTypically, we perform alignment by\nSelecting several alignment criteria (e.g., follow instructions, avoid harmful output, avoid hallucination, produce interesting/creative output, etc.) Finetuning the model \u0026mdash;\u0026mdash; via SFT and RLHF \u0026mdash;\u0026mdash; to satisfy these criteria. The final model can further finetuned and used to solve a downstream application via prompting (or in-context learning). Procedure of SFT and RLHF. As shown in the figure above, to apply RLHF:\nPrepare a set of prompts and generate several outputs for each prompt with the language model. Ask a group of human annotators to rank/score the responses to each prompt according to our alignment criteria. Use these ranked responses to train a reward model that predicts a human preference score from a language model’s response. Use PPO (or other algorithms, e.g., VPG, TRPO) to finetune our language model to maximize the human preferences scores (predicted by the reward model) of its outputs. 5.2. Kullback–Leibler (KL) Divergence At the highest level, the Kullback-Leibler (KL) Divergence is just a method of comparing two probability distributions.\nThe idea of KL divergence has its roots in information theory and is highly related to the concept of entropy According to Shannon\u0026rsquo;s Source Coding Theorem, the optimal number of bits required to encode a message with probability $p(x)$ is given by $−\\log_{2}{p(x)}$.\nHigh probability event ($p(x) \\approx 1$): $−log_2​(1)=0$. It takes very few bits to encode a highly probable event.\nLow probability event ($p(x) \\approx 0$): $−log_2​(p(x))$ is a large number. It takes many bits to encode a rare event. :\n$$ H= \\begin{cases} -\\mathbb{E} \\left[ \\log p(x) \\right] \u0026\\text{(Continuous Case)} \\\\ -\\sum_{i=1}^{N} p(x_i) \\cdot \\log p(x_i) \u0026\\text{(Discrete Case)} \\end{cases} $$In the equation above, we can see common formulations of entropy $H$ for a probability distribution $p$. Intuitively, the entropy value captures how much information is stored within a probability distribution \u0026mdash;\u0026mdash; a lower entropy means that you would need fewer bits to encode the information stored within $p$.\nInstead of a single probability distribution $p$, the KL divergence considers two probability distributions: $p$ and $q$. Then, mirroring the above entropy formulation, we compute KL divergence by finding the expected difference in log probabilities between these two distributions:\n$$ \\begin{align*} D_{\\text{KL}}(p||q) \u0026= H(p, q) - H(p) \\\\ \u0026= \\begin{cases} \\mathbb{E} \\left[ \\log p(x) - \\log q(x) \\right] ~~~ \\text{(Continuous Case)} \\\\ \\sum_{i=1}^{N} p(x_i) \\cdot \\left( \\log p(x_i) - \\log q(x_i) \\right) ~~~ \\text{(Discrete Case)} \\end{cases} \\end{align*} $$where:\n$H(p,q)$: The average number of bits used with the approximate code. $H(p)$: The minimum possible average number of bits used with the optimal code. $D_{\\text{KL}}(p||q)$: The penalty, or the expected number of extra bits \u0026ldquo;wasted\u0026rdquo; or \u0026ldquo;lost\u0026rdquo; per message due to the approximation. The KL divergence is commonly explained in the context of approximations. Namely, if we approximate $p$ with $q$, the KL divergence is the number of bits we would expect to lose by making this approximation.\nKL divergence is heavily used across different domains of AI/ML research. For example, it is commonly used in loss functions for training neural networks, either as the core loss or as an added regularization term.\nThe final reward function we use during optimization contains a [KL divergence] penalty term … we find this constraint is useful for training stability, and to reduce reward hacking.” \u0026mdash;\u0026mdash; [4]\n5.3. Trust Region Policy Optimization (TRPO) VPG is limited by the fact that it can only perform a single policy update for each estimate of the policy gradient that is derived. Given that VPG is notoriously data inefficient, meaning that we have to sample a lot of data when deriving a policy update, performing multiple (or larger) updates may seem enticing. However, such an approach is not justified theoretically and, in practice, leads to policy updates that are too large, thus damaging performance.\nTrust Region Policy Optimization (TRPO) [5] aims to solve the problem described above using an approach that is similar to VPG. At each step of the optimization process, however, we find the largest possible policy update that still improves performance. Simply put, TRPO allows us to learn faster by finding a reliable way to make larger policy updates that do not damage performance.\nMore specifically, we update the policy under a constraint—based on the KL divergence—that captures the distance between policies before and after the current update. Considering this constraint allows us to find a balance between update size and the amount of change to the underlying policy:\n$$ \\begin{equation*} \\begin{gathered} \\theta_{k+1} = \\operatorname{argmax}_{\\theta} \\mathbb{E}_{(s,a) \\sim (\\pi_{\\theta_k}, T)} \\left[ \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)} A^{\\pi_{\\theta_k}}(s,a) \\right] \\\\ \\text{such that } \\overline{D}_{\\text{KL}}(\\theta||\\theta_k) \u003c \\delta \\end{gathered} \\end{equation*} $$where:\n$\\mathbb{E}_{(s,a) \\sim (\\pi_{\\theta_k}, T)}$ is the expectation over state-action pairs sampled from the current policy $\\pi_{\\theta_k}$ and transition function $T$. $\\pi_{\\theta}(a|s)$ is the probability of taking action $a$ in state $s$ according to the new policy $\\pi_{\\theta}$. $\\pi_{\\theta_k}(a|s)$ is the probability of taking action $a$ in state $s$ according to the current policy $\\pi_{\\theta_k}$. $A^{\\pi_{\\theta_k}}(s,a)$ is the advantage function for the current policy $\\pi_{\\theta_k}$. $\\overline{D}_{\\text{KL}}(\\theta||\\theta_k)$ is the average KL divergence between the new policy $\\pi_{\\theta}$ and the current policy $\\pi_{\\theta_k}$. $\\delta$ is a hyperparameter that controls the maximum allowed change in the policy. Formulation of TRPO has several critical differences from VPG:\nThe terms in the expectation are modified slightly to express the probability of a given action a as a ratio between old and updated policies. The update has an added constraint based on the KL divergence between old and updated policies. Instead of performing gradient ascent, we are solving a constrained maximization problem to generate each new policy The implementation of TRPO is similar to that of VPG. We allow our current policy to interact with the environment and collect data. From this observed data, we can compute the approximate update for TRPO as described above. Then, we can continue the process of collecting data and performing an update until we arrive at a policy that performs quite well.\nBecause we are using the actual policy being trained to collect the data used to train it, TRPO is an on-policy reinforcement learning algorithm.\n5.4. TRPO vs. VPG: Larger Policy Updates As mentioned previously, the VPG algorithm is based upon gradient ascent, which \u0026mdash;\u0026mdash; by nature \u0026mdash;\u0026mdash; ensures that updates to the policy\u0026rsquo;s parameters $\\theta$ are not too large. In particular, we use a learning rate to perform updates with VPG, which can control the size of the update in the parameter space:\n$$ \\theta_{t+1} = \\theta_t + \\alpha \\nabla_{\\theta} J(\\pi_{\\theta})|_{\\theta_t} $$Here, only the size of the update to $\\theta$ is controlled, and so that the old and updated policies are close in the parameter space. However, small changes to $\\theta$ may also drastically alter the policy, because ensuring that policy updates are small in the parameter space does not provide much of a guarantee on changes to the resulting policy.\nAs a result, we are constrained to relatively small updates within the VPG algorithm \u0026mdash;\u0026mdash; larger or multiple updates could be harmful.\nTRPO sidesteps this issue by considering the size of our policy update from an alternative viewpoint. Namely, we compare updated and old policies using the KL divergence, which measures the difference in probability distributions over the action space produced by the two policies. Such an approach compares policies based upon the actions they take rather than their underlying parameters $\\theta$.\nIn this way, we can perform large policy updates while ensuring that the new policy does not produce actions that are significantly different from the old policy.\n5.5. Proximal Policy Optimization (PPO) We introduce proximal policy optimization, a family of policy optimization methods that use multiple epochs of stochastic gradient ascent to perform each policy update. These methods have the stability and reliability of trust-region methods but are much simpler to implement \u0026hellip; applicable in more general settings, and have better overall performance. \u0026mdash;\u0026mdash; [6]\nTRPO has improved data efficiency, stability, and reliability compared to the VPG algorithm, but there are still limitations that need to be addressed.\nNamely, the algorithm is complicated, can only perform a single update each time new data is sampled from the environment, and is only applicable to certain problem setups.\nAiming to develop a better approach, authors in [6] propose Proximal Policy Optimization (PPO), another policy gradient algorithm that alternates between collecting data from the environment and performing several epochs of training over this sampled data. PPO shares the reliability of TRPO and is:\nMuch simpler More data efficient More generally applicable Similar to TRPO, we perform policy updates in PPO according to a surrogate objective. However, this surrogate objective has a \u0026ldquo;clipped\u0026rdquo; probability ratio, as shown in the equation below:\n$$ L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t \\left[ \\min(\\frac{\\pi_{\\theta}(o_t|q, o_{\u003c t})}{\\pi_{\\theta_{old}}(o_t|q, o_{\u003c t})}A_t, \\text{CLIP}(\\frac{\\pi_{\\theta}(o_t|q, o_{\u003c t})}{\\pi_{\\theta_{old}}(o_t|q, o_{\u003c t})}, 1-\\varepsilon, 1+\\varepsilon) A_t) \\right] $$The surrogate objective for PPO is expressed as a minimum of two values. The first value is the same surrogate objective from TRPO, while the second value is a \u0026ldquo;clipped\u0026rdquo; version of this objective that lies within a certain range. In practice, this expression is formulated such that there is no reward for moving the probability ratio beyond the interval $[1 - \\varepsilon, 1 + \\varepsilon]$, see the figure below:\nFrom [2]. In other words, PPO has no incentive for excessively large policy updates. Plus, by taking the minimum of the clipped and unclipped version of the surrogate objective, we only ignore excessive changes to the probability ratio if they improve the underlying objective. In the figure above, we see a basic depiction of this trend for both positive and negative values of the advantage function.\nTo understand PPO\u0026rsquo;s surrogate objective more intuitively, we should look at the figure below, which plots several objective functions as we interpolate between an old and updated policy obtained via PPO. In this figure, we see the KL divergence, the TRPO surrogate objective (labeled as CPI), the clipped surrogate objective, and the full PPO surrogate objective. From these plots, we can see that the PPO surrogate objective is a pessimistic/lower bound for the TRPO surrogate objective, where a penalty is incurred for having too large of a policy update.\nFrom [2]. While TRPO sets a hard constraint to avoid policy updates that are too large, PPO simply formulates the surrogate objective such that a penalty is incurred if the KL divergence is too large. Such an approach is much simpler, as we no longer have to solve a difficult, constrained optimization problem. Rather, we can compute PPO’s surrogate loss with only minor tweaks to the VPG algorithm.\nPPO has several benefits compared to TRPO. First, the implementation of PPO is much simpler compared to TRPO, as we can use automatic differentiation and gradient-based optimization techniques9 instead of deriving an (approximate) solution for a complex, constrained objective function. Additionally, while TRPO makes only a single policy update each time new data is collected, PPO performs multiple epochs of optimization via stochastic gradient ascent over the surrogate objective, which improves data efficiency.\nFinally, computing estimates of the advantage function (e.g., via Generalized Advantage Estimation (GAE)) typically requires that we learn a corresponding value function. In TRPO, we must learn this state-value function with a separate neural network. However, PPO—due to its compatibility with a wider scope of architectures (including those with parameter sharing) \u0026mdash;\u0026mdash; can train a joint network for policy and value functions by just adding an extra term to the loss function that computes the mean-squared error (MSE) between estimated and actual value function values.\n$$ L^{\\text{CLIP+VF}}(\\theta) = \\mathbb{E}_t \\left[ L^{\\text{CLIP}}(\\theta) - c_1 L^{\\text{VF}}(\\theta) \\right] $$where:\n$L^{\\text{CLIP}}(\\theta)$ is the PPO surrogate objective. $L^{\\text{VF}}(\\theta)$ is the MSE loss for the value function. $c_1$ is a hyperparameter that controls the weight of the value function loss in the overall loss function. 6. Group Relative Policy Optimization (GRPO) From [7]. Different from PPO, GRPO:\nRemoves the value function model. The policy model generates multiple outputs for each input, and the reward model calculates the reward for each output, and calculates the advantage scores after group computation. Removes the GAE, and changes the method to calculate KL. In PPO, we optimizes LLMs by maximizing the following objective function:\n$$ \\begin{align*} \\mathcal{J}_{\\text{PPO}}(\\theta) = \\mathbb{E}\u0026_{q \\sim P(Q), o \\sim \\pi_{\\theta_{\\text{old}}}(O|q)} \\left[ \\frac{1}{|o|} \\sum_{t=1}^{|o|} \\min \\left( \\frac{\\pi_{\\theta}(o_t|q, o_{\u003c t})}{\\pi_{\\theta_{\\text{old}}}(o_t|q, o_{\u003c t})} A_t, \\right. \\right. \\\\ \u0026 \\left. \\left. \\text{clip}\\left(\\frac{\\pi_{\\theta}(o_t|q, o_{\u003c t})}{\\pi_{\\theta_{\\text{old}}}(o_t|q, o_{\u003c t})}, 1-\\varepsilon, 1+\\varepsilon\\right) A_t \\right) \\right] \\end{align*} $$where:\n$\\pi_{\\theta}$ and $\\pi{\\theta_{\\text{old}}}$ are the current and old policy models; $q$, $o$ are questions and outputs sampled from the question dataset and the old policy $\\pi{\\theta_{\\text{old}}}$; $\\varepsilon$ is a clipping-related hyper-parameter introduced in PPO for stabilizing training; $A_t$ is the advantage, which is computed by applying Generalized Advantage Estimation (GAE), based on the rewards ${r_{\\geq t}}$ and a learned value function $V_{\\psi}$. Thus, in PPO, a value function needs to be trained alongside the policy model and to mitigate over-optimization of the reward model, the standard approach is to add a per-token KL penalty from a reference model in the reward at each token, i.e.:\n$$ r_t = r_{\\varphi}(q, o_{\\leq t}) - \\beta \\log \\frac{\\pi_{\\theta}(o_t|q, o_{\u003c t})}{\\pi_{ref}(o_t|q, o_{\u003c t})} $$There are several issues with PPO:\nAs the value function employed in PPO is typically another model of comparable size as the policy model, it brings a substantial memory and computational burden. During RL training, the value function is treated as a baseline in the calculation of the advantage for variance reduction. While in the LLM context, usually only the last token is assigned a reward score by the reward model, which may complicate the training of a value function that is accurate at each token. To address these issues, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\dots , o_G\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model by maximizing the following objective:\n$$ \\begin{align*} \\mathcal{J}_{\\text{GRPO}}(\\theta) = \\mathbb{E}\u0026[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] \\\\ \u0026\\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\left\\{ \\min\\left(\\frac{\\pi_{\\theta}(o_{i,t}|q, o_{i,\u003c t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t}|q, o_{i,\u003c t})} \\hat{A}_{i,t}, \\right.\\right. \\\\ \u0026\\left.\\left. \\text{clip}\\left(\\frac{\\pi_{\\theta}(o_{i,t}|q, o_{i,\u003c t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t}|q, o_{i,\u003c t})}, 1-\\varepsilon, 1+\\varepsilon\\right) \\hat{A}_{i,t}\\right) - \\beta D_{KL}[\\pi_{\\theta}||\\pi_{ref}] \\right\\} \\end{align*} $$where:\n$\\varepsilon$ and $\\beta$ are hyper-parameters; $\\hat{A}_{i,t}$ is the advantage calculated based on relative rewards of the outputs inside each group only. See here for verl\u0026rsquo;s implementation. For simplicity, given input scores (a tensor of padded rewards with shape (batch_size, response_len)), the algorithm first sums the rewards for each response, then normalizes the rewards in each group (a batch may contain multiple groups), and finally broadcasts and multiplies the rewards (of shape (batch_size, 1)) with reponse mask (of shape (batch_size, response_len)) to get the advantage That is to say, for each response of shape (1, response_len), the advantages are the same, except for the padded positions, which are 0. . Note that:\nInstead of adding KL penalty in the reward, GRPO regularizes by directly adding the KL divergence between the trained policy and the reference policy to the loss, avoiding complicating the calculation of $\\hat{A}_{i,t}$. Different from the KL penalty term used in [6], GRPO estimate the KL divergence with the following unbiased estimator: $$ \\mathbb{D}_{\\text{KL}}\\left[ \\pi_{\\theta}||\\pi_{ref}\\right] = \\frac{\\pi_{ref}(o_{t,t}|q, o_{t,\u003c t})}{\\pi_{\\theta}(o_{t,t}|q, o_{t,\u003c t})} - \\log \\frac{\\pi_{ref}(o_{t,t}|q, o_{t,\u003c t})}{\\pi_{\\theta}(o_{t,t}|q, o_{t,\u003c t})} - 1 $$ which is guaranteed to be positive. The code of the objective function, i.e., compute_policy_loss, is show here in verl. It can be seen that the implementation is a bit different from the equation above:\nThe $\\frac {\\pi_{\\theta}(o_{i,t}|q, o_{i,\u003c t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t}|q, o_{i,\u003c t})}$ term is replaced with $e^{\\log \\pi_{\\theta}(o_{i,t}|q, o_{i,\u003c t}) - \\log \\pi_{\\theta_{\\text{old}}}(o_{i,t}|q, o_{i,\u003c t})}$, which is more numerically stable, as shown in these lines in function compute_policy_loss. The kl penalty term is removed, and calculated separately in DataParallelPPOActor.update_policy, according to option use_kl_loss (which is default false but shoule be set to true for GRPO), as shown here. Both the policy loss and the KL loss would be passed to function agg_loss, which aggregates them to scalar values. The default method is token-mean, which is different from \u0026ldquo;the original GRPO paper takes the sample-level loss (seq-mean-token-mean), which may be unstable in long-CoT scenarios\u0026rdquo;. All GRPO example scripts provided in verl uses the default configuration \u0026ldquo;token-mean\u0026rdquo; for loss aggregation instead.\nBtw, token-mean will means the (policy gradient) loss across all the tokens in all the sequences in a mini-batch; An idea of DAPO? 7. DAPO: An Open-Source LLM Reinforcement Learning System at Scale [8] Clip-Higher, which promotes the diversity of the system and avoids entropy collapse; Configured here and used here in compute_policy_loss. Dynamic Sampling, which improves training efficiency and stability; Shown here in RayDPAOTrainer. Token-Level Policy Gradient Loss, which is critical in long-CoT RL scenarios; Configured here, which has been introduced in the above GRPO section. Overlong Reward Shaping, which reduces reward noise and stabilizes training; Configured here and used here in DapoRewardManager. Before defining the object function, it is worth noting that the KL term is excluded from the proposed algorithm.\nThe KL penalty term is used to regulate the divergence between the online policy and the frozen reference policy. In the RLHF scenario, the goal of RL is to align the model behavior without diverging too far from the initial model. However, during training the long-CoT reasoning model, the model distribution can diverge significantly from the initial model, thus this restriction is not necessary.\nThe objective function of DAPO is shown below:\n$$ \\begin{align*} \\mathcal{J}_{\\text{DAPO}}(\\theta) =\u0026 ~ \\mathbb{E}_{(q,a) \\sim \\mathcal{D}, \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(\\cdot|q)} \\\\ \u0026\\left[ \\frac{1}{\\sum_{i=1}^G |o_i|} \\sum_{i=1}^G \\sum_{t=1}^{|o_i|} \\min \\left( r_{i,t}(\\theta)\\hat{A}_{i,t}, \\text{clip}(r_{i,t}(\\theta), 1 - \\epsilon_{\\text{low}}, 1 + \\epsilon_{\\text{high}})\\hat{A}_{i,t} \\right) \\right] \\\\ \\text{s.t. } \u00260 \u003c \\left | \\{o_i \\mid \\text{is_equivalent}(a, o_i)\\} \\right | \u003c G, \\end{align*} $$where:\n$$ r_{i,t}(\\theta) = \\frac{\\pi_\\theta(o_{i,t} \\mid q, o_{i, \u003c t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t} \\mid q, o_{i, \u003c t})}, \\quad \\hat{A}_{i,t} = \\frac{R_i - \\text{mean}(\\{R_i\\}_{i=1}^G)}{\\text{std}(\\{R_i\\}_{i=1}^G)} $$8. Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning [9] Token entropy calculation:\n$$ \\begin{gather*} H_t := - \\sum_{j=1}^{V} p_{t,j} \\log p_{t,j} \\\\ \\text{ where } (p_{t,1}, \\cdots, p_{t,V}) = \\boldsymbol{p}_t = \\pi_{\\theta}(\\cdot \\mid q, o_{ \u003c t}) = \\text{Softmax}\\left(\\frac{\\mathbf{z}_t}{T}\\right) \\end{gather*} $$Here,\n$\\pi_{\\theta}$ denotes an LLM parameterized by $\\theta$ $q$ is the input query, and $o_{\u003c t} = (o_1, o_2, \\ldots, o_{t-1})$ represents the previously generated tokens $V$ is the vocabulary size $\\mathbf{z}_t \\in \\mathbb{R}^V$ denotes the pre-softmax logits at time step $t$ $\\boldsymbol{p}_t \\in \\mathbb{R}^V$ is the corresponding probability distribution over the vocabulary $T \\in \\mathbb{R}$ is the decoding temperature In off-policy settings, sequences are generated by a rollout policy $\\pi_{\\phi}$ while the training policy is $\\pi_{\\theta}$, with $\\phi \\neq \\theta$. The entropy is still calculated using $\\pi_{\\theta}$, as defined in Equation (1), to measure the uncertainty of the training policy in the given sequence.\nReferences Cameron R. Wolfe. Basics of Reinforcement Learning for LLMs. Cameron R. Wolfe. Proximal Policy Optimization (PPO): The Key to LLM Alignment. Achiam, Josh. Spinning Up in Deep RL. OpenAI, 2018. Touvron, Hugo, et al. \u0026ldquo;Llama 2: Open foundation and fine-tuned chat models.\u0026rdquo; arXiv preprint arXiv:2307.09288 (2023). Schulman, John, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. \u0026ldquo;Trust region policy optimization.\u0026rdquo; In International conference on machine learning, pp. 1889-1897. PMLR, 2015. Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. \u0026ldquo;Proximal policy optimization algorithms.\u0026rdquo; arXiv preprint arXiv:1707.06347 (2017). Shao, Zhihong, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang et al. \u0026ldquo;Deepseekmath: Pushing the limits of mathematical reasoning in open language models.\u0026rdquo; arXiv preprint arXiv:2402.03300 (2024). Yu, Qiying, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai et al. \u0026ldquo;Dapo: An open-source llm reinforcement learning system at scale.\u0026rdquo; arXiv preprint arXiv:2503.14476 (2025). Wang, Shenzhi, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang et al. \u0026ldquo;Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning.\u0026rdquo; arXiv preprint arXiv:2506.01939 (2025). ","permalink":"https://jamesnulliu.github.io/blogs/reinforcement-learning-for-llms/","summary":"\u003ch2 id=\"1-basics\"\u003e1. Basics\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eRLHF: Reinforcement Learning from Human Feedback\u003c/li\u003e\n\u003cli\u003eSFT: Supervised Fine-Tuning\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eRL trains neural networks through \u003cstrong\u003etrial\u003c/strong\u003e and \u003cstrong\u003eerror\u003c/strong\u003e. When finetuning a language model with RLHF, the model produces some text then receives a score/reward from a human annotator that captures the quality of that text. Then, we use RL to finetune the language model \u003cstrong\u003eto generate outputs with high scores\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eIn this case, we cannot apply a loss function that trains the language model to maximize human preferences with supervised learning. This is because there’s no easy way to explain the score human give or connect it mathematically to the output of the neural network. In other words, we cannot backpropagate a loss applied to this score through the rest of the neural network. This would require that we are able to \u003cstrong\u003edifferentiate (i.e., compute the gradient of) the system that generates the score\u003c/strong\u003e, which is a human that subjectively evaluates the generated text.\u003c/p\u003e","title":"Reinforcement Learning for LLMs"},{"content":"1. Introduction to Memory Coalescing 1.1. Dynamic Random Access Memories (DRAMs) Accessing data in the global memory is critical to the performance of a CUDA application.\nIn addition to tiling techniques utilizing shared memories, we discuss memory coalescing techniques to move data efficiently from global memory into shared memory and registers.\nGlobal memory is implemented with dynamic random access memories (DRAMs). Reading one DRAM is a very slow process.\nModern DRAMs use a parallel process: Each time a location is accessed, many consecutive locations that includes the requested location are accessed.\nIf an application uses data from consecutive locations before moving on to other locations, the DRAMs work close to the advertised peak global memory bandwidth.\n1.2. Memory Coalescing Recall that all threads in a warp execute the same instruction.\nWhen all threads in a warp execute a load instruction, the hardware detects whether the threads access consecutive memory locations.\nThe most favorable global memory access is achieved when the same instruction for all threads in a warp accesses global memory locations.\nIn this favorable case, the hardware coalesces all memory accesses into a consolidated access to consecutive DRAM locations.\nDefinition: Memory Coalescing\nIf, in a warp, thread $0$ accesses location $n$, thread $1$ accesses location $n + 1$, \u0026hellip; thread $31$ accesses location $n + 31$, then all these accesses are coalesced, that is: combined into one single access.\nThe CUDA C Best Practices Guide gives a high priority recommendation to coalesced access to global memory.\n2. Example: Vector Addition 2.1. Coalesced Access Coalesced Memory Access means that each thread in a warp accesses consecutive memory locations so that the hardware can combine all these accesses into one single access. By doing so, fewer wasted data are transferred and the memory bandwidth is fully utilized.\nClick to See Example Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 __global__ void vecAddKernel(const fp32_t* a, const fp32_t* b, fp32_t* c, int32_t n) { int gtid = threadIdx.x + blockDim.x * blockIdx.x; if (gtid \u0026lt; n) { // [DRAM] 2 load, 1 store, 3 inst c[gtid] = a[gtid] + b[gtid]; } } void launchVecAdd(const fp32_t* d_A, const fp32_t* d_B, fp32_t* d_C, size_t n) { dim3 blockSize = {std::min\u0026lt;uint32_t\u0026gt;(n, 1024), 1, 1}; dim3 gridSize = {ceilDiv\u0026lt;uint32_t\u0026gt;(n, blockSize.x), 1, 1}; vecAddKernel\u0026lt;\u0026lt;\u0026lt;gridSize, blockSize\u0026gt;\u0026gt;\u0026gt;(d_A, d_B, d_C, int32_t(n)); } Note that in NVIDIA GPUs:\nWARP is the smallest unit of execution, which contains 32 threads. SECTOR is the smallest unit of data that can be accessed from global memory, which is exactly 32 bytes. In the example above, all threads in a warp access consecutive memory locations both for a, b, and c, and for each $32 * 4 / 32 = 4$ sectors, only ONE instruction to a warp is needed to access the data. This is so-called coalesced memory access.\nCoalesced Memory Access. There are 2N loads operations and 1N store operations in the kernel, which in all are 2N/32 load instructions and 1N/32 store instructions for warps (each warp executes 1 instruction). Since the access to memroy is coalesced, one instruction will transfer 4 sectors of data. There are no any wasted data. Another example of coalesced memory access is shown below:\nClick to See Example Code 1 2 3 4 5 6 7 8 9 10 __global__ void vecAddKernelv1(const fp32_t* a, const fp32_t* b, fp32_t* c, int32_t n) { int gtid = threadIdx.x + blockDim.x * blockIdx.x; gtid = gtid % 2 == 0 ? gtid + 1 : gtid - 1; if (gtid \u0026lt; n) { // [DRAM] 2 load, 1 store, 3 inst c[gtid] = a[gtid] + b[gtid]; } } Crompared to the previous example, each 2 threads exchange their access positions. However, the access to memory is still coalesced.\nAnother Example of Coalesced Memory Access. 1 intruction will transfer 4 sectors of data. There are no any wasted data. 2.2. Non-Coalesced Access Non-Coalesced Memory Access means that some thread in a warp accesses non-consecutive memory locations so that the hardware cannot combine all these accesses into one single access. By doing so, more wasted data are transferred and the memory bandwidth is not fully utilized.\nSee the example code below. Originally, 32 threads in a warp would access 32 consecutive fp32 elements. However, I make the first thread in each warp access the 33th fp32 element (which should be accessed by the next warp), making an intented non-coalesced access.\nClick to See Example Code 1 2 3 4 5 6 7 8 9 10 11 12 __global__ void vecAddKernelv1(const fp32_t* a, const fp32_t* b, fp32_t* c, int32_t n) { int gtid = threadIdx.x + blockDim.x * blockIdx.x; if (gtid % warpSize == 0) { gtid = (gtid + warpSize) % (ceilDiv(n, warpSize) * warpSize); } if (gtid \u0026lt; n) { // [DRAM] 2 load, 1 store, 3 inst c[gtid] = a[gtid] + b[gtid]; } } The memory access pattern is shown in the figure below. Campare to the previous examples, you can see that despite the total number of load/store instructions is the same (2N/32 load instructions and 1N/32 store instructions), for each warp, 5 sectors of data are now being transferred per instruction. From the perspective of hardware, more data are being transferred than needed.\nNon-Coalesced Memory Access. There are 2N/32 load instructions and 1N/32 store instructions for warps. But one instruction will transfer 5 sectors of data, as shown in the first warp with 5 orange sectors. In Nsight Compute, you can see the performance analysis in the \u0026ldquo;Memory Workload Analysis\u0026rdquo; section. Optimization suggestions are provided for reducing wasted data transfer.\nPerformance analysis of non-coalesced memory access using Nsight Compute. References Programming Massively Parallel Processors: A Hands-on Approach, 4th Edition 【CUDA调优指南】合并访存 Memory Coalescing Techniques ","permalink":"https://jamesnulliu.github.io/blogs/cuda-programming-notes-01-memory-coalescing/","summary":"Introduction to memory coalescing with Nsight Compute.","title":"CUDA Programming Notes | 01: Memory Coalescing"},{"content":"1. Estimating Total FLOPs We only consider the FLOPs of Transformer layers, excluding the embedding layer and the output layer.\nAttention:\nEach projection for Q, K and V is matmul of input (B, S, H) and weight (H, H), yielding (B, S, H):\n$$ \\text{FLOPs} = 3 \\times (2 \\times B \\times S \\times H \\times H) = 6 \\times B \\times S \\times H^2 $$ $S = QK^T$, matmul of $Q$ (B, S, H) and $K^T$ (B, H, S), yielding (B, S, S):\n$$ \\text{FLOPs} = 2 \\times B \\times S \\times S \\times H = 2 \\times B \\times S^2 \\times H $$ $L = S \\cdot V$, matmul of $S$ (B, S, S) and $V$ (B, S, H), yielding (B, S, H):\n$$ \\text{FLOPs} = 2 \\times B \\times S \\times H \\times S $$ $O = L \\cdot W_O$, matmul of $L$ (B, S, H) and $W_O$ (H, H), yielding (B, S, H):\n$$ \\text{FLOPs} = 2 \\times B \\times S \\times H^2 $$ Total attention FLOPs per Transformer layer: $$ \\text{FLOPs} = 8 \\times B \\times S \\times H^2 + 4 \\times B \\times S^2 \\times H $$ Feed-Forward Networek\nTypically 2 linear layers, one mapping (B, S, H) to (B, S, 4H) and the other mapping (B, S, 4H) to (B, S, H):\nTotal FFN FLOPs per Transformer layer:\n$$ \\begin{align*} \\text{FLOPs} \u0026= 2 \\times B \\times S \\times H \\times (4 \\times H) + 2 \\times B \\times S \\times (4 \\times H) \\times H \\\\ \u0026= 16 \\times B \\times S \\times H^2 \\end{align*} $$ Total FLOPs: $N$ Layers of Transformer\nEach Transformer layer consists of an attention mechanism and a feed-forward network\nWhen prefilling, the total FLOPs is:\n$$ \\text{FLOPs}_\\text{total} = N (24 B S H^2 + 4 B S^2 H) $$ When decoding, suppose the input is of shape (B, Si, H) and KV cache is of shape (B, Sc, H), the total FLOPs is:\n$$ \\text{FLOPs}_\\text{total} = N (24 B S_i H^2 + 4 B S_i S_c H) $$ 2. Estimating Total Bytes Transfered In FP16, each parameter or activation element is 2 bytes.\nData transferred includes loading model weights and handling activations.\nSuppose we have a $Z$-B-fp16 model and $N$ Transformer layers, each with input size (B, S, H).\nModel Weights\nA $Z$-B-fp16 model has $Z \\times 10^9$ fp16 parameters, each 2 bytes:\n$$ \\text{Bytes}_\\text{weights} = Z \\times 10^9 \\times 2 ~ \\text{Bytes} = 2 \\times Z ~ \\text{GBytes} $$In an optimized GPU inference, weights are typically loaded into high-bandwidth memory (HBM) once and reused, so we assume $2Z$ GB is read once per forward pass.\nActivations\nFor each Transfomer layer, input and output activations are of shape (B, S, H), and each element is 2 bytes in fp16: $$ \\text{Bytes}_\\text{act-layer} = B \\times S \\times H \\times 2 ~ \\text{Bytes} $$ For $N$ layers, activations are computed sequentially. Since each layer’s output becomes the next layer’s input (read once, written once):\n$$ \\begin{align*} \\text{Bytes}_\\text{act-total} \u0026= 2 \\times N \\times \\text{Bytes}_\\text{act-layer} ~ \\text{Bytes} \\\\ \u0026= 4 \\times N \\times B \\times S \\times H ~ \\text{Bytes} \\end{align*} $$ KV Caches\nWhen decoding, each Transformer layer would load cached K and V both of shape (B, Sc, H). After decoding, the new K and V of shape (B, Si, H) are computed and cached for the next layer. So the bytes transfered for one forward pass is:\n$$ \\text{Bytes}_\\text{KV} = N \\times (B \\times S_c \\times H + 2 \\times B \\times S_i \\times H) \\times 2 ~ \\text{Bytes} $$ Total Data Transferred\nWhen prefilling, the total bytes transferred is:\n$$ \\begin{align*} \\text{Bytes}_\\text{total} \u0026= \\text{Bytes}_\\text{weights} + \\text{Bytes}_\\text{act-total} \\\\ \u0026= 2 Z \\text{e}^9 + 4 N B S H ~ \\text{Bytes} \\end{align*} $$ When decoding, suppose cached sequence length is $S_c$ and the input sequence length is $S_i$, the total bytes transferred is:\n$$ \\begin{align*} \\text{Bytes}_\\text{total} \u0026= \\text{Bytes}_\\text{weights} + \\text{Bytes}_\\text{act-total} + \\text{Bytes}_\\text{KV} \\\\ \u0026= 2 Z \\text{e}^{9} + 8 N B S_i H + 2 N B S_c H ~ \\text{Bytes} \\end{align*} $$ 3. Arithmetic Intensity When prefilling, there is no cached K and V, so the arithmetic intensity is:\n$$ \\begin{align*} \\text{Arithmetic Intensity} \u0026= \\text{FLOPs}_\\text{total} / \\text{Bytes}_\\text{total} \\\\ \u0026= \\frac{N (24 B S H^2 + 4 B S^2 H)}{2 Z 10^9 + 4 N B S H} \\end{align*} $$When decoding, suppose cached sequence length is $S_c$ and the input sequence length is $S_i$ , then the arithmetic intensity is:\n$$ \\begin{align*} \\text{Arithmetic Intensity} \u0026= \\text{FLOPs}_\\text{total} / \\text{Bytes}_\\text{total} \\\\ \u0026= \\frac{N (24 B S_i H^2 + 4 B S_i S_c H)}{2 Z 10^9 + 8 N B S_i H + 2 N B S_c H} \\end{align*} $$4. Roofline Model Roofline Model. If the arithmetic intensity is on the right side of the machine balance, the performance compute-bound. If it is on the left side, the performance is memory-bound. A100-80GB has the following hardware `specifications:\nPeak FLOPs ($\\pi$): $312 \\times 10^{12}$ FLOPs/s Memory Bandwidth ($\\beta$): $2039 \\times 10^9$ B/s Machine Balance ($I_{max}$): $312 \\times 10^{12} / (2039 \\times 10^9) \\approx 153$ FLOPs/Byte Here are two examples of arithmetic intensity estimation:\nSee: Arithmetic Intensity for Prefilling See: Arithmetic Intensity for Speculative Decoding 5. Discussion: Tensor Parallelism If the model is split across multiple GPUs using TP, the hidden size H and the model weight is divided by the number of GPUs.\n","permalink":"https://jamesnulliu.github.io/blogs/arithmetic-intensity-estimation-of-large-language-models/","summary":"This blog post discusses the arithmetic intensity of large language models and how it affects the performance of these models.","title":"Arithmetic Intensity Estimation of Large Language Models"},{"content":" 1. Introduction to Speculative Decoding Given a score model S (for example, LLAMA-3-70B) and a draft model D (for example, LLAMA-3-7B), the process of speculative decoding can be described as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 input_ids = Tensor(...) # (seq_len,) while True: # D generates tokens[seq_len, ..., seq_len + k] draft_outputs = D(input_ids) # (k,) # Given tokens[seq_len - 1, ..., seq_len + k], S generates real # prediction for tokens[seq_len, ..., seq_len + k, seq_len + k + 1] with # one forward pass. score_outputs = S(cat(input_ids, draft_outputs)) # (k + 1,) i = 0 for i in range(k): if not verify(draft_outputs[i], score_outputs[i]): break input_ids.append(draft_outputs[i]) input_ids.append(score_outputs[i]) Speculative decoding workflow in vLLM (k=3, top-p=1). k=3 indicates that the draft model generates 3 tokens per forward pass, and top-p=1 means that for each token, only 1 candidate is proposed. As shown in the picture, at prefill statge, input sequence would first be fed into both draft and score models to acquire kv caches. The output of draft model at this stage is omitted. Then, T5 is fed into draft model to generate proposed T6\u0026#39;, T7\u0026#39;, and T8\u0026#39;. To verify these tokens, T5, T6\u0026#39;, T7\u0026#39; and T8\u0026#39; are fed into the score model to get T6, T7*, T8* and T9* in one forward pass. Note that here T6 must be correct because it is generated by T5 through the score model; However, T7*, T8* and T9* are not guaranteed to be correct. The final step is to verify T6\u0026#39;, T7\u0026#39; and T8\u0026#39; to see if T7*, T8* and T9* are correct. For example, if T6\u0026#39; and T7\u0026#39; is correct, then the final accepted tokens would be T6\u0026#39;, T7\u0026#39; and T8\u0026#39;, which means the socore model generates 3 tokens in one forward pass. Workflow of spuculative decoing in vLLM (k=1, top-p=1). Like the previous picture, if T6\u0026#39; is correct, then the final accepted tokens would be T6\u0026#39; and T7*, one generated by the draft model and the other by the score model. The score model generates 2 tokens in one forward pass. 2. How Speculative Decoding Works in vLLM In vLLM, speculative decoding is integrated with the system\u0026rsquo;s continuous batching architecture, where different requests are processed together in a single batch, enabling higher throughput. vLLM uses two key components to implement this:\nDraft Runner: This runner is responsible for executing the smaller proposer model to propose candidate tokens. Target Runner: The target runner verifies the tokens by running the larger scorer model. vLLM\u0026rsquo;s system is optimized to handle this process efficiently, allowing speculative decoding to work seamlessly with continuous batching, which increases the overall system performance.\nDiagram illustrating how the draft and target runners interact within the vLLM batching system. To implement speculative decoding in vLLM, two crucial components had to be modified:\nScheduler: The scheduler was adjusted to handle multiple token slots within a single forward pass, enabling the simultaneous generation and verification of several tokens. Memory Manager: The memory manager now handles the KV cache for both the draft and scorer models, ensuring smooth processing during speculative decoding. System architecture of speculative decoding in vLLM. 3. Types of Speculative Decoding Supported in vLLM 3.1. Draft Model-Based Speculative Decoding This is the most commonly used form of speculative decoding, where a smaller model predicts the next tokens, and a larger model verifies them. A common example would be using a Llama 68M model to predict tokens for a Llama 2 70B model. This approach requires careful selection of the draft model to balance accuracy and overhead.\nChoosing the correct draft model is essential for maximizing the efficiency of speculative decoding. The draft model needs to be small enough to avoid creating significant overhead but still accurate enough to provide a meaningful performance boost.\nHowever, selecting the right draft model can be challenging. For example, in models like Llama 3, finding a suitable draft model is difficult due to differences in vocabulary size. Speculative decoding requires that the draft and target models share the same vocabulary, and in some cases, this can limit the use of speculative decoding. Therefore, in the following sections, we introduce several draft-model free speculative decoding methods.\n3.2. Prompt Lookup Decoding An example of prompt lookup decoding. Given the prompt, we build all 2-grams as the lookup key. The values are the three tokens following the lookup key. During generation, we will check if the current 2-gram matches any key. If so, we will propose the following tokens with the value. Otherwise known as n-gram matching, this approach is effective for use cases like summarization and question-answering, where there is a significant overlap between the prompt and the answer. Instead of using a small model to propose tokens, the system speculates based on the information already available in the prompt. This works particularly well when the large model repeats parts of the prompt in its answers.\n4. MEDUSA 4.1. Roadmap [vllm][ISSUE] | Can vLLM support medusa head? #1023 [vllm][ISSUE] | [Discussion] Will vLLM consider using Speculative Sampling to accelerating LLM decoding? #1171 [vllm][PR] | [Speculative Decoding] Medusa Implementation with Top-1 proposer #4978 4.1. MEDUSA Heads MEDUSA heads are additional decoding heads appended to the last hidden states of the original model.\nThree heads are used to propose tokens for the following three positions. Head 1 is proposing [\u0026#34;is\u0026#34;, \u0026#34;\\\u0026#39;\u0026#34;, \u0026#34;the\u0026#34;] for the first position. Head 2 is proposing [\u0026#34;difficult\u0026#34;, \u0026#34;is\u0026#34;, \u0026#34;\\\u0026#39;\u0026#34;] for the second position. Head 3 is proposing [\u0026#34;not\u0026#34;, \u0026#34;difficult\u0026#34;, \u0026#34;a\u0026#34;] for the third position. NOTE: All heads take the output of the last transformer block as the input. Specifically, given the original model’s last hidden states $h_t$ at position $t$, we add $K$ decoding heads to $h_t$. The $k$-th head is used to predict the token in the $(t + k + 1)$-th position of the next tokens (the original language model head is used to predict the $(t + 1)$-th position).\n$$ \\begin{aligned} p_{t}^{(k)} \u0026 =\\mathrm{softmax}\\left(W_{2}^{(k)}\\cdot\\left(\\mathrm{SiLU}(W_{1}^{(k)}\\cdot h_{t})+h_{t}\\right)\\right), \\\\ \u0026 \\mathrm{where~}W_{2}^{(k)}\\in\\mathbb{R}^{d\\times V},W_{1}^{(k)}\\in\\mathbb{R}^{d\\times d}. \\end{aligned} $$Unlike a draft model, MEDUSA heads are trained in conjunction with the original backbone model, which can remain frozen during training (MEDUSA-1) or be trained together (MEDUSA-2).\n4.2. Tree Attention The top-2 predictions from the first MEDUSA head and the top-3 from the second result in a total of $2 \\times 3 = 6$ candidates. Each of these candidates corresponds to a distinct branch within the tree structure.\nTo guarantee that each token only accesses its predecessors, an attention mask is devised that exclusively permits attention flow from the current token back to its antecedent tokens.\n5. EAGLE 5.1. Roadmap [vllm][PR] | [Speculative Decoding] EAGLE Implementation with Top-1 proposer #6830 5.2. Detailed Process A comparison of the methods for drafting the fourth and fifth tokens, t4 and t5. t (represented by blue blocks) denotes tokens, and f (orange blocks) signifies the features, with subscripts indicating their positions in the sequence. The red border indicates the predictions of the draft model. For simplicity, the n in the n-gram for Lookahead, as shown in the figure, has been set to 2. This link is a Feishu drawboard to show the detailed process of speculative decoding with EAGLE in vLLM:\nSpeculative Decoding with EAGLE in vLLM 6. DeepseekMTP Structure of DeepseekMTP. This figure also demonstrates the training process of draft models, which are fed with continuous tokens and corresponding masks to predict the next tokens for each position. This process is similar to the pre-training process of the larger scorer model. Compute graph of DeepseekMTP. 7. Discussion 7.1. Performance Insights, Speedups, and Trade-offs Ref: [vllm] | How Speculative Decoding Boosts vLLM Performance by up to 2.8x Speculative decoding offers significant performance benefits in low-QPS (queries per second) environments. For example, in testing on the ShareGPT dataset, vLLM demonstrated up to a 1.5x speedup in token generation when using draft model-based speculative decoding. Similarly, prompt lookup decoding has shown speedups of up to 2.8x when applied to summarization datasets, such as CNN/DailyMail.\nPerformance comparison showing spec decode delivering up to 1.5x Speedup at QPS=1 Llama3-70B on ShareGPT with 4xH100 using draft model (turboderp/Qwama-0.5B-Instruct) and up to 2.8x Speedup at QPS=1 Llama3-70B on CNN Dailymail with 4xH100 using n-grams. However, in high-QPS environments, speculative decoding may introduce performance trade-offs. The extra compute required to propose and verify tokens can sometimes slow down the system when it is already compute-bound, as seen when the number of requests per second increases. In such cases, the overhead of speculative decoding can outweigh its benefits, leading to reduced performance.\nAs high QPS, we see 1.4x slowdown Llama3-70B on ShareGPT with 4xH100, 1.8x slowdown Llama3-70B on CNN Dailymail with 4xH100 7.2. Why exactly is batch expansion inefficient? Ref: Optimizing attention for spec decode can reduce latency / increase throughput Looking at Llama2 architecture, each component has the following algorithmic complexity wrt speculative tokens and sequence length. The baseline is non-speculative decoding, so factors such as d_model are ignored as they are the same in either case.\nEach of these scales linearly with number of speculative tokens, except for attention, which scales by num_spec_tokens * seq_len. This means that for large batch sizes and/or large speculative trees and/or large sequence lengths, attention will be the computational bottleneck.\nTo optimize the attention operation, the key is that components of the attention operation are duplicated when scoring different speculative tokens given the same prefix sequence:\nSpeaking theoretically, we can optimize attention for speculative scoring by reducing redundant QK^T computations + loads and Softmax(...)V loads:\nShare K loads for common tokens Share K*Q compute for common tokens Share V loads for common tokens We should experimentally verify this analysis: one weakness is that Softmax(...)V computation is still O(num_spec_tokens * seq_len).\nReferences [vllm] | Speculative Decoding [vllm] | How Speculative Decoding Boosts vLLM Performance by up to 2.8x [vllm] | How to Use Speculative Decoding in vLLM . [vllm][PR] | [Speculative Decoding] Medusa Implementation with Top-1 proposer #4978 A Hitchhiker\u0026#39;s Guide to Speculative Decoding [vllm] | What is lookahead scheduling in vLLM? Optimizing attention for spec decode can reduce latency / increase throughput [vllm][ISSUE] | [RFC]: Automate Speculative Decoding #4565 [HF] | Faster Assisted Generation with Dynamic Speculation ","permalink":"https://jamesnulliu.github.io/blogs/a-brief-talk-on-speculative-decoding/","summary":"A brief talk on speculative decoding in large language models.","title":"A Brief Talk on Speculative Decoding"},{"content":"1. Motivation Coding on Windows feels like a nightmare, but most games do not run on Linux, and I hate MacOS 🤢🤮 from many aspects.\nAfter trying many ways, now I firmly believe that WSL (as a Docker launcher 🤣) is the best solution if:\nYou are a programmer specifically relying on Unix environment, for example, Deep Learning, CUDA, C++, etc. You are a gamer who wants to play games on Windows. You want to switch between Linux and Windows with zero cost. 💡NOTE\nWindows Subsystem for Linux (WSL) is a feature of Windows that allows you to run a Linux environment on your Windows machine, without the need for a separate virtual machine or dual booting. WSL is designed to provide a seamless and productive experience for developers who want to use both Windows and Linux at the same time.\n2. Installation Official Doc: How to install Linux on Windows with WSL ⚠️WARNING\nIf you are using scoop on Windows, make sure uninstall it before the first-time installation of a WSL distribution. You can reinstall scoop after the installation.\nList all available WSL distributions:\n1 wsl --list --online Install a specific WSL distribution:\n1 wsl --install -d \u0026lt;distro-name\u0026gt; Run a WSL distribution:\n1 wsl -d \u0026lt;distro-name\u0026gt; 3. Change the Storage Location of a Distribution List installed WSL distributions:\n1 wsl -l -v Shutdown WSL service:\n1 wsl --shutdown Export a WSL distribution:\n1 wsl --export \u0026lt;distro-name\u0026gt; \u0026lt;path-to-exported-tar\u0026gt;.tar Unregister the target WSL distribution:\n1 wslconfig /u \u0026lt;distro-name\u0026gt; Import a WSL distribution from tar and specify the storage location; Note that you can specify any new distro name here:\n1 wsl --import \u0026lt;new-distro-name\u0026gt; \u0026lt;path-to-the-new-storage-dir\u0026gt; \u0026lt;path-to-exported-tar\u0026gt;.tar After importing, the default user of \u0026lt;new-distro-name\u0026gt; would become root. You can set the default user for a WSL distribution by modifying the /etc/wsl.conf file. See 7. Default User .\n4. Work with VSCode Run a WSL distribution:\n1 wsl -d \u0026lt;distro-name\u0026gt; Open a directory in WSL with VSCode:\n1 code \u0026lt;path-to-a-directory-in-wsl\u0026gt; 5. Install Docker in WSL I don\u0026rsquo;t like Docker Desktop for Windows. Instead, you can install Docker in WSL as you do in a normal Linux system.\nIf you want to play with cuda and deep learning in your WSL, see this blog: Docker Container with Nvidia GPU Support If you need a concise mannual for docker images and containers, see this blog: Something about Docker To open a directory inside a running container with VSCode, install extension ms-vscode-remote.remote-containers, and:\nOpen a directory in a WSL (where you installed docker and ran containers) with VSCode following 4. Work with VSCode . Press ctrl + shift + p, search for command \u0026ldquo;Dev Containers: Attach to Running Container\u0026hellip;\u0026rdquo;. Choose and click the container you want to open. That\u0026rsquo;s it. 6. Proxy Somethimes you may need to set the proxy for your subsystem.\nFrom my experience, the easiest way is to turn on System Proxy and TUN Mode in clash or v2ray on windows, and your WSL and the running containers will automatically use the proxy.\n7. Default User You can set the default user for a WSL distribution by modifying the /etc/wsl.conf file.\nFirst, login to a WSL distribution:\n1 wsl -d \u0026lt;distro-name\u0026gt; Then, create or modify the /etc/wsl.conf file:\n1 sudo vim /etc/wsl.conf Add the following lines to the file:\n1 2 [user] default=\u0026lt;your-username\u0026gt; 8. Hostname You can set the hostname for a WSL distribution by modifying the /etc/hostname file.\nFirst, login to a WSL distribution:\n1 wsl -d \u0026lt;distro-name\u0026gt; Then, create or modify the /etc/wsl.conf file:\nsudo vim /etc/wsl.conf Add the following lines to the file:\n[network] hostname = \u0026lt;your-hostname\u0026gt; 9. Compress the WSL File System Virtual disk files will grow automatically as you add files, but they will not shrink automatically when you delete files. This can lead to a large virtual disk file that takes up a lot of space on your hard drive.\nFirst, search for a .vdhx file in your computer, which is the virtual disk file for your WSL distribution. The default location is \u0026ldquo;C:/Users/\u0026lt;your-username\u0026gt;/AppData/Local/wsl/\u0026lt;hash-value\u0026gt;/ext4.vhdx\u0026rdquo;.\nThen, run the following command in PowerShell:\n1 wsl --shutdown After that, run diskpart:\n1 diskpart In the pop-up window, run the following commands:\n1 2 3 select vdisk file=\u0026#34;\u0026lt;path-to-your-vhdx-file\u0026gt;\u0026#34; compact vdisk detach vdisk ","permalink":"https://jamesnulliu.github.io/blogs/wsl-is-all-you-need/","summary":"How do I work with WSL","title":"WSL is All You Need"},{"content":"0. Introduction These days I am reading Programming Massively Parallel Processors: A Hands-on Approach, 4th Edition , and created a project to store my notes as I learn.\nOne of the most important parts in the book is writing cuda kernels, so I decided to build all kernels into shared libraries and test those implementations both in C++ and Python.\nI generated my project using this template specifically tailored for the similar scenario, but still met some problems such as conflicts when linking libtorch and gtest 🤯.\nSo the purpose of this blog is to provide a concise guide to:\nBuild a C++, CUDA and LibTorch library, test it with gtest. Load the library into torch, call the operaters in Python. Resolve problems when linking all the libraries. ⚠️WARNING\nFind some tutorials on how to use cmake and vcpkg before reading this blog.\n1. Environment and Quick Start Check README.md of the project repository.\n2. Create a C++, CUDA and LibTorch Project I put all C++ codes in \u0026ldquo; ./csrc/ \u0026rdquo; and build them with cmake. The intermediate files should be generated in \u0026ldquo;./build/\u0026rdquo; and that is just about using some command-line arguments, see this line .\nVcpkg is used to manage the dependencies of the project. I am not going to teach you how to use vcpkg in this blog, but I will mention some pitfalls I met when using it.\n😍️ I really enjoy building C++ projects with cmake and vcpkg. Have a try if you haven\u0026rsquo;t used them before.\n2.1. How to Link against LibTorch Since you have installed pytorch in 1. Environment , now you already have libtorch installed in your conda environment. Run this command, and you will get the cmake prefix path of libtorch:\n1 python -c \u0026#34;import torch;print(torch.utils.cmake_prefix_path)\u0026#34; To integrate libtorch into cmake, I create this file and this file to find libtorch in the current project and use them here .\nNow you can link your targets against libtorch simply like what I do here .\n📝NOTE\nWhen you link your target against ${TORCH_LIBRARIES}, cuda libraries are being linked automatically, which means you don\u0026rsquo;t have to find and link cuda using something like I write here 2.2. CMake and VCPKG Configuration Currently, I am planning to use the C/C++ packages listed in this file . I load the packages with these lines in \u0026#34;./csrc/CMakeLists.txt\u0026#34; . Then I link those packages to my targets here and here .\n📝NOTE\nlibtorch \u0026lt; 2.6 is compiled with _GLIBCXX_USE_CXX11_ABI=0 to use legacy ABI before C++11, which conflicts with the packages managed by vcpkg in default. Consequentially, you have to create a custom vcpkg triplet to control the behaviors when vcpkg actually build the packages. The triplet file is here and is enabled by these lines when building the C++ part.\nI also set CMAKE_CXX_SCAN_FOR_MODULES to OFF on this line because some compile errors occurs. This is a temporary solution but I am not planning to use modules from C++20 in this project, so just ignoring it.\n2.3. Write and Register Custom Torch Operators In order to register a custom torch operator, basically what you need to do next is to write a function that usually takes several torch::Tensor as input and returns a torch::Tensor as output, and then register this function to torch.\nFor example, I implement pmpp::ops::cpu::launchVecAdd in this cpp file and pmpp::ops::cuda::launchVecAdd in this cu file and provide the corresponding torch implentations pmpp::ops::cpu::vectorAddImpl and pmpp::ops::cuda::vectorAddImpl in this file .\n🤔 I didn\u0026rsquo;t add any of those function declarations in hpp files under \u0026ldquo;./include\u0026rdquo; because I don\u0026rsquo;t think they should be exposed to the users of the library. For the testing part, I will get and test the functions using torch::Dispatcher which aligns with the operaters invoked in python.\nTo register these implementations as an operater into pytorch, see this line , this line , and this line , where I:\nDefine a python function vector_add with signature: vector_add(Tensor a, Tensor b) -\u0026gt; Tensor. Register the CPU implementation of the function. Register the CUDA implementation of the function. Now vector_add is a custom torch operator which can be called in both C++ and Python. All you need to do is to build these codes into a shared library like what I did here in cmake .\n2.4. Test the Custom Torch Operators in C++ As long as a custom torch operator is registered, normally one or multiple shared libraries will be generated. For C++ users, you should link your executable target against libtorch and the generated shared libraries so that those registered operators can be called.\nSince I have linked libPmppTorchOps against libtorch as PUBLIC in this line , the test target will link against libtorch automatically as long as it links against libPmppTorchOps, see this line .\n📝NOTE\nYou may be confused about why -Wl,--no-as-needed is added before ${PROJECT_NAMESPACE}pmpp-torch-ops. This is because the shared libraries are not directly used in the test target (an operator is register in the library but not called directly in the executable), and the linker will not link against them by default. This flag will force the linker to link against the shared libraries even if they are not directly used.\nThe registered operators can be dispatched in a not-so-intuitional way 🤣 based on the official documentation, see here .\nNow the only thing is to test the operators in C++ using gtest, but this is not the focus of this blog. So let\u0026rsquo;s move on to the next part.\n3. Create and Package a Python Project 3.1. pyproject.toml and setup.py In modern python, pyproject.toml is a de-facto standard configuration file for packaging, and in this project, setuptools is used as the build backend because I believe it is the most popular one and is easy to cooperate with cmake.\nParticularly, \u0026ldquo; ./pyproject.toml \u0026rdquo; and \u0026ldquo; ./setup.py \u0026rdquo; defines what will happen when you run pip install . in the root directory of the project. I created CMakeExtention and CMakeBuild ( here ) and pass them to setup function ( here ) so that the C++ library libPmppTorchOps (under \u0026ldquo;./csrc/\u0026rdquo;) will be built and installed before installing the python package.\nYou can easily understand what I did by reading the source code of these two files, and there is one more thing I want to mention.\nBased on 2. Create a C++, CUDA and LibTorch Project, you should find that the generated shared library is under ./build/lib ending with .so on linux or .dll on windows. Additionally, I added an install procedure here which will copy the shared libraries to \u0026ldquo;./src/pmpp/_torch_ops\u0026rdquo;.\nNote that \u0026ldquo; ./src/pmpp \u0026rdquo; is already an existing directory being the root of the actual python package, and \u0026ldquo;./src/pmpp/_torch_ops\u0026rdquo; will be created automatically while installing the shared libraries.\nThe problem is, when packaging the python project, only the directory containing \u0026ldquo;__init__.py\u0026rdquo; will be considered as a package (or module), and I don\u0026rsquo;t want to add this file to \u0026ldquo;./src/pmpp/_torch_ops\u0026rdquo; due to my mysophobia 😷. Therefore, I used find_namespace_packages instead of find_packages and specified package_data to include the shared libraries here .\n3.2. Install the Python Package If you are planning to build your libraries with dependencies listed here while installing the python project, I don\u0026rsquo;t really suggest installing it in an isolated python environment (which is the default behavior of setuptools). All packages listed here have to be re-installed and in our case you need to at least append torch to that list.\nAlternatively, try this command, which will directly use the torch installed in current conda environment:\npip install --no-build-isolation -v . 3.3. Test the Custom Torch Operators in Python As long as you have the shared libraries built in 2. Create a C\u0026#43;\u0026#43;, CUDA and LibTorch Project , all you need to do is to use torch.ops.load_library to load the shared libraries and call the registered operators.\nI write this process into \u0026ldquo; src/pmpp/__init__.py \u0026rdquo;, so the time you import pmpp in python, your custom torch operators will be ready to use. See this file for an example of testing the operators.\n","permalink":"https://jamesnulliu.github.io/blogs/create-a-libtorch-project/","summary":"How to create a LibTorch project.","title":"Create A LibTorch Project"},{"content":"Nothing but my self-use configurations of vim.\nCreate a file \u0026ldquo;~/.vimrc\u0026rdquo; and write the following content:\n\u0026#34; All system-wide defaults are set in $VIMRUNTIME/debian.vim and sourced by \u0026#34; the call to :runtime you can find below. If you wish to change any of those \u0026#34; settings, you should do it in this file (/etc/vim/vimrc), since debian.vim \u0026#34; will be overwritten everytime an upgrade of the vim packages is performed. \u0026#34; It is recommended to make changes after sourcing debian.vim since it alters \u0026#34; the value of the \u0026#39;compatible\u0026#39; option. runtime! debian.vim \u0026#34; Uncomment the next line to make Vim more Vi-compatible \u0026#34; NOTE: debian.vim sets \u0026#39;nocompatible\u0026#39;. Setting \u0026#39;compatible\u0026#39; changes \u0026#34; numerous options, so any other options should be set AFTER changing \u0026#34; \u0026#39;compatible\u0026#39;. \u0026#34;set compatible \u0026#34; Vim5 and later versions support syntax highlighting. Uncommenting the next \u0026#34; line enables syntax highlighting by default. if has(\u0026#34;syntax\u0026#34;) syntax on endif \u0026#34; If using a dark background within the editing area and syntax highlighting \u0026#34; turn on this option as well \u0026#34;set background=dark \u0026#34; Uncomment the following to have Vim jump to the last position when \u0026#34; reopening a file au BufReadPost * if line(\u0026#34;\u0026#39;\\\u0026#34;\u0026#34;) \u0026gt; 1 \u0026amp;\u0026amp; line(\u0026#34;\u0026#39;\\\u0026#34;\u0026#34;) \u0026lt;= line(\u0026#34;$\u0026#34;) | exe \u0026#34;normal! g\u0026#39;\\\u0026#34;\u0026#34; | endif \u0026#34; Uncomment the following to have Vim load indentation rules and plugins \u0026#34; according to the detected filetype. \u0026#34;filetype plugin indent on \u0026#34; Source a global configuration file if available if filereadable(\u0026#34;/etc/vim/vimrc.local\u0026#34;) source /etc/vim/vimrc.local endif \u0026#34; The following are commented out as they cause vim to behave a lot \u0026#34; differently from regular Vi. They are highly recommended though. set showcmd \u0026#34; Show (partial) command in status line. set showmatch \u0026#34; Show matching brackets. set ignorecase \u0026#34; Do case insensitive matching set smartcase \u0026#34; Do smart case matching set incsearch \u0026#34; Incremental search set autowrite \u0026#34; Automatically save before commands like :next and :make set hidden \u0026#34; Hide buffers when they are abandoned set mouse=a \u0026#34; Enable mouse usage (all modes) set number set cursorline set cursorcolumn set shiftwidth=4 set tabstop=4 set expandtab set scrolloff=10 set showmode set hlsearch set autoindent set smartindent set cindent filetype indent on \u0026#34; Set the menu \u0026amp; message to English set langmenu=en_US let $LANG=\u0026#39;en_US\u0026#39; source $VIMRUNTIME/delmenu.vim source $VIMRUNTIME/menu.vim augroup numbertoggle autocmd! autocmd BufEnter,FocusGained,InsertLeave,WinEnter * if \u0026amp;nu \u0026amp;\u0026amp; mode() != \u0026#34;i\u0026#34; | set rnu | endif autocmd BufLeave,FocusLost,InsertEnter,WinLeave * if \u0026amp;nu | set nornu | endif augroup END \u0026#34; Set the menu \u0026amp; message to English set langmenu=en_US let $LANG=\u0026#39;en_US\u0026#39; source $VIMRUNTIME/delmenu.vim source $VIMRUNTIME/menu.vim ","permalink":"https://jamesnulliu.github.io/blogs/my-vimrc/","summary":"My configurations of vim.","title":"My vimrc"},{"content":" Here is my template repository of building a CMake-CXX project (with CUDA): VSC-CMake-CXX-Project-Template !\nSuppose that you are managing your project with CMake. To build an executable, first write all your build commands in a bash script. For example, create a new file \u0026ldquo;./scripts/build.sh\u0026rdquo;:\n1 2 3 build_type=$1 cmake -S . -B ./build -DCMAKE_BUILD_TYPE=$build_type cmake --build ./build -j $(nproc) Second, add the following code to \u0026ldquo;./.vscode/tasks.json\u0026rdquo; (create the file if it does not exist):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \u0026#34;version\u0026#34;: \u0026#34;2.0.0\u0026#34;, \u0026#34;tasks\u0026#34;: [ // Other tasks..., { // Task name, anything you want, must match the preLaunchTask in // launch.json \u0026#34;label\u0026#34;: \u0026#34;Build: Debug 01\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, // Command: bash \u0026lt;script\u0026gt; \u0026lt;args...\u0026gt; \u0026#34;command\u0026#34;: \u0026#34;bash\u0026#34;, \u0026#34;args\u0026#34;: [ // Your build script path \u0026#34;${workspaceFolder}/scripts/build.sh\u0026#34;, // Build script arguments \u0026#34;Debug\u0026#34; ], \u0026#34;group\u0026#34;: \u0026#34;build\u0026#34; }, // Other tasks... ] } Next, add the following code to \u0026ldquo;./.vscode/launch.json\u0026rdquo; (create the file if it does not exist):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ // Other configurations..., { // Launch configuration name, anything you want \u0026#34;name\u0026#34;: \u0026#34;Launch: Debug 01\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cppdbg\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, // Path to the generated executable \u0026#34;program\u0026#34;: \u0026#34;${workspaceFolder}/\u0026lt;path-to-generated-executable\u0026gt;\u0026#34;, // Arguments to pass to the program \u0026#34;args\u0026#34;: [ \u0026#34;arg1\u0026#34;, \u0026#34;arg2\u0026#34;, // Other arguments... ], \u0026#34;externalConsole\u0026#34;: false, \u0026#34;stopAtEntry\u0026#34;: false, // Working directory \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}\u0026#34;, // MIMode should be \u0026#34;gdb\u0026#34; for gdb, \u0026#34;lldb\u0026#34; for lldb \u0026#34;MIMode\u0026#34;: \u0026#34;gdb\u0026#34;, // Path to the gdb executable // Change this to lldb path if you are using lldb \u0026#34;miDebuggerPath\u0026#34;: \u0026#34;/usr/bin/gdb\u0026#34;, // Pre-launch task, make sure it matches the task label in // tasks.json \u0026#34;preLaunchTask\u0026#34;: \u0026#34;Build: Debug 01\u0026#34;, // Environment variables \u0026#34;environment\u0026#34;: [ // This is an example of adding a path to the PATH environment // variable { \u0026#34;name\u0026#34;: \u0026#34;PATH\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\u0026lt;some-path\u0026gt;:${env:PATH}\u0026#34; } ], \u0026#34;setupCommands\u0026#34;: [ { \u0026#34;description\u0026#34;: \u0026#34;Enable pretty-printing for gdb/lldb\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;-enable-pretty-printing\u0026#34;, \u0026#34;ignoreFailures\u0026#34;: true } ], }, // Other configurations..., ] } Finally, click on the \u0026ldquo;Run and Debug\u0026rdquo; icon on the left sidebar, choose the configuration with the name you specified in \u0026ldquo;launch.json\u0026rdquo;, then click on the green play button to start debugging.\n","permalink":"https://jamesnulliu.github.io/blogs/vscode-debug-cxx/","summary":"This post shows how to configure launch.json in VSCode for debugging C++.","title":"VSCode: Debug C++"},{"content":" Here is my template repository of building a Python project (with Pytorch and cutomized CUDA kernels): VSC-Pytorch-Project-Template !\nFirst, add the following code to \u0026ldquo;./.vscode/launch.json\u0026rdquo; (create the file if it does not exist):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ // Other configurations..., { \u0026#34;name\u0026#34;: \u0026#34;DebugPy: Current File\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;debugpy\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;console\u0026#34;: \u0026#34;integratedTerminal\u0026#34;, // Whether to jump to external code when debugging \u0026#34;justMyCode\u0026#34;: true, // Path to the Python file to debug; If set to \u0026#34;${file}\u0026#34;, it will // use the currently opened file \u0026#34;program\u0026#34;: \u0026#34;${file}\u0026#34;, // Arguments to pass to the program \u0026#34;args\u0026#34;: [ \u0026#34;\u0026lt;arg1\u0026gt;\u0026#34;, \u0026#34;\u0026lt;arg2\u0026gt;\u0026#34;, // ... ], // Environment variables \u0026#34;env\u0026#34;: { \u0026#34;\u0026lt;YOUR_ENV_VAR\u0026gt;\u0026#34;: \u0026#34;\u0026lt;VALUE\u0026gt;\u0026#34; }, }, // Other configurations..., ] } Next, click on the \u0026ldquo;Run and Debug\u0026rdquo; icon on the left sidebar, choose the configuration with the name you specified in \u0026ldquo;launch.json\u0026rdquo;, then click on the green play button to start debugging.\n","permalink":"https://jamesnulliu.github.io/blogs/vscode-debug-python/","summary":"This post shows how to configure launch.json in VSCode for debugging Python.","title":"VSCode: Debug Python"},{"content":" This blog should be a complete guide to render mathematics in Hugo. However, if you have problems reproducing this blog, note that the official documentation is always the best place to start:\nHUGO - Mathematics in Markdown Cloudfare - Use a Newer HUGO Verseion First, add following code to hugo.yaml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 params: # ... math: true # ... markup: goldmark: extensions: passthrough: delimiters: block: - - \\[ - \\] - - $$ - $$ inline: - - \\( - \\) - - $ - $ enable: true Second, create a new file layouts/partials/extend_head.html:\n{{ if .Param \u0026#34;math\u0026#34; }} {{ partialCached \u0026#34;math.html\u0026#34; . }} {{ end }} Note that name and path of the created file is based on your theme configuration.\nFor example, in theme PaperMode (which I use), \u0026ldquo;extend_head.html\u0026rdquo; indicates that, to extend the head, I can create a file named extend_head.html in ./layouts/partials/ (so-called global layouts, without modifying layouts inside the theme).\nIn other words, if your theme does not support this feature, you may need to copy the head.html from the theme to global layouts and modify it, or simply modify the theme directly (but rememenber that modifications in git submodules will not be committed to the remote repository).\nNext, create a new file layouts/partials/math.html:\n1 2 3 4 5 6 7 8 9 \u0026lt;script id=\u0026#34;MathJax-script\u0026#34; async src=\u0026#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; MathJax = { tex: { displayMath: [[\u0026#39;\\\\[\u0026#39;, \u0026#39;\\\\]\u0026#39;], [\u0026#39;$$\u0026#39;, \u0026#39;$$\u0026#39;]], // block inlineMath: [[\u0026#39;\\\\(\u0026#39;, \u0026#39;\\\\)\u0026#39;], [\u0026#39;$\u0026#39;, \u0026#39;$\u0026#39;]] // inline } }; \u0026lt;/script\u0026gt; Now, you can render both block and inline mathematics in your content files. For example, the following code renders the equation $O=Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$:\n1 2 3 $$ O=Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$ Test it locally by running hugo server -D . to see if the equation rendered correctly.\nFinally, there is one more thing to note before deploying your website.\nYou should always use the newest version of hugo to support all features (in goldmark).\nWhy? In previous versions, the passthrough extension is not supported, and consequently, \\\\ inside the equation will be rendered as \\ instead of a new line.\nBased on the official doc, if you are using Cloudflare to deploy your website:\nGo to Workers \u0026amp; Pages -\u0026gt; Your Porject -\u0026gt; Settings -\u0026gt; Variables and Secrets; Add a new variable HUGO_VERSION with value 0.135.0 at least. This will specify the version of Hugo used by Cloudflare to build your website.\n","permalink":"https://jamesnulliu.github.io/blogs/render-mathematics-in-hugo/","summary":"This post shows how to render mathematics in Hugo.","title":"Render Mathematics in Hugo"},{"content":"1. Why Attention\u0026rsquo;s $O_i$ only depends on $Q_i$ The Attention formula is:\n$$ O=Attention(Q,K,V)=softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$Assume $Q=\\begin{bmatrix}Q_0\\\\Q_1\\end{bmatrix}$, $K=\\begin{bmatrix}K_0\\\\K_1\\end{bmatrix}$\nThen:\n$$ O=softmax(\\frac{\\begin{bmatrix}Q_0K_0^T\u0026Q_0K_1^T\\\\Q_1K_0^T\u0026Q_1K_1^T\\end{bmatrix}}{\\sqrt{d_k}})V $$Let:\n$$ A=\\begin{bmatrix}A_0\\\\A_1\\end{bmatrix}=\\begin{bmatrix}Q_0K_0^T\u0026Q_0K_1^T\\\\Q_1K_0^T\u0026Q_1K_1^T\\end{bmatrix},f(x)=\\frac{softmax(x)}{\\sqrt{d_k}} $$At this point, $A_1$ only depends on $Q_1$ and is independent of $Q_0$, so:\n$$ \\begin{bmatrix}O_0\\\\O_1\\end{bmatrix}=O=\\begin{bmatrix}f(A_0)\\\\f(A_1)\\end{bmatrix}V=\\begin{bmatrix}f(A_0)V\\\\f(A_1)V\\end{bmatrix} $$Therefore, $O_i$ only depends on $A_i$, and according to the definition of $A$, $A_i$ only depends on $Q_i$, meaning:\nThe $i$-th output of the Attention matrix only depends on the $i$-th $Q$ and is independent of previous $Q$s.\nSummary:\nWhen predicting the next token, we only need to calculate the corresponding Q_new for the new token and perform attention calculation with the previously cached K_cache and V_cache. The new K_new and V_new will be added to the cache to provide the foundation for the next token generation. This process avoids repeated calculations for all historical tokens, greatly improving efficiency. 2. KV Cache Incremental Process Example code:\n\u0026ldquo;Learning-Programming-Massively-Parallel-Processors/src/pmpp/models/attention.py\u0026rdquo;\n2.1. Prefilling: Initial Input (Complete Sequence) Calculation For the initial input sequence (seq_len, vocab_size), we obtain Q, K, and V through linear transformations, all with shape (seq_len, embed_dim) (see this). Using Q and K to calculate attention scores through dot product, then combining with V to compute the output (seq_len, embed_dim) (see this), this is the first complete calculation for the initial sequence. 2.2. Decoding: Incremental Calculation When Predicting Next Token: When predicting the next token, there\u0026rsquo;s no need to perform complete Q, K, V calculations for the entire sequence. Instead, only an incremental calculation for the newly generated token is required. The process is as follows:\nInput New Token: Take the generated token from last round as input sequence, obtain Q_new, K_new and V_new through linear transformation. Update KV Cache: K_new and V_new are added to the end of K_cache and V_cache, making them a pair of (kv_len + 1, embed_dim) vectors. Attention Calculation with Updated K_cache and V_cache: Use Q_new to perform attention calculation with updated K_cache and V_cache. Q_new can directly perform dot product with K_cache to get attention scores, then combine with V_cache to get new output. Output: The output after attention calculation has shape (1, embed_dim), which is the newly generated token. 3. Paged Attention in vllm 3.1. Motivation: Memory Wastes The above figure shows possible memory waste scenarios. The main issue is that we don\u0026rsquo;t know where the EOS (end of sequence) token is. Random memory allocation may lead to significant memory fragmentation, resulting in reduced throughput.\n3.2. Solution: Managing Caches with Pages The above figure demonstrates how vLLM manages memory using Paged Attention.\nIn simple terms, before inference begins, vLLM allocates two long Tensors (k_cache and v_cache) for each Decoder Layer, dividing these Tensors into continuous equal-length PA blocks (each row in the figure represents one PA Block). Each PA Block can store K or V cache for BLOCK_SIZE tokens (each token\u0026rsquo;s shape can be recognized as (num_heads, head_size)).\nTherefore, the shapes of k_cache and v_cache can be recognized as (num_blocks, block_size, num_heads, head_size).\nFor a continuous sequence, PA blocks are allocated before the prefilling stage, and during inference:\nWhen computing prompt attention, the input K and V are first stored in k_cache and v_cache according to PA blocks; then attention is calculated using the entire QKV. When computing new tokens, Q and the block table are used to calculate attention during the decode phase; at this point, the memory access is to the PA blocks in k_cache and v_cache. 5. Paged Attention Kernel in Details References:\nvLLM Paged Attention vLLM皇冠上的明珠：深入浅出理解PagedAttention CUDA实现 The general structure of the Paged Attention kernel is as follows:\n5.1. 输入输出输出分析和参数说明 // Grid: (num_heads, num_seqs, 1). template\u0026lt; typename scalar_t, int HEAD_SIZE, int BLOCK_SIZE, int NUM_THREADS, int PARTITION_SIZE = 0\u0026gt; __device__ void paged_attention_kernel( ... // Other side args. const scalar_t* __restrict__ out, // [num_seqs, num_heads, max_num_partitions, head_size] const scalar_t* __restrict__ q, // [num_seqs, num_heads, head_size] const scalar_t* __restrict__ k_cache, // [num_blocks, num_kv_heads, head_size/x, block_size, x] const scalar_t* __restrict__ v_cache, // [num_blocks, num_kv_heads, head_size, block_size] ... // Other side args. ) 模板参数说明:\nscalar_t 元素类型 (实际代码中还有 cache_t 表示 KV cache 的元素类型). HEAD_SIZE 每个 head 中元素数量. BLOCK_SIZE 每个 PA block 中的 token 数量. KV cache 被存储在不同 PA blocks. 每个 PA block 存储一个 head 中 BLOCK_SIZE 个 token.\n例如, 若 BLOCK_SIZE=16, HEAD_SIZE=128, 则一个 PA block 能存储一个 head 的 16 * 128 = 2048 个元素. 每个 PA block 可能只包含一部分的 context tokens. 从 page 角度看, KV cache 是若干个 page 的集合; NUM_THREADS 每个 CUDA thread block 中 thread 的数量. PARTITION_SIZE 参与 TP 的 GPU 数量, 默认 0 表示单卡. (以下都以单卡为例说明) 额外的一些参数:\nnum_seqs: 本次推理请求 sequence 数目. 由于这个 kernel 只处理 decode 阶段单 query attention, 所以实际上每个 sequence 只有一个 query token.\nnum_heads: Q 的 head 数目 num_kv_heads: KV 的 head 数目, 对于 MHA 其值和 num_heads 相同; 如果是 GQA, MQA 则 num_kv_heads 小于 num_head. head_size: 即 HEAD_SIZE k_cache: (num_blocks, num_kv_heads, head_size/x, block_size, x), 其中 x 表示 THREAD_GROUP_SIZE * VEC_SIZE 的大小 (后面会细说). 下面结合 GPU architecture 初步分析一下参数.\n🧐 为什么要分 thread group?\n因为当一个 cuda block 要取的数据比较少的时候 (计算 QK), 一个 thread group 分别一次取 Q 和 K 中 16B; 当一个 cuda block 要取的数据比较多的时候 (计算 LV), 一个 thread 取 16B. 5.2.Shared Memory: q_vecs 的写入 从 kernel 中的第一个申请的 shared memory 开始说.\n关于 shared memeory:\n在 kernel 中申请的 shared memory 被当前 cuda block 中的所有 thread 共享. shared memory 的作用是为了减少 global memory 的访问次数，提高访存效率. 以下代码申请了一块 shared memroy 被整个 CUDA Block 中所有 kernel 共享:\n__shared__ Q_vec q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD]; 首先, q_vecs 覆盖了 Q 中 head_size 个元素 - 这也是一个 cuda block 需要处理的数据量.\n接着再说两个维度的参数的意思:\nconstexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE; constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE; THREAD_GROUP_SIZE: 每个 thread group 中的 thread 数量. 注意, 一个 cuda block 中有 NUM_THREADS 个 thread, NUM_THREAD_GROUPS 个 thread group. THREAD_GROUP_SIZE = MAX(WARP_SIZE/BLOCK_SIZE, 1). NUM_VECS_PER_THREAD: HEAD_SIZE 能被分成多少个 16B. (这个变量这么命名的理由是后面读取 K 的时候每个 thread 会往自己的寄存器内读 NUM_VECS_PER_THREAD 个 k_vec.) 证明: q_vecs 覆盖 Q 的一个 head, 并且 NUM_VECS_PER_THREAD 表示 Q 的一个 head 被分成多少个 16B.\n=\u0026gt; THREAD_GROUP_SIZE * VEC_SIZE = 16B / sizeof(scalar_t);\n=\u0026gt; NUM_VECS_PER_THREAD * 16B / sizeof(scalar_t) = HEAD_SIZE;\n然后看 load Q 的代码, 建议结合下面的图一起看:\n// Load Q to shmem #pragma unroll for (int i = thread_group_idx; i \u0026lt; NUM_VECS_PER_THREAD; i += NUM_THREAD_GROUPS) { const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE; q_vecs[thread_group_offset][i] = *reinterpret_cast\u0026lt;const Q_vec*\u0026gt;(q_ptr + vec_idx * VEC_SIZE); } thread_group_idx 表示当前 thread 属于当前 cuda block 中第几个 thread group. thread_group_offset 表示当前 thread 在当前 thread group 中是第几个 thread. 上图展示了循环具体是怎么跑的.\n一个紫色箭头表示一个 thread group. NUM_VECS_PER_THREAD 表示 HEAD_SIZE 能被分成多少个 16B. 实际读取 Q 的内存时, 所有 thread group 从 Q 的起始位置紧密排列, 根据图上看的话一共有 NUM_THREAD_GROUPS 个紫色箭头. 所有 thread group 读取一次 Q 并存入 q_vecs 对应循环中的一次迭代; 因此下次迭代 thread group 需要向后偏移 NUM_THREAD_GROUPS 个位置 (例如 i 从 1 变为 7). 此外, 读一次 16B 对应一个 thread 来说自然也是取一个 VEC. 对应到 kernel 编写, 还需要计算当前 thread 具体读取哪个 vec; 因此得到 vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE. 🤔 这里会不会有 bank conflict?\n总之现在我们把 (1, head_size) 大小的元素读到了 cuda block 共享的 shared memory q_vecs 中.\n5.3. 读取 K Cache 并计算 QK 现在从 cuda block 的角度看, 当前 block 已经获得了自己要算的 Q 中的一个 head (形状为 (1, head_size)), 接下来就是计算 Q 和 K 的点积.\n点积过程是把当前 block 拥有的 Q head 和整个 K Cache (迭代地) 进行点积运算. 参考下图:\nQK 乘积实际上被暂存在 logits (也是一块 shared memory) 中, 之后会被用来计算 softmax.\n😇 看下循环的具体代码吧:\n// Loop 1 for (int block_idx = start_block_idx + warp_idx; block_idx \u0026lt; end_block_idx; block_idx += NUM_WARPS) { // Physical block calculation ... // Loop 2 for (int i = 0; i \u0026lt; NUM_TOKENS_PER_THREAD_GROUP; i++) { // Offset calculation ... K_vec k_vecs[NUM_VECS_PER_THREAD]; // Loop 3 #pragma unroll for (int j = 0; j \u0026lt; NUM_VECS_PER_THREAD; j++) { // Load K to `k_vecs` ... } float qk = scale * Qk_dot\u0026lt;scalar_t, THREAD_GROUP_SIZE\u0026gt;::dot( q_vecs[thread_group_offset], k_vecs); // Add the ALiBi bias if slopes are given. qk += (alibi_slope != 0) ? alibi_slope * (token_idx - seq_len + 1) : 0; if (thread_group_offset == 0) { // Store the partial reductions to shared memory. // Mask // Update the max value. } } } 先说第一个循环, 其中比较重要的几个参数定义如下:\n// [start_block_idx, end_block_idx) is the range of blocks to process. const int start_block_idx = USE_PARTITIONING ? partition_idx * num_blocks_per_partition : 0; // If not using partitioning, `end_block_idx` should be equal to `num_seq_blocks`. const int end_block_idx = MIN(start_block_idx + num_blocks_per_partition, num_seq_blocks); // Number of blocks to process. const int num_blocks = end_block_idx - start_block_idx; 用文字描述就是:\nblk_idx 表示当前 thread 所在 warp 需要处理的 PA block 的在 block_table 中索引 (逻辑上的索引). start_block_idx 和 end_block_idx 表示当前 cuda block 需要处理的 block 范围. num_blocks 表示当前 cuda block 需要处理的 block 数量. NUM_WARPS 表示当前 cuda block 中 warp 的数量. 一个 warp 包含 32 个 thread. warp_idx 表示当前 warp 在当前 cuda block 中的索引. 说人话就是每个 warp 处理一个 PA block, 一开始 cuda block 中的所有 warp 紧密地指向最前面的 NUM_WARPS 个 PA block, 每次循环所有 warp 向后偏移 NUM_WARPS 个 PA block 的长度. 参考下图:\n🔔 这里再回顾一下, 一个 PA block 里存放了 BLOCK_SIZE 个 token 的 K 或 V cache.\n所以说这个循环和上面读取 Q 的循环一个尿性🤮, 不过是以 warp 的粒度处理数据;\n进入了第一个循环内部, 第一步当然是计算当前 thread 对应的 warp 应该计算哪个 PA block (物理上的索引), 因此得到了 physical_block_number:\nconst int64_t physical_block_number = static_cast\u0026lt;int64_t\u0026gt;(block_table[block_idx]); 然后解释第二个循环, 第二个循环的整体目标就是让当前 warp 计算好自己负责的 PA block 中 BLOCK_SIZE 个 token 的 QK 乘积.\n先看一下 i 的上界:\nconstexpr int NUM_TOKENS_PER_THREAD_GROUP = DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE); // ... // Loop 1 for (int block_idx = start_block_idx + warp_idx; block_idx \u0026lt; end_block_idx; block_idx += NUM_WARPS) { // Loop 2 for (int i = 0; i \u0026lt; NUM_TOKENS_PER_THREAD_GROUP; i++) { // ... } // ... } 从 kernel 角度看, 每个 thread 需要辅助当前 warp 计算自己负责的一整个 PA block (包含 BLOCK_SIZE 个 token), 而我们把这个过程拆分为 Loop 2 中的 NUM_TOKEN_PER_THREAD_GROUP (也就是 ceil(BLOCK_SIZE / 32)) 次循环;\n说人话就是一个 thread group 对应一个 token 中的一个 head, 如果 BLOCK SIZE 太大了后面每个 thread 向后偏移 i * WARP_SIZE 个 token 继续狠狠算🤣.\n也因此第二个循环内部一上来先计算了几个偏移量, 并且申请了 thread 内部私有的 k_vecs 数组:\nconst int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE; const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset; K_vec k_vecs[NUM_VECS_PER_THREAD]; thread_group_idx 表示当前 thread group 在整个 cuda block 中的索引. ☢️ 一个 thread group 在一次循环中负责 fetch 一个 PA block 中 K cache 的一个 token 中自己负责的 head. ☢️ 一个 thread group 负责计算一个 qk 值; 这个值显然是由一个 Q head 和一个 K head 点积得到的. physical_block_offset 表示当前要算的 token 在当前 PA block 中的偏移量 (注意和前面的 physical_block_number 区分). 加 i * WARP_SIZE 的原因是如果 BLOCK_SIZE 大于 32, 那么一个 warp 要多次循环才能处理完一个 PA block 中的所有 token, 对应 thread_group_idx 需要做偏移. token_idx 表示当前要算的 token 在整个 seq 的 KV cache 中的索引. k_vecs 中能存放 NUM_VECS_PER_THREAD 个 VEC, 而一整个 thread group 中所有的 thread 的 k_vecs 合起来才能组成一个 K 的 head (推导参考上面 Q 的 😇). 这就是为什么后面算 QK 的时候要 reduce. 🤔 看到这里读者可能有一个问题: 一个 token 的 K cache 应该对应多个 head, 为什么上面说一个 thread group 只负责一个 head?\n答: 因为实际计算的时候, 一个 cuda block 只负责计算一个 head, 对应到 K Cache 乃至后面 V Cache 的位置也是一样的.\n这里额外说一下, 读 K 的 head 的一个目标应该是在尽量少的 register 中装下一个 head 的所有元素, 这样后续和 shared memory 中的 Q 做点乘并规约的速度更快. 假设一个 head 有 128 个 float16, 则占用 256B, 而 A100 中一个 thread 最多能有 255 个 32-bit register (也就是 1020B), 此时可以认为一个 thread 能装下一个 head 的所有元素.\n但是由于目前 PA kernel 在 BLOCK_SIZE 为 16 的情况下 THREAD_GROUP_SIZE 等于 2, 因此一个 thread 只会装一个 head 的一半元素, 这样可能会导致 register 的使用率不高.\n接着进入第三个循环, 目的是让 thread group 从 K cache 中读一个 head, 并存入 k_vecs 中:\n// x == THREAD_GROUP_SIZE * VEC_SIZE // Each thread group fetches x elements from the key at a time. constexpr int x = 16 / sizeof(cache_t); //... // Loop 1 for (int block_idx = start_block_idx + warp_idx; block_idx \u0026lt; end_block_idx; block_idx += NUM_WARPS) { // Loop 2 for (int i = 0; i \u0026lt; NUM_TOKENS_PER_THREAD_GROUP; i++) { K_vec k_vecs[NUM_VECS_PER_THREAD]; // Loop 3 for (int j = 0; j \u0026lt; NUM_VECS_PER_THREAD; j++) { const cache_t* k_ptr = k_cache + physical_block_number * kv_block_stride + kv_head_idx * kv_head_stride + physical_block_offset * x; const int vec_idx = thread_group_offset + j * THREAD_GROUP_SIZE; const int offset1 = (vec_idx * VEC_SIZE) / x; const int offset2 = (vec_idx * VEC_SIZE) % x; // if Fp8KVCacheDataType::kAuto k_vecs[j] = *reinterpret_cast\u0026lt;const K_vec*\u0026gt;( k_ptr + offset1 * BLOCK_SIZE * x + offset2); } // ... } // ... } 老规矩, 先看 j, 本质就是从 0 迭代到 NUM_VECS_PER_THREAD, 每次迭代当前 thread 读取一个 VEC 存入 k_vecs 中.\n🔔 回顾:\nNUM_VECS_PER_THREAD 表示一个 head 被分成多少个 16B. k_cache 的 shape 为 (num_blocks, num_kv_heads, head_size/x, block_size, x). 其中的 x 表示一个 thread group 需要读取的元素数量 (VEC_SIZE * THREAD_GROUP_SIZE); 因此作者将 K Cache 的 layout 的最后一维设置为 x 其实也是方便后续 thread group 对 K cache 的读取.\n下图具体展示了寻址的过程:\n其中:\n在 MHSA 中, num_kv_heads 等于 num_heads; 而在 GQA, MQA 中, num_kv_heads 小于 num_heads. (1) 负责找到当前 thread 属于的 warp 要处理哪个 PA block. (2) 负责找到当前 thread 要计算的 head 在 K cache 中的位置. 这个 head 的索引和 Q 中 head 的索引在 MHSA 中相同. (3) 负责找到当前 thread group 要计算的 token 在当前 PA block 中的位置. (5) 负责找到当前 thread 在需要读取的 head (蓝色长方体) 中 x 的偏移, 通过 j 进行迭代读取. 每次循环 thread group 中的所有 thread 取一个 x. (6) 负责找到当前 thread 在 thread gruop 中读取的 x 中 VEC 的偏移; thread 一次读取一个 VEC. 🤔 为什么 (5) 在实际寻址时需要 * BLOCK_SIZE * x ?\n答: 这是根据 k_cache 的 layout 得到的 stride. 同理 (3) * x 也是 stride.\n第 3 个循环结束时当前 warp 负责的每个 token 中需要的 K cache head 已经全被加载入 thread 本地的 k_vecs 中了.\n由于一个 thread group 的 k_vecs 才能真正组成一个 head, 在退回第二个循环进行 QK dot 的时候, 需要做个 reduction, 具体的范围就是 THREAD_GROUP_SIZE 个 thread:\n// Loop 1 for (int block_idx = start_block_idx + warp_idx; block_idx \u0026lt; end_block_idx; block_idx += NUM_WARPS) { // Loop 2 for (int i = 0; i \u0026lt; NUM_TOKENS_PER_THREAD_GROUP; i++) { K_vec k_vecs[NUM_VECS_PER_THREAD]; // Loop 3 for (int j = 0; j \u0026lt; NUM_VECS_PER_THREAD; j++) { // ... } float qk = scale * Qk_dot\u0026lt;scalar_t, THREAD_GROUP_SIZE\u0026gt;::dot( q_vecs[thread_group_offset], k_vecs); } // ... } 计算完 qk 后, 由当前 thread group 中第一个 (offset 为 0) 的 thread 对自己刚才算出来的 qk 进行 mask, 顺便看看如果没有 mask 掉, 把 qk_max 赋值为 qk:\nif (thread_group_offset == 0) { // Store the partial reductions to shared memory. // NOTE(woosuk): It is required to zero out the masked logits. const bool mask = token_idx \u0026gt;= seq_len; logits[token_idx - start_token_idx] = mask ? 0.f : qk; // Update the max value. qk_max = mask ? qk_max : fmaxf(qk_max, qk); } 🧐 为什么要做 mask?\n因为一个 seq 的最后一个 PA block 可能覆盖不满 BLOCK_SIZE 个 token. 这里的 mask 就是把那部分 qk 置零. 5.4. Softmax 我勒个 QK 啊, 总算算完了, 锐克 five 都要被抽清仓了. 页意丁真, 鉴定为开算 softmax.\n主要步骤就是广播然后算, 算 softmax 需要知道每个 head 对应的 qk 的最大值. 由于一个 cuda block 负责的就是一个 head, 对于这个 head 上面的计算步骤一共算了 cache_len个 token 的 qk, 因此需要做一个 cuda block 范围的规约, 找到其中最大的 qk 值.\n先在 warp 层面规约.\n__shared__ float red_smem[2 * NUM_WARPS]; // ... // Perform reduction across the threads in the same warp to get the // max qk value for each \u0026#34;warp\u0026#34; (not across the thread block yet). // The 0-th thread of each thread group already has its max qk value. #pragma unroll for (int mask = WARP_SIZE / 2; mask \u0026gt;= THREAD_GROUP_SIZE; mask /= 2) { qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask)); } if (lane == 0) { red_smem[warp_idx] = qk_max; } __syncthreads(); red_smem 是之前申请的 shared memory. VLLM_SHFL_XOR_SYNC 是一个 warp 内的 shuffle 操作, 具体来说, 在每次循环时, 每个 thread 和自己相距 mask 位置的线程交换数据 (交换来的数据通过 fmaxf 比较), 并且 mask 会逐渐减半, 直到 THREAD_GROUP_SIZE 为止. lane 表示当前 warp 中的线程索引. 接着再对每个 warp 的最大值进行规约, 由于每个 warp 的最大值都被存入了 red_smem 中, 所以只需要再次进行 shuffle 操作即可.\n// TODO(woosuk): Refactor this part. // Get the max qk value for the sequence. qk_max = lane \u0026lt; NUM_WARPS ? red_smem[lane] : -FLT_MAX; #pragma unroll for (int mask = NUM_WARPS / 2; mask \u0026gt;= 1; mask /= 2) { qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask)); } 此时, 第 1 个线程的 qk_max 就是当前 cuda block 中所有 warp 中最大的 qk 值. 将其广播给所有线程:\n// Broadcast the max qk value to all threads. qk_max = VLLM_SHFL_SYNC(qk_max, 0); 在获得了 qk_max 后, 就可以计算 softmax 了:\n// Get the sum of the exp values. float exp_sum = 0.f; for (int i = thread_idx; i \u0026lt; num_tokens; i += NUM_THREADS) { float val = __expf(logits[i] - qk_max); logits[i] = val; exp_sum += val; } exp_sum = block_sum\u0026lt;NUM_WARPS\u0026gt;(\u0026amp;red_smem[NUM_WARPS], exp_sum); // Compute softmax. const float inv_sum = __fdividef(1.f, exp_sum + 1e-6f); for (int i = thread_idx; i \u0026lt; num_tokens; i += NUM_THREADS) { logits[i] *= inv_sum; } __syncthreads(); 5.5. LV (Logits * Value) 上图展示了 LV 的计算过程, 主要区别是由于要计算 Logits 的 shape 可以表示为 (num_heads, num_seqs, cache_len), 而 V 的 shape 可以表示为 (num_heads, cache_len, head_size), 因此 LV 的矩阵乘法中, 每计算一个元素需要读取 logits 的一行和 V 的一列进行计算.\n此时, 一个 cuda block 的职责从 \u0026ldquo;自 Q 中读取一个 head\u0026rdquo; 转变为 \u0026ldquo;计算 output 中的一个 head\u0026rdquo;.\n🧐 为什么在计算 LV 时, 去掉了 thread group 的概念, 每个 thread 都被设定为每次读取 16B?\n因为现在每计算一个元素, 需要的访存量更大, 因此给每个 thread 分配了更多的数据读取量. 也就是说, V_VEC_SIZE 比 VEC_SIZE 更大. 由于 cuda 访存模式按行读取更快, 所以实际的计算结果在遍历 PA block 时线程内部利用 accs 进行累计 (以实现与 V 的一列进行计算的行为):\nconstexpr int NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE; constexpr int NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW; constexpr int NUM_ROWS_PER_THREAD = DIVIDE_ROUND_UP(HEAD_SIZE, NUM_ROWS_PER_ITER); // NOTE(woosuk): We use FP32 for the accumulator for better accuracy. float accs[NUM_ROWS_PER_THREAD]; for (int block_idx = start_block_idx + warp_idx; block_idx \u0026lt; end_block_idx; block_idx += NUM_WARPS) { V_vec v_vec; // ... for (int i = 0; i \u0026lt; NUM_ROWS_PER_THREAD; i++) { // ... for (int j = 0; j \u0026lt; V_VEC_SIZE; j++) { // Load V to `v_vec` ... v_vec_ptr[j] = token_idx + j \u0026lt; seq_len ? v_vec_ptr[j] : zero_value; } // Accumulate the dot product. accs[i] += dot(logits_vec, v_vec); } } 由于每个线程负责的累计部分不满一整行/列, 所以进行规约:\n// Perform reduction within each warp. #pragma unroll for (int i = 0; i \u0026lt; NUM_ROWS_PER_THREAD; i++) { float acc = accs[i]; #pragma unroll for (int mask = NUM_V_VECS_PER_ROW / 2; mask \u0026gt;= 1; mask /= 2) { acc += VLLM_SHFL_XOR_SYNC(acc, mask); } accs[i] = acc; } // NOTE(woosuk): A barrier is required because the shared memory space for // logits is reused for the output. __syncthreads(); // Perform reduction across warps. float* out_smem = reinterpret_cast\u0026lt;float*\u0026gt;(shared_mem); #pragma unroll for (int i = NUM_WARPS; i \u0026gt; 1; i /= 2) { int mid = i / 2; // Upper warps write to shared memory. if (warp_idx \u0026gt;= mid \u0026amp;\u0026amp; warp_idx \u0026lt; i) { float* dst = \u0026amp;out_smem[(warp_idx - mid) * HEAD_SIZE]; #pragma unroll for (int i = 0; i \u0026lt; NUM_ROWS_PER_THREAD; i++) { const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER; if (row_idx \u0026lt; HEAD_SIZE \u0026amp;\u0026amp; lane % NUM_V_VECS_PER_ROW == 0) { dst[row_idx] = accs[i]; } } } __syncthreads(); // Lower warps update the output. if (warp_idx \u0026lt; mid) { const float* src = \u0026amp;out_smem[warp_idx * HEAD_SIZE]; #pragma unroll for (int i = 0; i \u0026lt; NUM_ROWS_PER_THREAD; i++) { const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER; if (row_idx \u0026lt; HEAD_SIZE \u0026amp;\u0026amp; lane % NUM_V_VECS_PER_ROW == 0) { accs[i] += src[row_idx]; } } } __syncthreads(); } 最后写入到输出中:\n// Write the final output. if (warp_idx == 0) { scalar_t* out_ptr = out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE + head_idx * max_num_partitions * HEAD_SIZE + partition_idx * HEAD_SIZE; #pragma unroll for (int i = 0; i \u0026lt; NUM_ROWS_PER_THREAD; i++) { const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER; if (row_idx \u0026lt; HEAD_SIZE \u0026amp;\u0026amp; lane % NUM_V_VECS_PER_ROW == 0) { from_float(*(out_ptr + row_idx), accs[i]); } } } ","permalink":"https://jamesnulliu.github.io/blogs/dive-into-paged-attention/","summary":"Dive into the paged attention mechanism of vLLM.","title":"Dive into Paged Attention"},{"content":" Reference: https://www.jeremykun.com/2023/08/10/mlir-running-and-testing-a-lowering/\nNote: Check Setup the Environment of MLIR for the environment setup.\n1. Implementing a Lowering Create a file \u0026ldquo;ctlz.mlir\u0026rdquo;:\nfunc.func @main(%arg0: i32) -\u0026gt; i32 { %0 = math.ctlz %arg0 : i32 func.return %0 : i32 } Lower the math.ctlz operation to the llvm.ctlz operation with mlir-opt:\nmlir-opt --convert-math-to-funcs=convert-ctlz ./ctlz.mlir ","permalink":"https://jamesnulliu.github.io/blogs/learning-notes-mlir/mlir-tutorial-01-running-and-testing-a-lowering/","summary":"My learning notes of MLIR.","title":"MLIR Tutorial 01 | Running and Testing a Lowering"},{"content":" Offical Docs:\nInstall Docker Engine on Ubuntu Install Docker Engine on CentOS Installing the NVIDIA Container Toolkit 1. Installation 1.1. Uninstall Docker For Ubuntu/Debian:\n1 2 3 4 5 6 7 # Uninstall old versions sudo apt-get remove docker.io docker-doc docker-compose docker-compose-v2 \\ podman-docker containerd runc # Uninstall docker engine sudo apt-get purge docker-ce docker-ce-cli containerd.io docker-buildx-plugin \\ docker-compose-plugin docker-ce-rootless-extras For CentOS/RHEL:\n1 2 3 4 5 6 7 8 9 10 # Uninstall old versions sudo yum remove docker docker-client docker-client-latest docker-common \\ docker-latest docker-latest-logrotate docker-logrotate docker-engine # Uninstall docker engine sudo yum remove docker-ce docker-ce-cli containerd.io docker-buildx-plugin \\ docker-compose-plugin docker-ce-rootless-extras sudo rm -rf /var/lib/docker sudo rm -rf /var/lib/containerd 1.2. Install Docker For Ubuntu/Debian:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Add Docker\u0026#39;s official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg \\ -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # Add the repository to Apt sources: echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;) stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null # Install Docker Engine: sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io \\ docker-buildx-plugin docker-compose-plugin # Enable and start the Docker service: sudo systemctl enable docker sudo systemctl start docker For CentOS/RHEL:\n1 2 3 4 5 6 7 8 sudo yum install -y yum-utils sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin \\ docker-compose-plugin sudo systemctl enable docker sudo systemctl start docker 1.3. Install Nvidia Container Toolkit For Ubuntu/Debian:\n1 2 3 4 5 6 7 8 9 10 11 12 13 curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ \u0026amp;\u0026amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\ sed \u0026#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g\u0026#39; | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list # Optionally, configure the repository to use experimental packages: sed -i -e \u0026#39;/experimental/ s/^#//g\u0026#39; /etc/apt/sources.list.d/nvidia-container-toolkit.list sudo apt-get update sudo apt-get install -y nvidia-container-toolkit sudo systemctl restart docker For CentOS/RHEL:\n1 2 3 4 5 6 7 8 9 curl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo | \\ sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo # Optionally, configure the repository to use experimental packages: sudo yum-config-manager --enable nvidia-container-toolkit-experimental sudo yum install -y nvidia-container-toolkit sudo systemctl restart docker 2. Create a Container Choose a base image that supports Nvidia GPU in doker hub of nvidia/cuda, run the following command to create a container:\n1 2 3 4 5 6 7 8 9 docker run \\ -it \\ --gpus all \\ --name \u0026lt;container_name\u0026gt; \\ -v $HOME/data:/root/data \\ -p \u0026lt;host_port\u0026gt;:\u0026lt;container_port\u0026gt; \\ --entrypoint /bin/bash \\ --shm-size \u0026lt;shm-size\u0026gt;G \\ \u0026lt;image_name\u0026gt;:\u0026lt;tag\u0026gt; If you need a concise mannual for docker images and containers, see this blog: Something about Docker\n","permalink":"https://jamesnulliu.github.io/blogs/docker-container-with-nvidia-gpu-support/","summary":"How to create a Docker container with Nvidia GPU support.","title":"Docker Container with Nvidia GPU Support"},{"content":" Check this blog if you want a Docker Container with Nvidia GPU Support.\n1. About Image 1.1. Check Existing Images and Statistics 1 2 3 4 # List all images docker images # Check disk usage of docker (images, conatiners) docker system df To list all images not being used by any container, run:\n1 2 3 4 5 docker images --filter \u0026#34;dangling=false\u0026#34; -q | xargs -r docker inspect --format \u0026#39;{{ .Id }} {{ .RepoTags }}\u0026#39; | grep -v \u0026#39;\u0026lt;none\u0026gt;\u0026#39; | while read id tags; do if ! docker ps -a --format \u0026#39;{{.Image}}\u0026#39; | grep -q \u0026#34;$(echo $tags | cut -d\u0026#39;[\u0026#39; -f2 | cut -d\u0026#39;]\u0026#39; -f1)\u0026#34;; then echo \u0026#34;$id $tags\u0026#34; fi done 1.2. Rename an Image 1 docker tag \u0026lt;image-id\u0026gt; \u0026lt;image-name\u0026gt;:\u0026lt;image-tag\u0026gt; 1.3. Pull an Image Search for an image in Docker Hub.\nPull the image by running:\n1 docker pull \u0026lt;image-name\u0026gt;:\u0026lt;image-tag\u0026gt; Or download the layers to \u0026lt;dir-path\u0026gt; by using this script:\n1 2 3 4 5 6 mkdir -p \u0026lt;dir-path\u0026gt; # Directory to store the layers bash download-frozen-image-v2.sh \u0026lt;dir-path\u0026gt; \u0026lt;image-name\u0026gt;:\u0026lt;image-tag\u0026gt; # Load the image tar -cC \u0026lt;dir-path\u0026gt; . | docker load 1.4. Build an Image Use a Dockerfile to build a new image based on an existing image.\nCheck this example project: jamesnulliu/deeplearning-docker-build .\nThe following command in ./scripts/build.sh would build image \u0026lt;image-name\u0026gt;:\u0026lt;image-tag\u0026gt;:\n1 2 3 4 docker build \\ -f Dockerfile \\ -t jamesnulliu/deeplearning:torch2.6-cuda12.6-ubuntu24.04 \\ . Note that . specifiles the build context, where docker can reference (e.g., COPY and ADD) the containing files during the build process. You can change it to other directory path according to your needs.\nYou will find the new image after building by runing docker images.\n1.5. Save an Image to a tar File To export an image to a tar file with reusable layer imformations, run the following command:\n1 2 3 docker save -o \u0026lt;some-name\u0026gt;.tar \u0026lt;image-name\u0026gt;:\u0026lt;image-tag\u0026gt; # Or docker save -o \u0026lt;some-name\u0026gt;.tar \u0026lt;image-id\u0026gt; See 1.6. Load an Image to load the generated tar file to an image.\n1.6. Load an Image Suppose you have a tar file \u0026lt;some-name\u0026gt;.tar, if the file is generated by docker save (see 1.5. Save an Image to a tar File), load the image by running:\n1 docker load -i \u0026lt;some-name\u0026gt;.tar Or if the file is generated by docker export (see 2.4 Export a container to a tar File), load the container to an image by running:\n1 docker import \u0026lt;some-name\u0026gt;.tar \u0026lt;image-name\u0026gt;:\u0026lt;image-tag\u0026gt; 1.7. Remove an Image Before removing an image, you need to:\nStop containers using the image: docker stop \u0026lt;container-id\u0026gt;. Remove containers using the image: docker rm \u0026lt;container-id\u0026gt;. Then remove the image:\n1 2 3 docker rmi \u0026lt;image-name\u0026gt;:\u0026lt;image-tag\u0026gt; # Or docker rmi \u0026lt;image-id\u0026gt; 2. About Container 2.1. Check Existing Containers and Statistics 1 2 3 4 5 6 # List running/[all] containers docker ps [-a] # Check statistics of all/[some] containers docker stats [\u0026lt;container-id\u0026gt;] # Check disk usage of docker (images, conatiners) docker system df 2.2. Create a Container Basic Parameters:\nClick to expand -it: Interactive mode with pseudo-TTY terminal --name \u0026lt;container-name\u0026gt;: Assign a name to the container for easier reference -p \u0026lt;host-port\u0026gt;:\u0026lt;container-port\u0026gt;: Map host port to container port (e.g. 8888:22) --entrypoint /bin/bash: Override default entrypoint with bash shell \u0026lt;image-name\u0026gt;: Name of the Docker image to use (e.g. ubuntu, nvidia/cuda) \u0026lt;image-tag\u0026gt;: Version/tag of the image (e.g. latest, 20.04) Additional Options:\nClick to expand --shm-size \u0026lt;shm-size\u0026gt;G: Set size of /dev/shm (shared memory) in GB (e.g. 2G) --gpus all: [Optional] Give container access to all GPUs (requires nvidia-docker) -v \u0026lt;host-path\u0026gt;:\u0026lt;container-path\u0026gt;: Mount host directory into container -e KEY=VALUE: Set environment variables --network=host: Use host network stack; If set, -p is not needed, and you can access host services directly with localhost --ipc=host: Use host IPC namespace --privileged: Give extended privileges to container -u $(id -u):$(id -g): Run as current user instead of root --rm: Automatically remove container when it exits Example:\n1 2 3 4 5 6 7 8 9 # Create and run a container from \u0026lt;image-name\u0026gt;:\u0026lt;image-tag\u0026gt; docker run -it \\ --gpus all \\ --name \u0026lt;container-name\u0026gt; \\ -p \u0026lt;host-port\u0026gt;:\u0026lt;container-port\u0026gt; \\ --entrypoint /bin/bash \\ --shm-size \u0026lt;shm-size\u0026gt;G \\ \u0026lt;image-name\u0026gt;:\u0026lt;image-tag\u0026gt; # Now you are in the container. To exit container and return to host, run exit, which will stop the containter; Or type Ctrl+P+Q, which will detach from container without stopping it.\n2.3. Start, Stop, Remove and Rename a Container Start a container:\n1 docker start \u0026lt;container-name\u0026gt; Attach to a container:\n1 docker exec -it \u0026lt;container-name\u0026gt; bash Stop a container:\n1 docker stop \u0026lt;contianer-name\u0026gt; Remove a container:\n1 docker rm \u0026lt;container-name\u0026gt; Rename a container:\n1 2 3 docker rename \u0026lt;container-id\u0026gt; \u0026lt;new-name\u0026gt; # or docker rename \u0026lt;old-name\u0026gt; \u0026lt;new-name\u0026gt; 2.4. Export a Container to a tar File Note that docker export only exports the container\u0026rsquo;s filesystem without layer information. It is more recommended to use docker save to save a image to a tar file (see 1.5. Save an Image to a tar File).\n1 docker export \u0026lt;container-id\u0026gt; \u0026gt; \u0026lt;some-name\u0026gt;.tar See 1.6. Load an Image to load the generated tar file to an image.\n3. Change Docker Root Directory to a Different Location Check current docker root directory:\n1 docker info | grep \u0026#34;Docker Root Dir\u0026#34; Stop docker daemon:\n1 systemctl stop docker Edit (or create) the docker configuration file:\n1 vim /etc/docker/daemon.json Add or modify the configuration:\n1 2 3 { \u0026#34;data-root\u0026#34;: \u0026#34;\u0026lt;target-docker-root\u0026gt;\u0026#34; } Copy the existing docker data to target location:\n1 rsync -aP \u0026lt;current-docker-root\u0026gt; \u0026lt;target-docker-root\u0026gt; Restart docker daemon:\n1 2 systemctl daemon-reload systemctl start docker Verify the new location:\n1 docker info | grep \u0026#34;Docker Root Dir\u0026#34; ","permalink":"https://jamesnulliu.github.io/blogs/something-about-docker/","summary":"Something about docker.","title":"Something about Docker"},{"content":"1. Introduction Here is a common scenario:\nYou have a target linux server without public network access, but you want to connect to it using VSCode Remote SSH. Or even further, your target server is running several Docker containers, and you want to attach to the containers on VSCode. You also need compatible extensions to be installed, so that you can actually work with your target server or containers. What a pain! But don\u0026rsquo;t worry, this article will show several methods to install VSCode Server and extensions offline, which can hopefully save you a lot of time and effort.\n2. Method 1: Copy from Another Linux Server 😎 This is the easiest method!\nConnect to another Linux server (or WSL) which has access to the public network with VSCode Remote SSH on your local machine. On the server, you would find the ~/.vscode-server directory, which contains everything you need for SSH connection and all the extensions you have installed. Copy the ~/.vscode-server directory to your target server. If you want to attach to a container on the server, copy the ~/.vscode-remote directory to the container; For example: 1 docker cp ~/.vscode-remote \u0026lt;container_id\u0026gt;:/root/.vscode-remote Now you can connect to the target server from your local machine using VSCode Remote SSH, and you can also attach to the container after connecting to the server. ⚠️ Note that each time you update your local VSCode, you need to first connect to another linux server and then repeat the above steps to copy the ~/.vscode-server directory to the target server or container.\n3. Method 2: Install Manually 😵‍💫 This is a relatively complex method, recommended only if you cannot use Method 1! Moreover, this method does not support installing extentions in your target server or container. To use extentions, you will have to copy the ~/.vscode-server/extenstions directory on another server to the target machine manually and then modify ~/.vscode-server/extensions/extensions.json, replacing all the extention paths to a correct path based on your environment. 3.1. VSCode Version and Commit-ID If your vscode binary is in env:PATH, you can get the version and commit-id by running the following command:\n1 code --version Or if not, open vscode, click Help =\u0026gt; About. Find the version and commit-id in the pop-up window:\nClick Help =\u0026gt; Click About =\u0026gt; Find the version and commit-id in the pop-up window. 3.2. Case A: If Your VSCode Version is Less than 1.19 (e.g, 1.18.1) Download vscode-server-linux-x64 with the following link and send it to the target server:\n1 2 3 4 5 6 7 8 9 # 1. Download the vscode-server-linux-x64 with the commit-id # If your local machine is Linux: wget https://update.code.visualstudio.com/commit:\u0026lt;commit-id\u0026gt;/server-linux-x64/stable # Or if your local machine is Windows: curl -O https://update.code.visualstudio.com/commit:\u0026lt;commit-id\u0026gt;/server-linux-x64/stable # 2. Send \u0026#34;./stable\u0026#34; from host to \u0026#34;~\u0026#34; on server with scp and rename it to # \u0026#34;~/vscode-server-linux-x64.tar.gz\u0026#34; scp -P \u0026lt;port\u0026gt; ./stable \u0026lt;username\u0026gt;@\u0026lt;server-ip\u0026gt;:~/vscode-server-linux-x64.tar.gz Now login to the the server with SSH on your local terminal:\n1 2 3 4 5 6 7 8 # 3. Create directory \u0026#34;~/.vscode-server/bin\u0026#34; mkdir -p ~/.vscode-server/bin # 4. Extract \u0026#34;~/vscode-server-linux-x64.tar.gz\u0026#34; to \u0026#34;~/.vscode-server/bin\u0026#34; tar -xf ~/vscode-server-linux-x64.tar.gz -C ~/.vscode-server/bin # 5. Rename the extracted directory \u0026#34;vscode-server\u0026#34; to \u0026#34;\u0026lt;commit-id\u0026gt;\u0026#34; mv ~/.vscode-server/bin/vscode-server ~/.vscode-server/bin/\u0026lt;commit-id\u0026gt; # 6. Optional: Copy the \u0026#34;.vscode-server\u0026#34; directory to target container docker cp ~/.vscode-server \u0026lt;container_id\u0026gt;:/root/.vscode-server Finally, go back to your local machine and connect to your server with VSCode Remote SSH, and everything should be okay.\n3.2. Case B: If Your VSCode Version is Greater than 1.19 Download vscode-cli with the following link and send it to the target server:\n1 2 3 4 5 6 7 8 # 1. Download the vscode-cli with the commit-id # If your local machine is Linux: wget https://update.code.visualstudio.com/commit:\u0026lt;commit-id\u0026gt;/cli-alpine-x64/stable # Or if your local machine is Windows: curl -O https://update.code.visualstudio.com/commit:\u0026lt;commit-id\u0026gt;/cli-alpine-x64/stable # 2. Send \u0026#34;./stable\u0026#34; from host to \u0026#34;~\u0026#34; on server and rename it to \u0026#34;~/vscode-cli.tar.gz\u0026#34; scp -P \u0026lt;port\u0026gt; ./stable \u0026lt;username\u0026gt;@\u0026lt;server-ip\u0026gt;:~/vscode-cli.tar.gz Now login to the the server with SSH on your local terminal:\n1 2 3 4 5 6 7 8 # 3. Create directory \u0026#34;~/.vscode-server/cli/servers/Stable-\u0026lt;commit-id\u0026gt;\u0026#34; mkdir -p ~/.vscode-server/cli/servers/Stable-\u0026lt;commit-id\u0026gt; # 4. Extract \u0026#34;~/vscode-cli.tar.gz\u0026#34; to \u0026#34;~/.vscode-server\u0026#34; tar -xzf ~/vscode-cli.tar.gz -C ~/.vscode-server # 5. Rename the extracted binary to \u0026#34;~/.vscode-server/code-\u0026lt;commit-id\u0026gt;\u0026#34; mv ~/.vscode-server/code ~/.vscode-server/code-\u0026lt;commit-id\u0026gt; # 6. Optional: Copy the \u0026#34;.vscode-server\u0026#34; directory to target container docker cp ~/.vscode-server \u0026lt;container_id\u0026gt;:/root/.vscode-server Finally, go back to your local machine and connect to your server with VSCode Remote SSH, and everything should be okay.\n","permalink":"https://jamesnulliu.github.io/blogs/offline-installation-of-vscode-server-and-extensions/","summary":"How to install VSCode Server offline.","title":"Offline Installation of vscode-server and Extensions"},{"content":" Reference: https://mlir.llvm.org/docs/Tutorials/Toy/Ch-4/\nNote: Check Setup the Environment of MLIR for the environment setup.\n1. Introduction Problem: Naively implementing each transformation for each dialect leads to large amounts of code duplication, as the internal algorithms are generally very similar.\nSolution: To provide the ability for transformations to opaquely hook into dialects like Toy to get the information they need.\n2. Add OpPrintInterface Define a new env var:\nexport TOY_CH4_HOME=\u0026#34;$MLIR_HOME/examples/toy/Ch4\u0026#34; 2.1. Define OpPrintInterface First, create a new file $TOY_CH4_HOME/include/toy/OpPrintInterface.td, define OpPrintOpInterface with method opPrint which returns a std::string:\n#ifndef PRINT_INTERFACE #define PRINT_INTERFACE include \u0026#34;mlir/IR/OpBase.td\u0026#34; def OpPrintOpInterface : OpInterface\u0026lt;\u0026#34;OpPrint\u0026#34;\u0026gt; { let description = [{ Interface to print something in an operator. }]; let methods = [ InterfaceMethod\u0026lt; \u0026#34;Print some information in the current operation\u0026#34;, \u0026#34;std::string\u0026#34;, \u0026#34;opPrint\u0026#34; \u0026gt; ]; } #endif // PRINT_INTERFACE Second, create a file $TOY_CH4_HOME/include/toy/OpPrintInterface.hpp:\n#ifndef OPPRINTINTERFACE_HPP_ #define OPPRINTINTERFACE_HPP_ #include \u0026#34;mlir/IR/OpDefinition.h\u0026#34; namespace mlir::toy { /// Include the auto-generated declarations. #include \u0026#34;toy/OpPrintOpInterface.h.inc\u0026#34; } // namespace mlir::toy #endif Third, in $TOY_CH4_HOME/include/toy/Dialect.h, include the new interface:\n#include \u0026#34;toy/OpPrintInterface.hpp\u0026#34; Fourth, make some modifications in $TOY_CH4_HOME/include/toy/Ops.td. Include the interface\u0026rsquo;s td at the beginning of the file:\ninclude \u0026#34;toy/OpPrintInterface.td\u0026#34; Then, for example, change the AddOp to declare that it implements the OpPrint interface:\ndef AddOp : Toy_Op\u0026lt;\u0026#34;add\u0026#34;, [ Pure, DeclareOpInterfaceMethods\u0026lt;ShapeInferenceOpInterface\u0026gt;, DeclareOpInterfaceMethods\u0026lt;OpPrintOpInterface\u0026gt; ]\u0026gt; { // ... } You can also do the similar declaration for other operations.\nFinally, some CMakeLists need to be modified.\nAdd following lines in $TOY_CH4_HOME/include/toy/CMakeLists.txt:\nset(LLVM_TARGET_DEFINITIONS OpPrintInterface.td) mlir_tablegen(OpPrintOpInterface.h.inc -gen-op-interface-decls) mlir_tablegen(OpPrintOpInterface.cpp.inc -gen-op-interface-defs) add_public_tablegen_target(ToyCh4OpPrintInterfaceIncGen) Change the add_toy_chapter in $TOY_CH4_HOME/CMakelists.txt to:\nadd_toy_chapter(toyc-ch4 toyc.cpp parser/AST.cpp mlir/MLIRGen.cpp mlir/Dialect.cpp mlir/ShapeInferencePass.cpp mlir/OpPrintInterfacePass.cpp mlir/ToyCombine.cpp DEPENDS ToyCh4OpsIncGen ToyCh4ShapeInferenceInterfaceIncGen ToyCh4CombineIncGen ToyCh4OpPrintInterfaceIncGen ) To match the listed source files, create a blank file $TOY_CH4_HOME/mlir/OpPrintInterfacePass.cpp. We will implement the pass later.\nNow build the MLIR:\nbash $LLVM_PROJ_HOME/scripts/build-mlir.sh Errors pop out because we haven\u0026rsquo;t implemented the OpPrintInterface which is declared in AddOp. Don\u0026rsquo;t worry, it will be implemented in the next section.\nNow you can check the generated C++ class declarations in, for example, $LLVM_PROJ_HOME/build/tools/mlir/examples/toy/Ch4/include/toy/OpPrintOpInterface.h.inc.\n2.3. Implement OpPrintInterface Since the interface is declared in AddOp (which is actually implemented by inheriting OpPrintOpInterface which provides a pure virtual function opPrint), we need to implement the function.\nIn $TOY_CH4_HOME/mlir/Dialect.cpp, add the following code:\nstd::string AddOp::opPrint() { return \u0026#34;I am AddOp\u0026#34;; } 2.4. Implement OpPrintPass In $TOY_CH4_HOME/include/toy/Passes.h, add the following line under mlir/examples/toy/Ch4/include/toy/Passes.h (inside namespace mlir::toy):\nstd::unique_ptr\u0026lt;Pass\u0026gt; createOpPrintPass(); In $TOY_CH4_HOME/mlir/OpPrintInterfacePass.cpp, add the following code:\n#include \u0026#34;mlir/IR/Operation.h\u0026#34; #include \u0026#34;mlir/Pass/Pass.h\u0026#34; #include \u0026#34;mlir/Support/LLVM.h\u0026#34; #include \u0026#34;mlir/Support/TypeID.h\u0026#34; #include \u0026#34;toy/Dialect.h\u0026#34; #include \u0026#34;toy/OpPrintInterface.hpp\u0026#34; #include \u0026#34;toy/Passes.h\u0026#34; #include \u0026lt;iostream\u0026gt; #include \u0026lt;memory\u0026gt; using namespace mlir; using namespace toy; #include \u0026#34;toy/OpPrintOpInterface.cpp.inc\u0026#34; namespace { struct OpPrintIntercfacePass : public mlir::PassWrapper\u0026lt;OpPrintIntercfacePass, OperationPass\u0026lt;toy::FuncOp\u0026gt;\u0026gt; { MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(OpPrintIntercfacePass) void runOnOperation() override { auto f = getOperation(); f.walk([\u0026amp;](mlir::Operation* op) { if (auto shapeOp = dyn_cast\u0026lt;OpPrint\u0026gt;(op)) { std::cout \u0026lt;\u0026lt; shapeOp.opPrint() \u0026lt;\u0026lt; std::endl; } }); } }; } // namespace std::unique_ptr\u0026lt;Pass\u0026gt; createOpPrintPass() { return std::make_unique\u0026lt;OpPrintIntercfacePass\u0026gt;(); } 2.5. Add Pass to Pass Manager In $TOY_CH4_HOME/toyc.cpp, add the following line after mlir::OpPassManager \u0026amp;optPM = pm.nest\u0026lt;mlir::toy::FuncOp\u0026gt;();:\noptPM.addPass(mlir::toy::createOpPrintPass()); Implement OpPrintPass in $TOY_CH4_HOME/mlir/OpPrintInterfacePass.cpp:\n#include \u0026#34;mlir/IR/Operation.h\u0026#34; #include \u0026#34;mlir/Pass/Pass.h\u0026#34; #include \u0026#34;mlir/Support/LLVM.h\u0026#34; #include \u0026#34;mlir/Support/TypeID.h\u0026#34; #include \u0026#34;toy/Dialect.h\u0026#34; #include \u0026#34;toy/OpPrintInterface.hpp\u0026#34; #include \u0026#34;toy/Passes.h\u0026#34; #include \u0026lt;iostream\u0026gt; #include \u0026lt;memory\u0026gt; using namespace mlir; using namespace toy; #include \u0026#34;toy/OpPrintOpInterface.cpp.inc\u0026#34; namespace { struct OpPrintIntercfacePass : public mlir::PassWrapper\u0026lt;OpPrintIntercfacePass, OperationPass\u0026lt;toy::FuncOp\u0026gt;\u0026gt; { MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(OpPrintIntercfacePass) void runOnOperation() override { auto f = getOperation(); f.walk([\u0026amp;](mlir::Operation* op) { if (auto shapeOp = dyn_cast\u0026lt;OpPrint\u0026gt;(op)) { std::cout \u0026lt;\u0026lt; shapeOp.opPrint() \u0026lt;\u0026lt; std::endl; } }); } }; } // namespace std::unique_ptr\u0026lt;Pass\u0026gt; mlir::toy::createOpPrintPass() { return std::make_unique\u0026lt;OpPrintIntercfacePass\u0026gt;(); } 2.6. Test the Pass Now, rebuild the MLIR:\nbash $LLVM_PROJ_HOME/scripts/build-mlir.sh Run the Test:\ntoyc-ch4 $MLIR_HOME/test/Examples/Toy/Ch4/codegen.toy -emit=mlir -opt ","permalink":"https://jamesnulliu.github.io/blogs/learning-notes-mlir/toy-ch4-enabling-generic-transformation-with-interfaces/","summary":"My learning notes of MLIR.","title":"Toy Ch4 | Enabling Generic Transformation with Interfaces"},{"content":"Open ~/.bashrc file.\nCreate 2 functions to load and unload environment variables:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 env_load() { local env_var=$1 local path=$2 if [[ \u0026#34;:${!env_var}:\u0026#34; != *\u0026#34;:$path:\u0026#34;* ]]; then export $env_var=\u0026#34;${!env_var}:$path\u0026#34; fi } env_unload() { local env_var=$1 local path=$2 local paths_array=(${!env_var//:/ }) local new_paths=() for item in \u0026#34;${paths_array[@]}\u0026#34;; do if [[ \u0026#34;$item\u0026#34; != \u0026#34;$path\u0026#34; ]]; then new_paths+=(\u0026#34;$item\u0026#34;) fi done export $env_var=$(IFS=:; echo \u0026#34;${new_paths[*]}\u0026#34;) } Now, you can use env_load and env_unload to manage environment variables.\nFor example, to manage CUDA environment, add these lines to ~/.bashrc:\n1 2 3 export CUDA_HOME=\u0026#34;/usr/local/cuda-12.1\u0026#34; alias LOAD_CUDA=\u0026#34;env_load PATH $CUDA_HOME/bin; env_load LD_LIBRARY_PATH $CUDA_HOME/lib64\u0026#34; alias UNLOAD_CUDA=\u0026#34;env_unload PATH $CUDA_HOME/bin; env_unload LD_LIBRARY_PATH $CUDA_HOME/lib64\u0026#34; Related Blogs User Management on Linux ","permalink":"https://jamesnulliu.github.io/blogs/environment-variable-management-on-linux/","summary":"An easy way to manage environment variables on Linux using load and unload.","title":"Environment Variable Management on Linux"},{"content":" Reference: https://mlir.llvm.org/docs/Tutorials/Toy/Ch-3/\nNote: Check Setup the Environment of MLIR for the environment setup.\n1. Run Example Emit MLIR:\ntoyc-ch3 $MLIR_HOME/test/Examples/Toy/Ch3/codegen.toy -emit=mlir -opt Key Points:\nPattern-match and rewrite; Declarative, rule-based pattern-match and rewrite (DRR); ","permalink":"https://jamesnulliu.github.io/blogs/learning-notes-mlir/toy-ch3-high-level-language-specific-analysis-and-transformation/","summary":"My learning notes of MLIR.","title":"Toy Ch3 | High-Level Language Specific Analysis and Transformation"},{"content":" Reference: https://mlir.llvm.org/docs/Tutorials/Toy/Ch-2/\nNote: Check Setup the Environment of MLIR for the environment setup.\n1. Run Example Define a new env var:\nexport TOY_CH2_HOME=\u0026#34;$MLIR_HOME/examples/toy/Ch2\u0026#34; Create a file $TOY_CH2_HOME/input.toy ; Add the following content to the file:\n# User defined generic function that operates on unknown shaped arguments. def multiply_transpose(a, b) { return transpose(a) * transpose(b); } def main() { var a\u0026lt;2, 3\u0026gt; = [[1, 2, 3], [4, 5, 6]]; var b\u0026lt;2, 3\u0026gt; = [1, 2, 3, 4, 5, 6]; var c = multiply_transpose(a, b); var d = multiply_transpose(b, a); print(d); } Emit to AST (Abstract Syntax Tree):\ntoy-ch2 $TOY_CH2_HOME/input.toy -emit=ast Emit to MLIR (Multi-Level Intermediate Representation):\ntoyc-ch2 $TOY_CH2_HOME/input.toy -emit=mlir 2. Add an Operator 2.1. Define the Operation Add following code to $TOY_CH2_HOME/include/toy/Ops.td:\n// SubtractOp def SubtractOp : Toy_Op\u0026lt;\u0026#34;subtract\u0026#34;\u0026gt; { let summary = \u0026#34;element-wise subtraction operation\u0026#34;; let description = [{ The \u0026#34;subtract\u0026#34; operation performs element-wise subtraction between two tensors. The shapes of the tensor operands are expected to match. }]; let arguments = (ins F64Tensor:$lhs, F64Tensor:$rhs); let results = (outs F64Tensor); // Indicate that the operation has a custom parser and printer method. let hasCustomAssemblyFormat = 1; // Allow building an AddOp with from the two input operands. let builders = [ OpBuilder\u0026lt;(ins \u0026#34;Value\u0026#34;:$lhs, \u0026#34;Value\u0026#34;:$rhs)\u0026gt; ]; } Build MLIR again to imply the modifications:\nbash $LLVM_PROJ_HOME/scripts/build-mlir.sh Build errors pop out, because:\nhasCustomAssemblyFormat is assigned with 1, but the custom parser and printer method is not implemented. OpBuilder is not implemented. These errors will be handled later.\nNote that the C++ implementation of class SubtractOp has been generated in $LLVM_PROJ_HOME/build/tools/mlir/examples/toy/Ch2/include/toy/Ops.h.inc, and as a result, you are now able to use class SubtractOp with code completion and syntax highlighting of clangd.\n2.2. Implement the Operations To implement custom parser and printer methods as well as OpBuilder, add the following code to $TOY_CH2_HOME/mlir/Dialect.cpp:\n// SubtractOp void SubtractOp::build(mlir::OpBuilder \u0026amp;builder, mlir::OperationState \u0026amp;state, mlir::Value lhs, mlir::Value rhs) { state.addTypes(UnrankedTensorType::get(builder.getF64Type())); state.addOperands({lhs, rhs}); } mlir::ParseResult SubtractOp::parse(mlir::OpAsmParser \u0026amp;parser, mlir::OperationState \u0026amp;result) { return parseBinaryOp(parser, result); } void SubtractOp::print(mlir::OpAsmPrinter \u0026amp;p) { printBinaryOp(p, *this); } 2.3. Emit - Operator Go to $TOY_CH2_HOME/mlir/MLIRGen.cpp, locate function mlirGen and add the specific case for -, as shown below:\nmlir::Value mlirGen(BinaryExprAST \u0026amp;binop) { // ... switch (binop.getOp()) { case \u0026#39;+\u0026#39;: return builder.create\u0026lt;AddOp\u0026gt;(location, lhs, rhs); case \u0026#39;*\u0026#39;: return builder.create\u0026lt;MulOp\u0026gt;(location, lhs, rhs); case \u0026#39;-\u0026#39;: return builder.create\u0026lt;SubtractOp\u0026gt;(location, lhs, rhs); } // ... } Rebuild the MLIR:\n$LLVM_PROJ_HOME/scripts/build-mlir.sh 2.4. Test the - Operator Change the content of $MLIR_HOME/input.toy to:\n# User defined generic function that operates on unknown shaped arguments. def multiply_transpose(a, b) { return transpose(a) * transpose(b); } def main() { var a\u0026lt;2, 3\u0026gt; = [[1, 2, 3], [4, 5, 6]]; var b\u0026lt;2, 3\u0026gt; = [1, 2, 3, 4, 5, 6]; var c = multiply_transpose(a, b); var d = multiply_transpose(b, a); var e = a - b; print(e); } Generate MLIR:\ntoyc-ch2 $TOY_CH2_HOME/input.toy -emit=mlir ","permalink":"https://jamesnulliu.github.io/blogs/learning-notes-mlir/toy-ch2-emitting-basic-mlir/","summary":"My learning notes of MLIR.","title":"Toy Ch2 | Emitting Basic MLIR"},{"content":"To build MLIR, follow the official guide: Getting Started.\nSet up some environment variables to make our life easier when working with MLIR:\nexport LLVM_PROJ_HOME=\u0026#34;/path/to/llvm-project\u0026#34; export MLIR_HOME=\u0026#34;$LLVM_PROJ_HOME/mlir\u0026#34; Write a script to help build MLIR from source:\n# @file $LLVM_PROJ_HOME/scripts/build-mlir.sh cd $LLVM_PROJ_HOME CC=clang CXX=clang++ \\ cmake -S . -B ./build -G Ninja ./llvm \\ -DCMAKE_BUILD_TYPE=Release \\ -DLLVM_ENABLE_PROJECTS=mlir \\ -DLLVM_BUILD_EXAMPLES=ON \\ -DLLVM_TARGETS_TO_BUILD=\u0026#34;Native;NVPTX;AMDGPU\u0026#34; \\ -DCMAKE_BUILD_TYPE=Release \\ -DLLVM_ENABLE_ASSERTIONS=ON \\ -DCMAKE_EXPORT_COMPILE_COMMANDS=ON \\ -DLLVM_ENABLE_LLD=ON cmake --build . --target check-mlir -j $(nproc) Now we can run the script to build the MLIR easily:\nbash $LLVM_PROJ_HOME/scripts/build-mlir.sh The generated binary files are in $LLVM_PROJ_HOME/build/bin.\nIt would be more convenient to add this directory to PATH when working with MLIR:\nexport PATH=\u0026#34;$LLVM_PROJ_HOME/build/bin:$PATH\u0026#34; ","permalink":"https://jamesnulliu.github.io/blogs/learning-notes-mlir/setup-the-environment-of-mlir/","summary":"My learning notes of MLIR.","title":"Setup the Environment of MLIR"},{"content":" Reference: The Deep Learning Compiler: A Comprehensive Survey\n1. ABSTRACT The DL compilers take the DL models described in different DL frameworks as input, and then generate optimized codes for diverse DL hardware as output.\nGenerally, the DL hardware can be divided into the following categories:\nGeneral-purpose hardware with software-hardware co-design; Dedicated hardware fully customized for DL models; Neuromorphic hardware inspired by biological brain science. However, the drawback of relying on the libraries is that they usually fall behind the rapid development of DL models, and thus fail to utilize the DL chips efficiently.\nTo address the drawback of DL libraries and tools, as well as alleviate the burden of optimizing the DL models on each DL hardware manually, the DL community has resorted to the domain specific compilers for rescue.\nThe DL compilers take the model definitions described in the DL frameworks as inputs, and generate efficient code implementations on various DL hardware as outputs.\n2. BACKGROUND 2.1. Deep Learning Frameworks \u0026hellip;\n2.3. Hardware-specific DL Code Generator Field Programmable Gate Arrays (FPGAs) are reprogrammable integrated circuits that contain an array of programmable logic blocks. Programmers can configure them after manufacturing.\nThe FPGA can bridge the gap between CPUs/GPUs and ASICs, which causes the FPGA to be an attractive platform for deep learning.\nMapping DL models to FPGAs remains a complicated work even with HLS, because:\nDL models are usually described by the languages of DL frameworks rather than bare mental C/C++ code; DL-specific information and optimizations are hard to be leveraged. The hardware-specific code generator targeting FPGA take the DL models or their domain-specific languages (DSLs) as the input, conduct the domain-specific (about FPGA and DL) optimizations and mappings, then generate the HLS or Verilog/VHDL and finally generate the bitstream. They can be classified into two categories according to the generated architectures of FPGA-based accelerators: the processor architecture and the streaming architecture.\nThe processor architecture has similarities with general-purpose processors. An FPGA accelerator of this architecture usually comprises several Processing Units (PUs), which are comprised of on-chip buffers and multiple smaller Processing Engines (PEs).\nThe streaming architecture has similarities with pipelines. An FPGA accelerator of this architecture consists of multiple different hardware blocks, and it nearly has one hardware block for each layer of an input DL mode\n3. COMMON DESIGN ARCHITECTURE OF DL COMPILERS Figure 1. Common Design Architecture of DL Compilers.\n4.1. High-level IR DAG-based IR - DAG-based IR is one of the most traditional ways for the compilers to build a computation graph, with nodes and edges organized as a directed acyclic graph (DAG). In DL compilers, the nodes of a DAG represent the atomic DL operators (convolution, pooling, etc.), and the edges represent the tensors. And the graph is acyclic without loops, which differs from the data dependence graphs (DDG) of generic compilers.\nLet-binding-based IR - Let-binding is one method to solve the semantic ambiguity by offering let expression to certain functions with restricted scope used by many high-level programming languages such as Javascript, F#, and Scheme. When using the let keyword to define an expression, a let node is generated, and then it points to the operator and variable in the expression instead of just building computational relation between variables as a DAG.\nRepresenting Tensor Computation - Different graph IRs have different ways to represent the computation on tensors:\nFunction Based Lambda Based Einstein notation Data representation - The data in DL compilers (e.g., inputs, weights, and intermediate data) are usually organized in the form of tensors, which are also known as multi-dimensional arrays. The DL compilers can represent tensor data directly by memory pointers, or in a more flexible way by placeholders. A placeholder contains the size for each dimension of a tensor. Alternatively, the dimension sizes of the tensor can be marked as unknown. For optimizations, the DL compilers require the data layout information. In addition, the bound of iterators should be inferred according to the placeholders.\n4.2. Low-level IR Low-level IR describes the computation of a DL model in a more fine-grained representation than that in high-level IR, which enables the target-dependent optimizations by providing interfaces to tune the computation and memory access.\nHalide-based IR - Halide is firstly proposed to parallelize image processing, and it is proven to be extensible and efficient in DL compilers (e.g., TVM). The fundamental philosophy of Halide is the separation of computation and schedule.\nPolyhedral-based IR - The polyhedral model is an important technique adopted in DL compilers. It uses linear programming, affine transformations, and other mathematical methods to optimize loop-based codes with static control flow of bounds and branches.\n4.3. Frontend Optimizations After constructing the computation graph, the frontend applies graph-level optimizations.\nThe frontend optimizations are usually defined by passes, and can be applied by traversing the nodes of the computation graph and performing the graph transformations:\nCapture the specific features from the computation graph; Rewrite the graph for optimization. Figure 2. Example of computation graph optimizations, taken from the HLO graph of AlexNet on Volta GPU using TensorFlow XLA.\n4.3.1. Node-level optimizations The nodes of the computation graph are coarse enough to enable optimizations inside a single node. And the node-level optimizations include node elimination that eliminates unnecessary nodes and node replacement that replaces nodes with other lower-cost nodes.\n4.3.2. Block-level optimizations Algebraic simplification\nThe algebraic simplification opti- mizations consist of :\nalgebraic identification; strength reduction, with which we can replace more expensive operators by cheaper ones; constant folding, with which we can replace the constant expressions by their values. Such optimizations consider a sequence of nodes, then take advantage of commutativity, associativity, and distributivity of different kinds of nodes to simplify the computation.\nOperator fusion\nOperator fusion is indispensable optimization of DL compilers. It enables better sharing of computation, eliminates intermediate allocations, facilitates further optimization by combining loop nests, as well as reduces launch and synchronization overhead.\nOperator sinking\nThis optimization sinks the operations such as transposes below operations such as batch normalization, ReLU, sigmoid, and channel shuffle. By this optimization, many similar operations are moved closer to each other, creating more opportunities for algebraic simplification.\n4.3.3. Dataflow-level optimizations Common sub-expression elimination (CSE) Dead code elimination (DCE) Static memory planning - Static memory planning optimizations are performed to reuse the memory buffers as much as possible. Usually, there are two approaches: in-place memory sharing and standard memory sharing. Layout transformation - Layout transformation tries to find the best data layouts to store tensors in the computation graph and then inserts the layout transformation nodes to the graph. 4.4. Backend Optimizations The backends of DL compilers have commonly included various hardware-specific optimizations, auto-tuning techniques, and optimized kernel libraries. Hardware-specific optimizations enable efficient code generation for different hardware targets. Whereas, auto-tuning has been essential in the compiler backend to alleviate the manual efforts to derive the optimal parameter configurations. Besides, highly-optimized kernel libraries are also widely used on general-purpose processors and other customized DL accelerators.\nFigure 3. Overview of hardware-specific optimizations applied in DL compilers.\n5. FUTURE DIRECTIONS Dynamic shape and pre/post processing Advanced auto-tuning Polyhedral model Subgraph partitioning Quantization Unified optimizations Differentiable programming Privacy protection Training support ","permalink":"https://jamesnulliu.github.io/blogs/learning-notes-mlir/paper-review-the-deep-learning-compiler-a-comprehensive-survey/","summary":"My learning notes of MLIR.","title":"Paper Review: The Deep Learning Compiler: A Comprehensive Survey"},{"content":"1. List, Add and Remove a User List all users:\n1 cat /etc/passwd Add a new user:\n1 2 3 4 5 6 7 # - `-m`: Creates the user\u0026#39;s home directory. # - `-s`: Specifies the user\u0026#39;s login shell. # - `-c`: Provides a comment, typically the user\u0026#39;s full name. useradd -m -s /bin/bash -c \u0026#34;\u0026lt;full-name\u0026gt;\u0026#34; \u0026lt;username\u0026gt; # Set password after the user is created passwd \u0026lt;username\u0026gt; Remove an existing user:\nRemove the user from the sudo or wheel group (if they were in) before deleting them:\n1 2 gpasswd -d \u0026lt;username\u0026gt; sudo gpasswd -d \u0026lt;username\u0026gt; wheel 1 2 # `-r`: Removes the user\u0026#39;s home directory and mail spool. userdel -r \u0026lt;username\u0026gt; 2. Change the Password of a User 1 passwd \u0026lt;username\u0026gt; 3. Superuser Grant write permission to /etc/sudoers:\n1 chmod u+w /etc/sudoers There are four ways to make a user a superuser:\nAdd \u0026lt;username\u0026gt; ALL=(ALL:ALL) ALL to the end of the file /etc/sudoers. This allows the user to execute any command with prefix sudo after entering the password. Add \u0026lt;username\u0026gt; ALL=(ALL:ALL) NOPASSWD: ALL to the end of the file /etc/sudoers. This allows the user to execute any command with prefix sudo without entering the password. Add %\u0026lt;groupname\u0026gt; ALL=(ALL:ALL) ALL to the end of the file /etc/sudoers. This allows all users in the group to execute any command with prefix sudo after entering the password. Add %\u0026lt;groupname\u0026gt; ALL=(ALL:ALL) NOPASSWD: ALL to the end of the file /etc/sudoers. This allows all users in the group to execute any command with prefix sudo without entering the password. Return the file /etc/sudoers to read-only mode:\n1 chmod u-w /etc/sudoers 4. User Groups List all user groups:\n1 cat /etc/group List the groups a user is in:\n1 groups \u0026lt;username\u0026gt; Create a new group:\n1 groupadd \u0026lt;groupname\u0026gt; Add a user to a group:\n1 gpasswd -a \u0026lt;username\u0026gt; \u0026lt;groupname\u0026gt; Remove a user from a group:\n1 gpasswd -d \u0026lt;username\u0026gt; \u0026lt;groupname\u0026gt; 5. Onwership and Permission of Files and Directories To check the owership and the permission of a file or directory:\n1 2 3 4 5 6 # File: ls -l \u0026lt;filename\u0026gt; # Directory: ls -ld \u0026lt;dirname\u0026gt; # List all files including the hidden ones ls -la Output example:\n1 2 3 4 5 6 # Permision|*|owner|group|bytes| date |file/dirname drwxr-xr-x 2 james james 4096 Dec 2 11:02 example-dir/ # *: Number of subdirectories. # If file, usually starts at 1; Numbers higher than 1 indicate how many hard # links point to this file. # If directory, the minimum value is 2 (\u0026#34;.\u0026#34; and \u0026#34;..\u0026#34;). To break down drwxr-xr-x:\n1 2 3 4 5 6 7 d | rwx | r-x | r-x ↓ ↓ ↓ ↓ | | | └── Others permissions (last 3 chars), 101=5 | | └──────── Group permissions (middle 3), 101=5 | └────────────── Owner permissions (first 3), 111=7 └────────────────── File type, d = directory; - = regular file; l = symbolic link; b = block device; c = character device To change the ownership:\n1 2 chown [-R] \u0026lt;user\u0026gt;:\u0026lt;group\u0026gt; \u0026lt;filename/dirname\u0026gt; chown [-R] :\u0026lt;group\u0026gt; \u0026lt;filename/dirname\u0026gt; To change the permission using numeric mode:\n1 chmod [-R] 764 \u0026lt;filename/dirname\u0026gt; Where:\n7=0b100+0b010+0b001, owner can Read Write Execute. 6=0b100+0b010+0b000, group can Read Write. 4=0b100+0b000+0b000, other can Read. To change the permission using symbolic mode:\n1 2 3 4 5 6 7 chmod +r foldername # Add read for everyone chmod a+r foldername # Add read for everyone chmod u+r foldername # Add read for owner only chmod g+r foldername # Add read for group only chmod o+r foldername # Add read for others only chmod a-rwx file # Remove all permissions from all # ... 6. Shared Directory To create a shared directory for all users in the same group being able to create, modify, execute, and delete files:\nClick to see file: create-shared-dir 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 #!/bin/bash set -e # ============================================================================= # Script: create-shared-dir # Description: Configures one or more directories to be shared with a specific # group. # # It performs the following actions on each target directory: # 1. Creates the directory if it doesn\u0026#39;t exist. # 2. Recursively sets the group ownership. # 3. Sets permissions (group: rwx, others: none). # 4. Sets the \u0026#39;setgid\u0026#39; bit on subdirectories to enforce group inheritance. # 5. Uses Access Control Lists (ACLs) to enforce permissions for new items. # # Usage: # sudo create-shared-dir \u0026lt;shared_group\u0026gt; \u0026lt;dir1\u0026gt; [\u0026lt;dir2\u0026gt; ...] # # Example: # sudo create-shared-dir developers /srv/data /home/shared/project # # ============================================================================= # --- Argument Parsing --- SHARED_GROUP=\u0026#34;$1\u0026#34; shift # Shift arguments so $@ contains only the directories TARGET_DIRS=(\u0026#34;$@\u0026#34;) # --- Input Validation --- if [ ${#TARGET_DIRS[@]} -eq 0 ] || [ \u0026#34;$1\u0026#34; == \u0026#34;-h\u0026#34; ] || [ \u0026#34;$1\u0026#34; == \u0026#34;--help\u0026#34; ] then echo \u0026#34;Usage: $0 \u0026lt;shared_group\u0026gt; \u0026lt;dir1\u0026gt; [\u0026lt;dir2\u0026gt; ...]\u0026#34; echo \u0026#34;Example: $0 developers /var/www/project_a\u0026#34; exit 1 fi # --- Pre-flight Checks --- # 1. Check for root privileges. if [[ $EUID -ne 0 ]]; then echo \u0026#34;Error: This script must be run as root (or with sudo).\u0026#34; \u0026gt;\u0026amp;2 exit 1 fi # 2. Check if the specified group exists. if ! getent group \u0026#34;$SHARED_GROUP\u0026#34; \u0026gt; /dev/null; then echo \u0026#34;Error: Group \u0026#39;$SHARED_GROUP\u0026#39; does not exist.\u0026#34; \u0026gt;\u0026amp;2 exit 1 fi # --- Main Logic --- echo \u0026#34;Configuring shared directories for group \u0026#39;$SHARED_GROUP\u0026#39;...\u0026#34; for target_dir in \u0026#34;${TARGET_DIRS[@]}\u0026#34;; do echo \u0026#34;--\u0026gt; Processing: $target_dir\u0026#34; mkdir -p \u0026#34;$target_dir\u0026#34; chgrp -R \u0026#34;$SHARED_GROUP\u0026#34; \u0026#34;$target_dir\u0026#34; chmod -R g=rwX,o-rwx \u0026#34;$target_dir\u0026#34; find \u0026#34;$target_dir\u0026#34; -type d -exec chmod g+s {} + setfacl -R -m \u0026#34;g:$SHARED_GROUP:rwX\u0026#34; \u0026#34;$target_dir\u0026#34; setfacl -R -d -m \u0026#34;g:$SHARED_GROUP:rwX\u0026#34; \u0026#34;$target_dir\u0026#34; done echo \u0026#34;Configuration complete.\u0026#34; exit 0 You may put the file to \u0026ldquo;/usr/local/bin/create-shared-dir\u0026rdquo;, and then change its mode with command:\n1 chmod +x /usr/local/bin/create-shared-dir Then you can run the script with the desired parameters, for example:\n1 sudo create-shared-dir developers /srv/data /home/shared/project Related Blogs Environment Varialble Management on Linux ","permalink":"https://jamesnulliu.github.io/blogs/user-management-on-linux/","summary":"Some useful commands for user management on Linux.","title":"User Management on Linux"},{"content":"The buzzer is a small speaker that emits a beep sound. It is used to notify the user of system events. However, sometimes the beep sound is annoying. To close the buzzer on Linux system, execute the following command:\n1 2 3 sudo bash -c \u0026#34;echo blacklist pcspkr \u0026gt; /etc/modprobe.d/blacklist-pcspkr.conf\u0026#34; # Reboot sudo reboot ","permalink":"https://jamesnulliu.github.io/blogs/turn-off-the-buzzer/","summary":"How to close the buzzer on Linux?","title":"Turn off The Buzzer"},{"content":"Install build essentials.\n1 sudo dnf groupinstall \u0026#34;Development Tools\u0026#34; # gcc-11 is installed by default Enable devel repository and install gcc toolset 13:\n1 2 3 sudo dnf config-manager --set-enabled devel sudo dnf update sudo dnf install gcc-toolset-13 To enable gcc-13:\n1 scl enable gcc-toolset-13 bash To disable gcc-13, just exit the shell:\n1 exit ","permalink":"https://jamesnulliu.github.io/blogs/install-gcc-13-on-rocky-9/","summary":"How to install GCC-13 in Rocky 9.","title":"Install GCC-13 on Rocky 9"},{"content":"To stop and disable gdm service (which is the default display manager):\n1 2 sudo systemctl disable gdm sudo systemctl stop gdm To disable the default nouveau driver:\n1 2 3 4 5 6 sudo bash -c \u0026#34;echo blacklist nouveau \u0026gt; /etc/modprobe.d/blacklist-nvidia-nouveau.conf\u0026#34; sudo bash -c \u0026#34;echo options nouveau modeset=0 \u0026gt;\u0026gt; /etc/modprobe.d/blacklist-nvidia-nouveau.conf\u0026#34; # Update the kernel initramfs sudo dracut --force # Reboot sudo reboot Install epel-release and dkms:\n1 2 sudo dnf install epel-release sudo dnf install dkms Download the installation LOCAL RUN FILE of THE LATEST CUDA Toolkit (\u0026gt;=12.5) from NVIDIA official website and install it (with driver).\n💬 There is a bug of the compatibility of the new linux kernel and previous cuda derviers (less than 555). You could install other versions of cuda toolkit but keep the latest driver.\nTo enable and start gdm service:\n1 2 sudo systemctl enable gdm sudo systemctl start gdm ","permalink":"https://jamesnulliu.github.io/blogs/install-nvidia-driver-and-cuda-toolkit-on-rocky-9/","summary":"How to install Nvidia driver and CUDA toolkit on Rocky 9.","title":"Install Nvidia Driver and CUDA Toolkit on Rocky 9"},{"content":"1. Introduction In general, you will need these things to train a model:\nA Model A Dataset A Dataloader A Loss Function (Criterion) An Optimizer 2. Model We will build a simple model for demonstration. The model takes a tensor of shape (batch_size, 10) as input and outputs a tensor of shape (batch_size, 2).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # @file simple_model.py import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 2) def forward(self, x): return self.fc(x) if __name__ == \u0026#34;__main__\u0026#34;: model = SimpleModel() x = torch.randn(4, 10) # Shape: (4, 10) y = model(x) print(y.shape) # Shape: (4, 2) You can run the script to check how the model works:\n1 python simple_model.py 3. Dataset We will build a simple dataset for demonstration. The dataset generates random data and labels.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # @file simple_dataset.py import torch from torch.utils.data import Dataset class SimpleDataset(Dataset): def __init__(self, size): self.size = size def __len__(self): return self.size def __getitem__(self, idx): x = torch.randn(10) # Shape: (10,); Element type: float32 y = torch.randint(0, 2, (1,)) # Shape: (1,); Element type: int64 return x, y if __name__ == \u0026#34;__main__\u0026#34;: dataset = SimpleDataset(4) x, y = dataset[0] print(x.shape, y.shape) # Shape: (10,), (1,) You can run the script to check how the dataset works:\n1 python simple_dataset.py 4. Dataloader As long as the dataset is built, creating a dataloader is quite easy.\nA dataloader will provide batch_size samples in each iteration. For example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 # @file temp.py from torch.utils.data import DataLoader from simple_dataset import SimpleDataset dataset = SimpleDataset(100) # Get a sample, shape: (10,), (1,) sample_x, sample_y = dataset[0] # Suppose batch_size is 16, the dataloader will provide 16 samples in each iteration dataloader = DataLoader(dataset, batch_size=16, shuffle=True, drop_last=True) for i, (x, y) in enumerate(dataloader): print(x.shape, y.shape) # Shape: (16, 10), (16, 1) break You can run the script to check how the dataloader works:\n1 python temp.py 5. Loss Function Different tasks require different loss functions. For example, a 2-class classification task can use nn.CrossEntropyLoss, while a regression task can use nn.MSELoss.\nIn our case, we will use nn.CrossEntropyLoss.\n6. Optimizer We will use torch.optim.SGD as the optimizer. torch.optim.Adam is also a good choice. This is a hyperparameter that you can tune.\n7. Trainpipeline Now we can build the trainpipeline. The trainpipeline will train the model on the dataset.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 # @file trainpipeline.py import torch import torch.nn as nn from torch.utils.data import DataLoader # This is the model we built from simple_model import SimpleModel # This is the dataset we built from simple_dataset import SimpleDataset DEVICE = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; BATCH_SIZE = 16 EPOCHS = 100 LEARNING_RATE = 0.01 def train(): # Create a model and move it to DEVICE model = SimpleModel().to(DEVICE) # Create train dataset and dataloader train_dataset = SimpleDataset(1000) val_dataset = SimpleDataset(100) train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True) val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False) # Create a loss function and an optimizer; The optimizer will update the model\u0026#39;s parameters criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE) for epoch in range(EPOCHS): model.train() # Set the model to training mode for i, (x, y) in enumerate(train_dataloader): x, y = x.to(DEVICE), y.to(DEVICE) optimizer.zero_grad() y_pred = model(x) loss = criterion(y_pred, y.squeeze()) loss.backward() optimizer.step() model.eval() # Set the model to evaluation mode with torch.no_grad(): # Disable gradient calculation total_loss = 0 total_correct = 0 total_samples = 0 for i, (x, y) in enumerate(val_dataloader): x, y = x.to(DEVICE), y.to(DEVICE) y_pred = model(x) loss = criterion(y_pred, y.squeeze()) total_loss += loss.item() total_correct += (y_pred.argmax(dim=1) == y.squeeze()).sum().item() total_samples += y.size(0) print(f\u0026#34;Epoch: {epoch}, Loss: {total_loss / total_samples}, Accuracy: {total_correct / total_samples}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: train() You can run the script to check how the trainpipeline works:\n1 python trainpipeline.py ","permalink":"https://jamesnulliu.github.io/blogs/a-simple-pytorch-trainpipeline/","summary":"How to build a simple Pytorch trainpipeline.","title":"A Simple Pytorch Trainpipeline"},{"content":"Term 13: Use objects to manage resources Do not make multiple std::auto_ptrs point to the same object.\nIf one is destroyed, the object would be released automatically.\nSimply put, do not use std::auto_ptr.\nUse RCSP (reference-counting smart pointer) to manage resources:\nstd::shared_ptr\nRemember: both auto_ptr and shared_ptr use delete instead of delete[]. As a result, do not write std::shared_ptr\u0026lt;int\u0026gt; spi(new int[1024]).\nTerm 14: Think carefully about copying behavior in resource-managing classes. Term 13 introduces such concept: Resource Acquisition Is Initialization, RAII. This concept is performed when std::auto_ptr and std::shared_ptr manage heap-based resource.\nNot all resource is heap-based, and it is usual that RCSP is not suitable resource handlers for these resource.\nThat is why sometimes you should write your own resource-managing classes.\nSuppose we use a mutex object with class Mutex, and there are two functions lock() and unlock(). In order not to forget to unlock any locked mutex, it is feasible to write a class Lock to manage the mutex. Example is shown as following:\nclass Lock { public: explicit Lock(Mutex* pm) : mutexPtr(pm) { lock(mutexPtr); } ~Lock() { unlock(mutexPtr); } private: Mutex* mutexPtr; } If we copy a Lock object to another, in most cases you would choose one of the following four options.\nTo Forbid Copying. By following term 6, you can make class Lock inherit from a base class whose copy constructor is declared private. Use Reference-Count for Underlying Resource. Use std::shared_ptr\u0026lt;Mutex\u0026gt; to manage Lock::mutexPtr in stead of Mutex*. However, one question is that what a std::shared_ptr do is when reference-count equals to 0, the underlying pointer is deleted, and that is not what we want. We want to call function unlock. The lucky thing is that std::share_ptr allow users to specify a deleter, so the class Lock can be written as follow: class Lock { public: explicit Lock(Mutex* pm) : mutexPtr(pm, unlock) // Use func {unlock} to sepecify a deleter and initialize a std::shared_ptr { lock(mutexPtr.get()); } // Destructor is omitted because {mutexPtr} would automatically invoke func {unlock}. private: std::shared_ptr\u0026lt;Mutex\u0026gt; mutexPtr; } Deep Copying. Transfer Ownership. For example: std::auto_ptr. Term 15: Provide access to raw resources in resource-managing classes std::shared_ptr and std::auto_ptr both provide get() methods to give access to the underlying raw pointers.\nclass Font { public: explicit Font(FontHandle fh) : f(fh) {} ~Font() { releaseFont(f); } FontHandle get() const { return f; } // Explicit conversion operator FontHandle() const { return f; } // Implicit conversion private: FontHandle f; // Raw font resource } Implicit conversion could be dangerous.\nTerm 16: Use the same form in corresponding uses of new and delete. Term 17: Store new objects in smart pointers in standalone statements. Suppose there is a function:\nvoid processWidget(std::shared_ptr\u0026lt;Widget\u0026gt; pw, int priority); Following call of the function is invalid:\nvoid processWidget(new Widget, priority()); The reason is that construction of std::shared_ptr is declared explicitly. If we adjust the statement to:\nvoid processWidget(std::share_ptr\u0026lt;Widget\u0026gt;(new Widget), priority()); This could cause memory leak. The above statement has to do 3 things:\nCall priority() Run new Widget Call constructor of std::shared_ptr In C++, in order to generate more efficient code, the execution sequence could be:\nRun new Widget Call priority() Call constructor of std::share_ptr In this situation, if an exception is thrown when priority() is called, the resource accuired by new Widget would not be free properly. This is memory leak.\nTo avoid this, write as following:\nstd::shared_ptr\u0026lt;Widget\u0026gt; pw(new Widget); processWidget(pw, priority()); ","permalink":"https://jamesnulliu.github.io/blogs/learning-notes-effective-cxx/03-resource-management/","summary":"My learning notes of \u0026ldquo;Effective C++\u0026rdquo; by Scott Meyers.","title":"03 | Resource Management"},{"content":"Term 05 Know what functions C++ silently writes and calls A compiler will automatically generate four functions for a class if the class does not define them by itself:\ndefault constructor copy constructor copy assignment operator destructor The destructor compiler automatically generates is non-virtual, unless that the base class\u0026rsquo;s destructor is virtual. (The virtualness mainly comes from the base class.)\nIf a class has reference members or const members, you have to define a copy assignment operator by yourself. The compiler would not generate a default copy assignment operator.\nIf some base class declares that its copy assignment operator is private, the compiler will refuse to generate a default copy assignment operator for its derived class.\nTerm 06 Explicitly disallow the use of compiler-generated functions you do not want One way to disallow the use of copy constructor and copy assignment (more specifically, the compiler-generated ones) is to inherit a class that declared private copy constructor and copy assignment operator.\nTerm 07 Declare destructor virtual in polymorphic base classes When deleting a base pointer pointing to a instance of a derived class, if destructor is declared non-virtual in base class, only the memory of base part would be released.\nConsider about a class Point:\nclass Point { public: Point(int xCoord, int yCoord); ~Point(); private: int x, y; } If \u0026ldquo;int\u0026rdquo; takes 32 bits, a Point instance takes 64 bits, and can be easily passed to other languages such as C and FORTRAN.\nHowever, if we declare ~Point() as a virtual function, a vptr (virtual table pointer) would be added to the instance. That causes the instance takes upto 128 bits (2 32-bit integer and 1 64-bit pointer). Moreover, since C and FORTRAN does not have vptr, the class is not portable any more.\nTo create an abstract class, you could (I think you\u0026rsquo;d better declare other functions pure virtual) declare a pure virtual destructor; But you has to offer a definition for the destructor outside the class (maybe in a cpp file).\nThe reason is that when an instance of a derived class is dectructed, the destructor of the most derived class is called, and each base class of them is called after. So there is an actor that the pure virtual destructor (of base class) is called inside the derived class.\nSum up Do not delcare a function virtual with no reason. Do not inherit frome a class that does not have a virtual destructor (e.g., std::string, std::set, std::vector, std::map) when the class is not designed for polymorphism. Term 08 Prevent exceptions from leaving destructors Trow exceptions out from a destructor is not encouraged by C++.\nSuppose there are several objects in a block. At the end of the block, all the objects are destroyed automatically.\nIf destructor of the first object throws an exception, every thing is okay; Destructors of the other objects would be called porperly.\nHowever, if the second destructor also throws an exception, the program would either be terminated or cause an undefined behavior (that would be fatal).\nA good strategy is to give the chance to users that they can handle the exceptions themselves.\nIn the following example code, db is an instance of class DBConnect, and meanwhile it is a member of class DBConn.\nBefore calling ~DBConn() automatically, user (instead of the compiler) should call DBConn::close() at first and handle the possible exception thrown by DBConnect::close().\nclass DBConn { public: ... void close() // A close function for users to use { db.close(); closed = true; } ~DBConn() { if(!closed){ try{ db.close(); } catch(...) { Log the faliure; } } } private: DBConnection db; bool closed; } Sum up Do not give any chance for an exception to leave a destructor. Destructor should catch and handle the exceptions inside itself. Offer a function to let user handle the exception that inside the destructor. Term 09: Never call virtual functions during construction or distruction. In C++, when constructing a derived class, the base part is constructed first; And during the construction of base part, the vptr is still pointing at the base class. This means, if you invoke a virtual function in the constructor of a base class, when you create an instance of a derived class, the actual called virtual function is the base version, not the overrided one.\nTerm 10: Have assignment operator return a reference to *this class Widget { public: Widget\u0026amp; operator+=(const Widget\u0026amp; rhs) { ...; return *this; } Widget\u0026amp; operator=(int rhs) { ...; return *this; } private: Bitmap* pb; }; Term 11: Handle assignment to self in operator= Self-assignment could cause a question that the resources are released before they are assigned.\nTraditionally, identity test can check whether there is an assignment to self:\nWidget\u0026amp; Widget::operator=(const Widget\u0026amp; rhs) { if(this == \u0026amp;rhs) return *this; // Identity test delete pb; pb = new Bitmap(*rhs.pb); return *this; } However, if the exception occurs (either because the memory is not enough when allocation or the copy constructor throws one exception), the pointer pb would ultimately points to a deleted Bitmap, and that is harmful.\nNowadays more people tends to care for exception safety rather than self-allocation safety. For example:\nWidget\u0026amp; Widget::operator=(const Widget\u0026amp; rhs) { Bitmap* pOrigin = pb; pb = new Bitmap(*rhs.pb); delete pOrigin; return *this; } Even if without identity test, self-assignment can be handled, and pb has no chance to point to a deleted Bitmap.\nIdentity test can be put back to the begin of the funtion; But that may lower the efficiency, since self-assignment does not happen so much.\nThere is another way called copy and swap technic. For example:\nWidget\u0026amp; Widget::operator=(const Widget\u0026amp; rhs) { Widget temp(rhs); this-\u0026gt;swap(temp); return *this; } Or:\nWidget\u0026amp; Widget::operator=(Widget rhs) { this-\u0026gt;swap(thd); return *this; } The second way sacrificces clearity; However, because it moves \u0026ldquo;copy\u0026rdquo; action from the function body to \u0026ldquo;parameter-constructing stage\u0026rdquo;, sometimes the compiler could generate more efficient codes.\nTerm 12: Copy all parts of an object. Compiler would not warn you if there is a particial copy, and do not let that happen.\nCopy constructor of a derived class should invoke the copy constructor of base class:\nclass Base { public: Base(const Base\u0026amp; rhs) : name(rhs.name), lastTransaction(rhs.lastTransaction) { log(); } private: std::string name; Date lastTransaction; }; class Derived : Base { public: Derived(const Derived\u0026amp; rhs) : Base(rhs), // Invoke the copy constructor of base class priority(rhs.priority) { log(); } private: int priority; } Do not have copy assignment operator call copy constructor, vice versa.\nIf you want, you can write a function init() additionally and call it in both functions.\n","permalink":"https://jamesnulliu.github.io/blogs/learning-notes-effective-cxx/02-constructors-destructors-and-assignment-operators/","summary":"My learning notes of \u0026ldquo;Effective C++\u0026rdquo; by Scott Meyers.","title":"02 | Constructors, Destructorsm and Assignment Operators"},{"content":"Term 01: View C++ as a federation of languages. Today\u0026rsquo;s C++ is a multiparadigm programming language, one supporting a combination of procedural, object-oriented, functional, generic, and metaprogramming features.\nTerm 02: Prefer consts, enums, inlines to #defines The substitution of a macro could result in multiple copies of the object in your object code, while the use of the constant should never result in more than one copy.\n🎼 Constant Pointer To define a constant char*-based string in a header file, for example, you have to write const twice:\n// File header.h const char* const authorName = \u0026#34;Scott Meyers\u0026#34;; 💡 Note: A constant object can be defined in a header file, and there will be no redefinition error when the header file is included in multiple source files.\n🎼 Static Constant Members of a Class To limit the scope of a constant to a class, you must make it a member, and to ensure there\u0026rsquo;s at most one copy of the constant, you must make it a static member:\n// File GamePlayer.h class GamePlayer { private: static const int NumTurns = 5; // Declaration of a const int scores[NumTurns]; } What you see above is a declaration for NumTurns, not a definition.\nUsually, C++ requires that you provide a definition for anything you use, but classspecific constants that are static and of integral type (e.g., integers, chars, bools) are an exception.\nAs long as you don\u0026rsquo;t take their address, you can declare them and use them without providing a definition. If you do take the address of a class constant, or if your compiler incorrectly insists on a definition even if you don\u0026rsquo;t take the address, you should provide a separate definition like this:\n// File GamePlayer.cpp const int GamePlayer::NumTurns; // Definition of a const You put this in an implementation file, not a header file. Because the initial value of class constants is provided where the constant is declared (e.g., NumTurns is initialized to 5 when it is declared), no initial value is permitted at the point of definition.\nFor non-integral types, you must provide a definition for the constant in the header file, like this:\n// File CostEstimate.h class A { private: static const double FudgeFactor; // Declaration of a class static const } // File CostEstimate.cpp const double CostEstimate::FudgeFactor = 1.35; // Defination of a class static const 💡 Keypoints:\nDeclare class-specific constants as static members of the class. Provide a separate definition in an implementation file if the compiler requires it. Only for static constants of integral type, provide an initial value at the point of declaration. Otherwise, provide an initial value of a static member at the point of definition. 🎼 Enum Hack class GamePlayer2 { private: enum { NumTurns = 5}; // \u0026#34;the enum hack\u0026#34; - let {NumTurns} be a marker of 5 int scores[NumTurns]; // valid } The enum hack is worth knowing about for several reasons.\nThe enum hack behaves in some ways more like a #define than a const does, and sometimes that\u0026rsquo;s what you want. It\u0026rsquo;s not legal to take the address of an enum, and it\u0026rsquo;s typically not legal to take the address of a #define, either. Also, like #defines, enums never result in unnecessary memory allocation. The enum hack is purely pragmatic. The enum hack is a fundamental technique of template metaprogramming (item 48). 🎼 Inline Use inline functions instead of #defines.\n#define CALL_WITH_MAX(a, b) f((a)\u0026gt;(b)?(a):(b)) int a = 5, b = 0; CALL_WITH_MAX(++a, b); // a is incremented twice CALL_WITH_MAX(++a, b+10); // a is incremented once template\u0026lt;class T\u0026gt; inline void callWithMax(const T\u0026amp; a, const T\u0026amp; b) { f(a \u0026gt; b ? a : b); } Term 03: Use const Whenever Possible 🎼 const and Pointers If the word const appears to the left of the asterisk, what\u0026rsquo;s pointed to is constant; if the word const appears to the right of the asterisk, the pointer itself is constant; if const appears on both sides, both are constant.\nFor example:\nchar greeting[] = \u0026#34;Hello\u0026#34;; char* p = greeting; // non-const pointer, non-const data const char* p = greeting; // non-const pointer, const data char* const p = greeting; // const pointer, non-const data const char* const p = greeting; // const pointer, const data 🎼 Use const to Restrict the User\u0026rsquo;s Behavior class A { public: A operator+(const A\u0026amp; a) { return A(); } }; int main() { A a1, a2; a1 + a2 = A(); // This is not expected. return 0; } Where a1 + a2 = A(); is not expected, because the result of a1 + a2 is a temporary object, and it is not allowed to assign a value to a temporary object.\nTo prevent this, you can add const to the return value of the operator+ function:\nclass A { public: const A operator+(const A\u0026amp; a) { return A(); } }; 🎼 Const Member Functions There are two prevailing notions: bitwise constness (also known as physical constness) and logical constness.\nThe bitwise const camp believes that a member function is const if and only if it doesn\u0026rsquo;t modify any of the object\u0026rsquo;s data members (excluding those that are static), i.e., if it doesn\u0026rsquo;t modify any of the bits inside the object.\nThe nice thing about bitwise constness is that it\u0026rsquo;s easy to detect violations: compilers just look for assignments to data members.\nUnfortunately, many member functions that don\u0026rsquo;t act very const pass the bitwise test. For exapmle:\nclass CTextBlock { public: ... char\u0026amp; operator[](std::size_t position) const { return pText[position]; // Not suitable } private: char* pText; } It is worth noting that you should return a const char\u0026amp; instead of a char\u0026amp; in the operator[] function above.\nThis leads to the notion of logical constness. Adherents to this philosophy (and you should be among them) — argue that a const member function might modify some of the bits in the object on which it\u0026rsquo;s invoked, but only in ways that clients cannot detect. For example:\nclass CTextBlock { public: std::size_t length() const { if(!lengthIsValid) { textLength = std::strlen(pText); lengthIsValid = true; } return textLength; } private: char* pText; mutable std::size_t textLength; mutable bool lengthIsValid; } 🎼 Avoiding Duplication in const and Non-const Member Functions When you have a const and a non-const member function that have essentially identical implementations, you can avoid code duplication by having the non-const member function call the const member function. For example:\nclass TextBlock { public: const char\u0026amp; operator[](std::size_t position) const { return text[position]; } char\u0026amp; operator[](std::size_t position) { return const_cast\u0026lt;char\u0026amp;\u0026gt;( static_cast\u0026lt;const TextBlock\u0026amp;\u0026gt;(*this)[position] ); } } 💡 Note: Do not avoiding duplication by having the const version call the non-const version. A const member function promises never to change the logical state of its object, but a non-const member function makes no such promise.\n🎼 Things to Remember Declaring something const helps compilers detect usage errors. const can be applied to objects at any scope, to function parameters and return types, and to member functions as a whole. Compilers enforce bitwise constness, but you should program using logical constness. When const and non-const member functions have essentially identical implementations, code duplication can be avoided by having the non-const version call the const version. Term 04: Make Sure the Objects are initialized before they are used Always initialize objects before they are used.\n🎼 Member initialization list Always use the member initialization list to initialize member objects.\nOne aspect of C++ that isn\u0026rsquo;t fickle is the order in which an object\u0026rsquo;s data is initialized. This order is always the same: base classes are initialized before derived classes (see also Item 12), and within a class, data members are initialized in the order in which they are declared.\n🎼 Initialize Static Objects A static object is one that exists from the time it\u0026rsquo;s constructed until the end of the program. Stack and heap-based objects are thus excluded.\nIncluded are:\nglobal objects objects defined at namespace scope objects declared static inside classes objects declared static inside functions objects declared static at file scope Static objects inside functions are known as local static objects (because they\u0026rsquo;re local to a function), and the other kinds of static objects are known as non-local static objects.\nStatic objects are destroyed when the program exits, i.e., their destructors are called when main finishes executing.\n⚠ Warning: If initialization of a non-local static object in one translation unit uses a non-local static object in a different translation unit, the object it uses could be uninitialized, because the relative order of initialization of non-local static objects defined in different translation units is undefined.\n💬 Multiple translation units and non-local static objects is generated through implicit template instantiations (which may themselves arise via implicit template instantiations). It\u0026rsquo;s not only impossible to determine the right order of initialization, it\u0026rsquo;s typically not even worth looking for special cases where it is possible to determine the right order.\nTo avoid the problem of undefined initialization order, you can use a function-local static object instead of a non-local static object. These functions return references to the objects they contain. (Aficionados of design patterns will recognize this as a common implementation of the Singleton Pattern.)\nFor example:\nclass FileSystem {...}; inline FileSystem\u0026amp; tfs() { static FileSystem fs; return fs; } class Directory {...}; Directory::Directory() { ... std::size_t disks = tfs().numDisks(); ... } inline Directory\u0026amp; tempDir() { static Directory td; return td; } This approach is founded on C++\u0026rsquo;s guarantee that local static objects are initialized when the object\u0026rsquo;s definition is first encountered during a call to that function. So if you replace direct accesses to non-local static objects with calls to functions that return references to local static objects, you\u0026rsquo;re guaranteed that the references you get back will refer to initialized objects. As a bonus, if you never call a function emulating a non-local static object, you never incur the cost of constructing and destructing the object, something that can\u0026rsquo;t be said for true non-local static objects.\nHowever, the fact that these functions contain static objects makes them problematic in multithreaded systems. Then again, any kind of non-const static object — local or non-local — is trouble waiting to happen in the presence of multiple threads.\nOne way to deal with such trouble is to manually invoke all the reference-returning functions during the single-threaded startup portion of the program. This eliminates initialization-related race conditions.\n🎼 Things to Remember Manually initialize objects of built-in type, because C++ only sometimes initializes them itself. In a constructor, prefer use of the member initialization list to assignment inside the body of the constructor. List data members in the initialization list in the same order they\u0026rsquo;re declared in the class. Avoid initialization order problems across translation units by replacing non-local static objects with local static objects. ","permalink":"https://jamesnulliu.github.io/blogs/learning-notes-effective-cxx/01-accustoming-yourself-to-cxx/","summary":"My learning notes of \u0026ldquo;Effective C++\u0026rdquo; by Scott Meyers.","title":"01 | Accustoming Yourself to C++"},{"content":"🤖 Brief Introduction Hello, I am JamesNULLiu (Yanchen Liu, 刘彦辰).\nI am a first-year MSCS student at University of Southern California, now working as a research intern in INK Lab @ USC, advised by Prof. Xiang Ren and Postdoc. Siyuan Wang. My current research focuses on RLHF, Test-time Computing and Reasoning of Large Language Models.\nI achieved my Bachelor\u0026rsquo;s degree in Computer Science from Shanghai University. During that time, I conducted in-depth research on Inference Acceleration, High Performance Computing, In-vehicle System Security and Super-Resolution.\nIf you have any inquiries or are interested in collaboration, please feel free to contact me via email at jamesnulliu@gmail.com.\n🧑‍🎓 Education History 2025.09 - 2027.06: Master of Computer Science, University of Southern California, Los Angeles, California, USA. 2021.09 - 2025.06: Bachelor of Computer Science, Shanghai University, Shanghai, China. 💻 Professional Experience 2025.07 - Present: Graduate Research Intern, INK Lab, University of Southern California. 2024.07 - 2025.06: MLE Intern, Shanghai AI Laboratory. 2023.03 - 2025.04: Undergraduate Research Assistant, SHUCS Lab, Shanghai University. 2023.03 - 2025.04: Team Leader, Shangai University Super Computing Team. 2023.06 - 2024.07: Undergraduate Research Intern, Shanghai University and East China Air Traffic Control Bureau. 🎉 Honors and Awards [2024.04] First Prize and Group Competition Award in 2024 ASC Student Supercomputer Challenge Global Final. [2022.06] First-Class Academic Scholarship for outstanding academic performance, Shanghai University. 📰 Publications 2025 \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\n[Preprint | Code] J. Lv, X. He, Y. Liu, A. Shen, X. Dai$^*$, Y. Li, J. Hao, J. Ding, Y. Hu, S. Yin. \u0026ldquo;HPCTransCompile: An AI Compiler Generated Dataset for High-Performance CUDA Transpilation and LLM Preliminary Exploration\u0026rdquo;. [Preprint | Code] Q. Liu$^\\dagger$, Y. Liu$^\\dagger$, R. Li, C. Cao, Y. Li$^*$, X. Li$^*$, P. Wang, R. Feng. \u0026ldquo;MDHP-Net: Detecting an Emerging Time-exciting Threat in IVN\u0026rdquo;. [Preprint] Z. Xu, A. Shen, D. Kong, X. Dai, J. Liu, Y. Liu, L. Wang, S. Wei, Y. Hu and S. Yin*. \u0026ldquo;LLMEngine: Disaggregated Mapping and Memory Management Co-scheduling for Wafer-scale Chips\u0026rdquo;. 2024 \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\n[IEEE Internet of Things Journal | Code] Q. Liu, X. Li, K. Sun, Y. Li$^*$ and Y. Liu$^*$. \u0026ldquo;SISSA: Real-Time Monitoring of Hardware Functional Safety and Cybersecurity With In-Vehicle SOME/IP Ethernet Traffic\u0026rdquo;. [MDPI Future Internet | Code] X. Li, R. Li, and Y. Liu. \u0026ldquo;HP-LSTM: Hawkes Process–LSTM-Based Detection of DDoS Attack for In-Vehicle Network\u0026rdquo;. 🤪 Hobbies 🧙 Animations, Comics and Games 🎸 Electric Guitar 🎼 Jazz, Fusion, Metal, Core, Djent ","permalink":"https://jamesnulliu.github.io/about_me/","summary":"\u003ch2 id=\"-brief-introduction\"\u003e🤖 Brief Introduction\u003c/h2\u003e\n\u003cp\u003eHello, I am JamesNULLiu (Yanchen Liu, 刘彦辰).\u003c/p\u003e\n\u003cp\u003eI am a first-year MSCS student at \u003cstrong\u003eUniversity of Southern California\u003c/strong\u003e, now working as a research intern in \u003ca href=\"https://inklab.usc.edu/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eINK Lab @ USC\u003c/a\u003e, advised by Prof. \u003ca href=\"https://www.seanre.com/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eXiang Ren\u003c/a\u003e and Postdoc. \u003ca href=\"https://siyuanwangw.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eSiyuan Wang\u003c/a\u003e. My current research focuses on \u003cspan style=\"color: #8860cc; font-weight: bold;\"\u003eRLHF\u003c/span\u003e, \u003cspan style=\"color: #8860cc; font-weight: bold;\"\u003eTest-time Computing\u003c/span\u003e and \u003cspan style=\"color: #8860cc; font-weight: bold;\"\u003eReasoning\u003c/span\u003e of \u003cspan style=\"color: #8860cc; font-weight: bold;\"\u003eLarge Language Models\u003c/span\u003e.\u003c/p\u003e\n\u003cp\u003eI achieved my Bachelor\u0026rsquo;s degree in Computer Science from \u003cstrong\u003eShanghai University\u003c/strong\u003e. During that time, I conducted in-depth research on \u003cstrong\u003eInference Acceleration\u003c/strong\u003e, \u003cstrong\u003eHigh Performance Computing\u003c/strong\u003e, \u003cstrong\u003eIn-vehicle System Security\u003c/strong\u003e and \u003cstrong\u003eSuper-Resolution\u003c/strong\u003e.\u003c/p\u003e","title":"About Me"},{"content":"FFmpeg is a powerful tool for video processing. It supports a wide range of codecs and formats. In this post, I will show you how to build FFmpeg with NVENC support.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # Install cuda 12.2 first sudo apt update sudo apt install autoconf automake build-essential cmake libass-dev \\ libfreetype6-dev libsdl2-dev libtool libva-dev libvdpau-dev \\ libvorbis-dev libxcb1-dev libxcb-shm0-dev libxcb-xfixes0-dev \\ pkg-config texinfo wget yasm zlib1g-dev git clone https://git.videolan.org/git/ffmpeg/nv-codec-headers.git cd nv-codec-headers \u0026amp;\u0026amp; sudo make install cd .. git clone git@github.com:FFmpeg/FFmpeg.git cd FFmpeg ./configure --prefix=/usr/local/ffmpeg --enable-nonfree \\ --enable-cuda-nvcc --disable-x86asm --nvcc=$CUDA_HOME/nvcc \\ --enable-gpl --enable-libass --enable-libfreetype --enable-libvorbis \\ --enable-libx265 --enable-cuvid --enable-nvenc --enable-libnpp \\ --extra-cflags=-I$CUDA_HOME/include --extra-ldflags=-L$CUDA_HOME/lib64 # Example: Following code encode a video with nvenc ffmpeg -i input.mp4 -c:v hevc_nvenc -preset fast -rc:v vbr_hq \\ -cq:v 19 -b:v 0 -s 1280x720 output.mp4 ","permalink":"https://jamesnulliu.github.io/blogs/build-ffmpeg-against-nvenc/","summary":"How to build FFmpeg against NVENC.","title":"Build FFmpeg against NVENC"},{"content":"Install build essentials.\n1 sudo apt install build-essential Check which version of gcc and g++ is installed on your system:\n1 gcc -v # or g++ -v Suppose that your currently installed gcc and g++ version is 11, you should be able to find gcc-11 and g++-11 under \u0026ldquo;/usr/bin/\u0026rdquo;:\n1 2 ls /usr/bin # All files under \u0026#34;/usr/bin\u0026#34; would be listd. Now, install gcc-13 and g++-13, while keeping older version existed:\n1 2 3 4 5 6 7 8 9 10 11 12 13 # Install gcc-13 and g++-13 sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt-get update sudo apt-get install gcc-13 g++-13 # Register gcc-11 and g++-11 as one group of alternatives sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 110 --slave /usr/bin/g++ g++ /usr/bin/g++-11 # Register gcc-13 and g++-13 as another group of alternatives sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-13 130 --slave /usr/bin/g++ g++ /usr/bin/g++-13 # Pop a prompt to select the default version of gcc, and g++ would be updated automatically sudo update-alternatives --config gcc For general purpose of C++ programing, it is suggested that gcc-11 and g++-11 is installed on your system. Some softwares may have a strict rule for gcc version not larger than 12.\n","permalink":"https://jamesnulliu.github.io/blogs/install-gcc-13-on-ubuntu-20/","summary":"How to install GCC-13 in Ubuntu 20 (or earlier).","title":"Install GCC-13 on Ubuntu 20 (or Earlier)"},{"content":" luminolt\u0026#39;s Page Cryptography Learner Jonathan523\u0026#39;s Page 每一个不曾起舞的日子, 都是对生命的辜负 Kosmo CHE\u0026#39;s Page 摇撼生命，至死方休 ","permalink":"https://jamesnulliu.github.io/friends/","summary":"\u003cp\u003e\u003ca target=\"_blank\" href=https://luminolt.cn/ title=luminolt\u0026#39;s\u0026#32;Page class=\"friendurl\"\u003e\n  \u003cdiv class=\"frienddiv\"\u003e\n    \u003cdiv class=\"frienddivleft\"\u003e\n      \u003cimg class=\"myfriend\" src=https://luminolt.cn/author/chenghao-chen/avatar_hu_79c871b478558618.jpg /\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"frienddivright\"\u003e\n      \u003cdiv class=\"friendname\"\u003eluminolt\u0026#39;s Page\u003c/div\u003e\n      \u003cdiv class=\"friendinfo\"\u003eCryptography Learner\u003c/div\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n\u003c/a\u003e\n\u003ca target=\"_blank\" href=https://www.cestlavie.moe/ title=Jonathan523\u0026#39;s\u0026#32;Page class=\"friendurl\"\u003e\n  \u003cdiv class=\"frienddiv\"\u003e\n    \u003cdiv class=\"frienddivleft\"\u003e\n      \u003cimg class=\"myfriend\" src=/imgs/people/Jonathan523.png /\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"frienddivright\"\u003e\n      \u003cdiv class=\"friendname\"\u003eJonathan523\u0026#39;s Page\u003c/div\u003e\n      \u003cdiv class=\"friendinfo\"\u003e每一个不曾起舞的日子, 都是对生命的辜负\u003c/div\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n\u003c/a\u003e\n\u003ca target=\"_blank\" href=https://kosmoche.github.io/ title=Kosmo\u0026#32;CHE\u0026#39;s\u0026#32;Page class=\"friendurl\"\u003e\n  \u003cdiv class=\"frienddiv\"\u003e\n    \u003cdiv class=\"frienddivleft\"\u003e\n      \u003cimg class=\"myfriend\" src=https://kosmoche.github.io/imgs/people/kosmoche.jpeg /\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"frienddivright\"\u003e\n      \u003cdiv class=\"friendname\"\u003eKosmo CHE\u0026#39;s Page\u003c/div\u003e\n      \u003cdiv class=\"friendinfo\"\u003e摇撼生命，至死方休\u003c/div\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n\u003c/a\u003e\u003c/p\u003e","title":"Friends"}]