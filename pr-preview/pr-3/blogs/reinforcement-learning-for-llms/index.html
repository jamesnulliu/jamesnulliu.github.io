<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Reinforcement Learning for LLMs | 秋水·JamesNULLiu</title><meta name=keywords content="reinforcement learning,llm,ppo,grpo,dapo,rlvr"><meta name=description content="1. Basics

RLHF: Reinforcement Learning from Human Feedback
SFT: Supervised Fine-Tuning

RL trains neural networks through trial and error. When finetuning a language model with RLHF, the model produces some text then receives a score/reward from a human annotator that captures the quality of that text. Then, we use RL to finetune the language model to generate outputs with high scores.
In this case, we cannot apply a loss function that trains the language model to maximize human preferences with supervised learning. This is because there’s no easy way to explain the score human give or connect it mathematically to the output of the neural network. In other words, we cannot backpropagate a loss applied to this score through the rest of the neural network. This would require that we are able to differentiate (i.e., compute the gradient of) the system that generates the score, which is a human that subjectively evaluates the generated text."><meta name=author content="jamesnulliu"><link rel=canonical href=https://jamesnulliu.github.io/blogs/reinforcement-learning-for-llms/><link crossorigin=anonymous href=/assets/css/stylesheet.62cb9c488bb33c0e9a9d3c29b7f4259cbb0db25aaa19ba672188203d3d5bcaf9.css integrity="sha256-YsucSIuzPA6anTwpt/QlnLsNslqqGbpnIYggPT1byvk=" rel="preload stylesheet" as=style><link rel=icon href=https://jamesnulliu.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://jamesnulliu.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://jamesnulliu.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://jamesnulliu.github.io/apple-touch-icon.png><link rel=mask-icon href=https://jamesnulliu.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://jamesnulliu.github.io/blogs/reinforcement-learning-for-llms/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]]}}</script><meta property="og:url" content="https://jamesnulliu.github.io/blogs/reinforcement-learning-for-llms/"><meta property="og:site_name" content="秋水·JamesNULLiu"><meta property="og:title" content="Reinforcement Learning for LLMs"><meta property="og:description" content="1. Basics RLHF: Reinforcement Learning from Human Feedback SFT: Supervised Fine-Tuning RL trains neural networks through trial and error. When finetuning a language model with RLHF, the model produces some text then receives a score/reward from a human annotator that captures the quality of that text. Then, we use RL to finetune the language model to generate outputs with high scores.
In this case, we cannot apply a loss function that trains the language model to maximize human preferences with supervised learning. This is because there’s no easy way to explain the score human give or connect it mathematically to the output of the neural network. In other words, we cannot backpropagate a loss applied to this score through the rest of the neural network. This would require that we are able to differentiate (i.e., compute the gradient of) the system that generates the score, which is a human that subjectively evaluates the generated text."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2025-06-02T08:25:00+08:00"><meta property="article:modified_time" content="2025-09-12T21:48:31+00:00"><meta property="article:tag" content="Rl4llm"><meta name=twitter:card content="summary"><meta name=twitter:title content="Reinforcement Learning for LLMs"><meta name=twitter:description content="1. Basics

RLHF: Reinforcement Learning from Human Feedback
SFT: Supervised Fine-Tuning

RL trains neural networks through trial and error. When finetuning a language model with RLHF, the model produces some text then receives a score/reward from a human annotator that captures the quality of that text. Then, we use RL to finetune the language model to generate outputs with high scores.
In this case, we cannot apply a loss function that trains the language model to maximize human preferences with supervised learning. This is because there’s no easy way to explain the score human give or connect it mathematically to the output of the neural network. In other words, we cannot backpropagate a loss applied to this score through the rest of the neural network. This would require that we are able to differentiate (i.e., compute the gradient of) the system that generates the score, which is a human that subjectively evaluates the generated text."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://jamesnulliu.github.io/blogs/"},{"@type":"ListItem","position":2,"name":"Reinforcement Learning for LLMs","item":"https://jamesnulliu.github.io/blogs/reinforcement-learning-for-llms/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reinforcement Learning for LLMs","name":"Reinforcement Learning for LLMs","description":"1. Basics RLHF: Reinforcement Learning from Human Feedback SFT: Supervised Fine-Tuning RL trains neural networks through trial and error. When finetuning a language model with RLHF, the model produces some text then receives a score/reward from a human annotator that captures the quality of that text. Then, we use RL to finetune the language model to generate outputs with high scores.\nIn this case, we cannot apply a loss function that trains the language model to maximize human preferences with supervised learning. This is because there’s no easy way to explain the score human give or connect it mathematically to the output of the neural network. In other words, we cannot backpropagate a loss applied to this score through the rest of the neural network. This would require that we are able to differentiate (i.e., compute the gradient of) the system that generates the score, which is a human that subjectively evaluates the generated text.\n","keywords":["reinforcement learning","llm","ppo","grpo","dapo","rlvr"],"articleBody":"1. Basics RLHF: Reinforcement Learning from Human Feedback SFT: Supervised Fine-Tuning RL trains neural networks through trial and error. When finetuning a language model with RLHF, the model produces some text then receives a score/reward from a human annotator that captures the quality of that text. Then, we use RL to finetune the language model to generate outputs with high scores.\nIn this case, we cannot apply a loss function that trains the language model to maximize human preferences with supervised learning. This is because there’s no easy way to explain the score human give or connect it mathematically to the output of the neural network. In other words, we cannot backpropagate a loss applied to this score through the rest of the neural network. This would require that we are able to differentiate (i.e., compute the gradient of) the system that generates the score, which is a human that subjectively evaluates the generated text.\nThe agent acts and receives rewards (and new states) from the environment. Problems that are solved via RL tend to be structured in a similar format. Namely, we have an agent that is interacting with an environment; see the figure above. The agent has a state in the environment and produces actions, which can modify the current state, as output. As the agent interacts with the environment, it can receive both positive and negative rewards for its actions. The agent’s goal is to maximize the rewards that it receives, but there is not a reward associated with every action taken by the agent. Rather, rewards may have a long horizon, meaning that it takes several correct, consecutive actions to generate any positive reward.\n2. Markov Decision Process (MDP) 2.1. Concepts and Definitions Markov Decision Process (MDP) is a way to formulate the system described above more formally and mathematically. Within an MDP, we have states, actions, rewards, transitions, and a policy, as shown in the equation below:\n$$ \\begin{cases} s \\in S \u0026 \\text{State} \\\\ a \\in A \u0026 \\text{Action} \\\\ r_s \\in \\mathbb{R} \u0026 \\text{Reward} \\\\ \\pi(a|s) \u0026 \\text{Policy} \\\\ T(s_{t+1}|s_{t},a_{t}) \u0026 \\text{Transition function} \\\\ \\end{cases} $$States and actions have discrete values, while rewards are real numbers.\nIn an MDP, we define two types of functions: transition and policy functions. The policy takes a state as input, then outputs a probability distribution over possible actions.\nNotably, the action that is chosen only depends on the current state and not any state history that precedes it. This is a key property of an MDP, which make the assumption that the next action only depends upon the current state.\nGiven this output, we can make a decision for the action to be taken from a current state, and the transition is then a function that outputs the next state based upon the prior state and chosen action. Using these components, the agent can interact with the environment in an iterative fashion, as the figure shown below.\nStructure of an MDP. The policy describes how the agent chooses its next action given the current state. The agent follows this strategy as it interacts with the environment. The goal is to learn a policy that maximizes the reward that the agent receives from the environment. As the agent interacts with the environment, we form a trajectory ($\\tau$) of states ($s$) and actions ($a$) that are chosen throughout this process. Then, given the reward ($r_s$) associated with each of these states, we get a total return ($R(\\tau)$) given by the equation below, where $\\gamma$ is the discount factor:\n$$ \\begin{cases} \\tau \u0026= \\{s_0, a_0, s_1, a_1, \\dots, s_t, a_t\\} \\quad \u0026\\text{(Trajectory)} \\\\ R(\\tau) \u0026= \\sum_t \\gamma^t r_{s_t} \\quad \u0026\\text{(Return)} \\end{cases} $$$R(\\tau)$ is the summed reward across the agent’s full trajectory, but rewards achieved at later time steps are exponentially discounted by the factor $\\gamma$ TL;DR. The fact that the discount rate is bounded to be smaller than 1 is a mathematical trick to make an infinite sum finite. This helps proving the convergence of certain algorithms. See Understanding the role of the discount factor in reinforcement learning. — This means that current rewards are more valuable than later rewards, due to both uncertainty and the simple fact that waiting to receive a reward is less desirable.\nThe goal of RL is to train an agent that maximizes this return. As shown by the equation below, we can characterize this as finding a policy that maximizes the return over trajectories that are sampled from the final policy:\nNote that the policy is a probability distribution over actions at each time step given the current state, so the exact trajectory produced is not deterministic. Many different trajectories can be obtained depending upon how we sample actions from the policy.\n$$ \\max_{\\pi} ~ \\mathbb{E}_{\\tau \\sim P_{\\pi, T}} ~ R(\\tau) $$where:\n$\\max_{\\pi}$ means to find the policy that yields the maximum return. $\\mathbb{E}_{\\tau \\sim P_{\\pi, T}}$ means to take the expectation or average over trajectories randomly sampled from a certain policy $\\pi$ and transition function $T$. $R(\\tau)$ is the return of the trajectory $\\tau$. 2.2. A Classical Example A classical example of an MDP is a maze, where the agent is trying to find the optimal path to the goal.\nA simple traditional RL environment. The target is to train the agent to find the optimal (largest return) solution path. States: The positions in the $2 \\times 3$ maze. The positions can be represented as a one-hot vector. Actions: The possible moves (up, down, left, right) — This is the agent’s (i.e., the model’s) each-step output. Rewards: The agent receives a reward of $+10$ for reaching the goal and $-10$ for reaching the trap. The agent receives a reward of $0$ for all other states. Transition function: The agent can move to an adjacent state based on the chosen action, but it cannot move through walls. The transition function defines how the agent moves from one state to another based on the action taken. Policy: The agent’s policy is a probability distribution over the possible actions given the current state. For example, if the agent is in the state (0, 0), it might have a policy that gives a high probability to moving right and a low probability to moving down. Trajectory: The sequence of states and actions taken by the agent as it navigates the maze. Like many problems that are solved with RL, this setup has an environment that is not differentiable (i.e., we can’t compute a gradient and train the model in a supervised fashion) and contains long-term dependencies, meaning that we might have to learn how to perform several sequential actions to get any reward.\n2.3. Taxonomy of modern RL algorithms Taxonomy of modern RL algorithms. 3. Deep Q-Leanring 3.1. Q-Learning Q-Learning is a model-free RL algorithm, meaning that we don’t have to learn a model for the environment with which the agent interacts. The goal of Q-Learning is to learn the value of any action at a particular state.\nThere are three key concepts to mention here:\nValue $Q(s, a)$ corresponds to choosing action $a$ at current state $s$. The value not only contains the determined reward by taking action $a$ at state $s$, but also contains a discounted and recursive future Q-value of the next state $s'$ after taking action $max_a$ (i.e., the best action at state $s'$). The higher a value $Q(s, a)$ is, the more valuable it is to take action $a$ at state $s$. A look-up table (Q-table) must be maintained to store the Q values for each state-action pair. The algorithm first initialize all Q values as zero and pick an initial state with which to start the learning process. Then, iterate over the following steps:\nPick an action to execute from the current state (using an $\\varepsilon$-Greedy Policy). Get a reward and next state from the (model-free) environment. Update the Q value in the Q-table based on the Bellman equation. Here we show a simplified update method which derives from the Bellman Optimality Equation and defines $Q(s_t, a_t)$ recursively:\n$$ Q(s_t, a_t) = r_t + \\gamma \\max_{a} Q(s_{t+1}, a) $$where:\n$Q(s_t, a_t)$ is the Q value of the current state $s_t$ and action $a_t$. $r_t$ is the reward received after taking action $a_t$ at state $s_t$. $\\gamma$ is the discount factor. $\\max_{a} Q(s_{t+1}, a)$ is the maximum Q value of the next state $s_{t+1}$ over all possible actions $a$. 3.2. Deep Q-Learning (DQL) In DQL, Q-table is replaced by a neural network.\nStructure of an DQL. In DQL, we have two neural networks: the Q network and the target network. These networks are identical, but the exact architecture they use depend upon the problem being solved7. To train these networks, we first gather data by interacting with the environment. This data is gathered using the current Q network with an ε-greedy policy. This process of gathering interaction data for training the Q network is referred to as experience replay; see above.\nFrom here, we use data that has been collected to train the Q network. During each training iteration, we sample a batch of data and pass it through both the Q network and the target network. The Q network takes the current state as input and predicts the Q value of the action that is taken (i.e., predicted Q value), while the target network takes the next state as input and predicts the Q value of the best action that can be taken from that state8 (i.e., target Q value).\nFrom here, we use the predicted Q value, the target Q value, and the observed reward to train the Q network with an MSE loss; see above. The target network is held fixed. Every several iterations, the weights of the Q network are copied to the target network, allowing this model to be updated as well. Then, we just repeat this process until the Q network converges. Notably, the dataset we obtain from experience replay is cumulative, meaning that we maintain all of the data we have observed from the environment throughout all iterations.\nWhy do we need the target network? The vanilla Q-learning framework leverages two Q values in its update rule: a (predicted) Q value for the current state-action pair and the (target) Q value of the best state-action pair for the next state. In DQL, we similarly have to generate both of these Q values. In theory, we could do this with a single neural network by making multiple passes through the Q network—one for the predicted Q value and one for the target Q value. However, the Q network’s weights are being updated at every training iteration, which would cause the target Q value to constantly fluctuate as the model is updated. To avoid this issue, we keep the target network separate and fixed, only updating its weights every several iterations to avoid creating a “moving target”.\nThis idea of using a separate network to produce a training target for another network—referred to as knowledge distillation [6]—is heavily utilized within deep learning. Furthermore, the idea of avoiding too much fluctuation in the weights of the teacher/target model has been addressed in this domain. For example, the mean teacher approach [7] updates the weights of the teacher model as an exponential moving average of the student network’s weights; see above. In this way, we can ensure a stable target is provided by the teacher during training.\n4. Policy Gradients In Policy Gradients, we will assume that our policy is a machine learning model (e.g., a deep neural network) with parameters $\\theta$. This policy takes a state as input and predicts some distribution over the action space. We use this output to decide what action should be taken next within the MDP:\n$$ \\begin{cases} s \\in S \u0026 \\text{State} \\\\ a \\in A \u0026 \\text{Action} \\\\ r_s \\in \\mathbb{R} \u0026 \\text{Reward} \\\\ \\pi_{\\theta}(a|s) \u0026 \\text{Policy} \\\\ T(s_{t+1}|s_{t},a_{t}) \u0026 \\text{Transition function} \\\\ \\end{cases} $$As our agent traverses the environment, it receives positive or negative reward signals for the actions it chooses and the states that it visits. Our goal is to learn a policy from these reward signals that maximizes total reward across an entire trajectory sampled from the policy. This idea is captured by the return, which sums the total rewards over an agent’s trajectory:\n$$ \\begin{cases} \\tau \u0026= \\{s_0, a_0, s_1, a_1, \\dots, s_t, a_t\\} \\quad \u0026\\text{(Trajectory)} \\\\ R(\\tau) \u0026= \\sum_t \\gamma^t r_{s_t} \\quad \u0026\\text{(Return)} \\end{cases} $$If $\\gamma \u003c 1$, then the return is Infinite-Horizon Discounted Return. If $\\gamma = 1$, then the return is Finite-Horizon Return.\n4.1. Value Functions and Advantage Functions One final concept that will be especially relevant is that value functions. In RL, there are four basic value functions, all of which assume the infinite-horizon discounted return:\n$$ \\begin{align*} V^{\\pi}(s) \u0026= \\mathbb{E}_{\\tau \\sim (\\pi,T)} [R(\\tau)|s_0 = s] \u0026\u0026 \\text{(On-Policy Value Function)} \\\\ Q^{\\pi}(s, a) \u0026= \\mathbb{E}_{\\tau \\sim (\\pi,T)} [R(\\tau)|s_0 = s, a_0 = a] \u0026\u0026 \\text{(On-Policy Action-Value Function)} \\\\ V^{*}(s) \u0026= \\max_{\\pi} \\mathbb{E}_{\\tau \\sim (\\pi,T)} [R(\\tau)|s_0 = s] \u0026\u0026 \\text{(Optimal Value Function)} \\\\ Q^{*}(s, a) \u0026= \\max_{\\pi} \\mathbb{E}_{\\tau \\sim (\\pi,T)} [R(\\tau)|s_0 = s, a_0 = a] \u0026\u0026 \\text{(Optimal Action-Value Function)} \\end{align*} $$ On-Policy Value Functio: expected return if starting in state $s$ and act according to policy $\\pi$ afterwards. On-Policy Action-Value Function: expected return if you start in state $s$, take some action $a$ (may not come from the current policy), and act according to policy $\\pi$ afterwards. Optimal Value Function: expected return if you start in state $s$ and always act according to the optimal policy afterwards. Optimal Action-Value Function: expected return if you start in state $s$, take some action a (may not come from the current policy), and act according to the optimal policy afterwards. There is an important connection between the optimal policy in an environment and the optimal action-value function. Namely, the optimal policy selects the action in state $s$ that maximizes the value of the optimal action-value function.\nUsing the value functions described above, we can define a special type of function called an advantage function, which is heavily used in RL algorithms based on policy gradients:\n$$ A^{\\pi}(s, a) = Q^{\\pi}(s, a) - V^{\\pi}(s) \\quad \\text{(Advantage Function)} $$ $Q^{\\pi}(s, a)$ on-policy action-value function. $V^{\\pi}(s)$ on-policy value function. Simply put, the advantage function characterizes how much better it is to take a certain action a relative to a randomly-selected action in state $s$ given a policy $\\pi$. Here, we should notice that the advantage function can be derived using the on-policy value and action-value functions defined before, as these functions assume that the agent acts according to a randomly-selected action from the policy $\\pi$.\nThe value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next. —— [3]\n4.2. Policy Optimization During the learning process, we aim to find parameters $\\theta$ for our policy that maximize the objective function below:\n$$ \\mathcal{J}(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim (\\pi_{\\theta},T)} [R(\\tau)] $$where:\n$\\pi_{\\theta}$ is the policy with network parameters $\\theta$. $\\tau$ is the trajectory sampled from the policy $\\pi_{\\theta}$ and transition function $T$. $T$ is the transition function that defines how the agent moves from one state to another based on the action taken. $R(\\tau)$ is the return of the trajectory $\\tau$. In words, this objective function measures the expected return of trajectories sampled from our policy within the specified environment.\nIf we want to find parameters $\\theta$ that maximize this objective function, one of the most fundamental techniques that we can use is gradient ascent, which iterates over parameters $\\theta$ using the update rule shown below:\n$$ \\theta_{t+1} = \\theta_t + \\alpha \\nabla_{\\theta} \\mathcal{J}(\\pi_{\\theta})|_{\\theta_t} $$Do a lot of math to compute $\\nabla_{\\theta} \\mathcal{J}(\\pi_{\\theta})$, and the final result of the basic policy gradient is:\n$$ \\nabla_{\\theta} \\mathcal{J}(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim (\\pi_{\\theta}, T)} \\left[ \\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) R(\\tau) \\right] $$Now, we have an actual expression for the gradient of our objective function that we can use in gradient ascent! Plus, this expression only depends on the return of a trajectory and the gradient of the log probability of an action given our current state. As long as we instantiate our policy such that the gradient of action probabilities is computable (e.g., this is pretty easy to do if our policy is implemented as a neural network), we can easily derive both of these quantities.\n4.3. Computing the Policy Gradient in Practice In practice, we can estimate the value of this expectation by sampling a fixed number of trajectories, by:\nSample several trajectories by letting the agent interact with the environment according to the current policy. Estimate the policy gradient using an average of relevant quantities over the fixed number of sample trajectories. Then given a set of sampled trajectories $\\mathcal{D} = \\{\\tau_0, \\tau_1, \\dots\\}$, we can estimate the policy gradient $\\overline{\\nabla_{\\theta} \\mathcal{J}(\\pi_{\\theta})}$ as follows:\n$$ \\overline{\\nabla_{\\theta} \\mathcal{J}(\\pi_{\\theta})} = \\frac{1}{|\\mathcal{D}|} \\sum_{\\tau \\in \\mathcal{D}} \\sum_{t=0}^{T} \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) R(\\tau) \\right] $$4.4. Vallina Poclicy Gradient (VPG) and Other Policy Gradients Given:\n$$ \\nabla_{\\theta} \\mathcal{J}(\\pi_{\\theta}) = \\mathbb{E} \\left[ \\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) \\Psi_t \\right] $$We have:\nBasic Policy Gradient: $$ \\Psi_t = R(\\tau) $$ Sum of all (potentially discounted) rewards obtained along the entire trajectory. Reward-to-Go: $$ \\Psi_t = \\sum_{i=t}^{T} r_{s_i, a_i} $$ Rewards after the current action. Reward-to-Go with Baseline: $$ \\Psi_t = \\sum_{i=t}^{T} r_{s_i, a_i} - b(s_i) $$ A baseline function to our expression that only depends on the current state. Vallina Policy Gradient (VPG): $$ \\Psi_t = A^{\\pi_{\\theta}}(s_t, a_t) = Q^{\\pi_{\\theta}}(s_t, a_t) - V^{\\pi_{\\theta}}(s_t) $$ where $A^{\\pi_{\\theta}}(s_t, a_t)$ is the advantage function, $Q^{\\pi_{\\theta}}(s_t, a_t)$ is the action-value function, and $V^{\\pi_{\\theta}}(s_t)$ is the value function. TODO: Why use VPG?\n5. Proximal Policy Optimization (PPO) DQL: can only be applied in relatively simple environments. VPG: has poor data efficiency and robustness, meaning that we must collect tons of data from our environment to eliminate noise within the policy gradient estimate. Motivation for TRPO and PPO:\nGenerally applicable (i.e., to both discrete and continuous problems) Data efficient Robust (i.e., works without too much tuning) Simple (i.e., not too difficult to understand/implement) TRPO satisfies the first two points outlined above, while PPO satisfies all four.\n5.1. Aligning LLMs with RL Basic procedure for aligning LLMs. After pre-training, the model perfroms next token prediction, but its output may be repetitive, uninteresting, or not useful. That’s the reason alignment is needed.\nTypically, we perform alignment by\nSelecting several alignment criteria (e.g., follow instructions, avoid harmful output, avoid hallucination, produce interesting/creative output, etc.) Finetuning the model —— via SFT and RLHF —— to satisfy these criteria. The final model can further finetuned and used to solve a downstream application via prompting (or in-context learning). Procedure of SFT and RLHF. As shown in the figure above, to apply RLHF:\nPrepare a set of prompts and generate several outputs for each prompt with the language model. Ask a group of human annotators to rank/score the responses to each prompt according to our alignment criteria. Use these ranked responses to train a reward model that predicts a human preference score from a language model’s response. Use PPO (or other algorithms, e.g., VPG, TRPO) to finetune our language model to maximize the human preferences scores (predicted by the reward model) of its outputs. 5.2. Kullback–Leibler (KL) Divergence At the highest level, the Kullback-Leibler (KL) Divergence is just a method of comparing two probability distributions.\nThe idea of KL divergence has its roots in information theory and is highly related to the concept of entropy According to Shannon’s Source Coding Theorem, the optimal number of bits required to encode a message with probability $p(x)$ is given by $−\\log_{2}{p(x)}$.\nHigh probability event ($p(x) \\approx 1$): $−log_2​(1)=0$. It takes very few bits to encode a highly probable event.\nLow probability event ($p(x) \\approx 0$): $−log_2​(p(x))$ is a large number. It takes many bits to encode a rare event. :\n$$ H= \\begin{cases} -\\mathbb{E} \\left[ \\log p(x) \\right] \u0026\\text{(Continuous Case)} \\\\ -\\sum_{i=1}^{N} p(x_i) \\cdot \\log p(x_i) \u0026\\text{(Discrete Case)} \\end{cases} $$In the equation above, we can see common formulations of entropy $H$ for a probability distribution $p$. Intuitively, the entropy value captures how much information is stored within a probability distribution —— a lower entropy means that you would need fewer bits to encode the information stored within $p$.\nInstead of a single probability distribution $p$, the KL divergence considers two probability distributions: $p$ and $q$. Then, mirroring the above entropy formulation, we compute KL divergence by finding the expected difference in log probabilities between these two distributions:\n$$ \\begin{align*} D_{\\text{KL}}(p||q) \u0026= H(p, q) - H(p) \\\\ \u0026= \\begin{cases} \\mathbb{E} \\left[ \\log p(x) - \\log q(x) \\right] ~~~ \\text{(Continuous Case)} \\\\ \\sum_{i=1}^{N} p(x_i) \\cdot \\left( \\log p(x_i) - \\log q(x_i) \\right) ~~~ \\text{(Discrete Case)} \\end{cases} \\end{align*} $$where:\n$H(p,q)$: The average number of bits used with the approximate code. $H(p)$: The minimum possible average number of bits used with the optimal code. $D_{\\text{KL}}(p||q)$: The penalty, or the expected number of extra bits “wasted” or “lost” per message due to the approximation. The KL divergence is commonly explained in the context of approximations. Namely, if we approximate $p$ with $q$, the KL divergence is the number of bits we would expect to lose by making this approximation.\nKL divergence is heavily used across different domains of AI/ML research. For example, it is commonly used in loss functions for training neural networks, either as the core loss or as an added regularization term.\nThe final reward function we use during optimization contains a [KL divergence] penalty term … we find this constraint is useful for training stability, and to reduce reward hacking.” —— [4]\n5.3. Trust Region Policy Optimization (TRPO) VPG is limited by the fact that it can only perform a single policy update for each estimate of the policy gradient that is derived. Given that VPG is notoriously data inefficient, meaning that we have to sample a lot of data when deriving a policy update, performing multiple (or larger) updates may seem enticing. However, such an approach is not justified theoretically and, in practice, leads to policy updates that are too large, thus damaging performance.\nTrust Region Policy Optimization (TRPO) [5] aims to solve the problem described above using an approach that is similar to VPG. At each step of the optimization process, however, we find the largest possible policy update that still improves performance. Simply put, TRPO allows us to learn faster by finding a reliable way to make larger policy updates that do not damage performance.\nMore specifically, we update the policy under a constraint—based on the KL divergence—that captures the distance between policies before and after the current update. Considering this constraint allows us to find a balance between update size and the amount of change to the underlying policy:\n$$ \\begin{equation*} \\begin{gathered} \\theta_{k+1} = \\operatorname{argmax}_{\\theta} \\mathbb{E}_{(s,a) \\sim (\\pi_{\\theta_k}, T)} \\left[ \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)} A^{\\pi_{\\theta_k}}(s,a) \\right] \\\\ \\text{such that } \\overline{D}_{\\text{KL}}(\\theta||\\theta_k) \u003c \\delta \\end{gathered} \\end{equation*} $$where:\n$\\mathbb{E}_{(s,a) \\sim (\\pi_{\\theta_k}, T)}$ is the expectation over state-action pairs sampled from the current policy $\\pi_{\\theta_k}$ and transition function $T$. $\\pi_{\\theta}(a|s)$ is the probability of taking action $a$ in state $s$ according to the new policy $\\pi_{\\theta}$. $\\pi_{\\theta_k}(a|s)$ is the probability of taking action $a$ in state $s$ according to the current policy $\\pi_{\\theta_k}$. $A^{\\pi_{\\theta_k}}(s,a)$ is the advantage function for the current policy $\\pi_{\\theta_k}$. $\\overline{D}_{\\text{KL}}(\\theta||\\theta_k)$ is the average KL divergence between the new policy $\\pi_{\\theta}$ and the current policy $\\pi_{\\theta_k}$. $\\delta$ is a hyperparameter that controls the maximum allowed change in the policy. Formulation of TRPO has several critical differences from VPG:\nThe terms in the expectation are modified slightly to express the probability of a given action a as a ratio between old and updated policies. The update has an added constraint based on the KL divergence between old and updated policies. Instead of performing gradient ascent, we are solving a constrained maximization problem to generate each new policy The implementation of TRPO is similar to that of VPG. We allow our current policy to interact with the environment and collect data. From this observed data, we can compute the approximate update for TRPO as described above. Then, we can continue the process of collecting data and performing an update until we arrive at a policy that performs quite well.\nBecause we are using the actual policy being trained to collect the data used to train it, TRPO is an on-policy reinforcement learning algorithm.\n5.4. TRPO vs. VPG: Larger Policy Updates As mentioned previously, the VPG algorithm is based upon gradient ascent, which —— by nature —— ensures that updates to the policy’s parameters $\\theta$ are not too large. In particular, we use a learning rate to perform updates with VPG, which can control the size of the update in the parameter space:\n$$ \\theta_{t+1} = \\theta_t + \\alpha \\nabla_{\\theta} J(\\pi_{\\theta})|_{\\theta_t} $$Here, only the size of the update to $\\theta$ is controlled, and so that the old and updated policies are close in the parameter space. However, small changes to $\\theta$ may also drastically alter the policy, because ensuring that policy updates are small in the parameter space does not provide much of a guarantee on changes to the resulting policy.\nAs a result, we are constrained to relatively small updates within the VPG algorithm —— larger or multiple updates could be harmful.\nTRPO sidesteps this issue by considering the size of our policy update from an alternative viewpoint. Namely, we compare updated and old policies using the KL divergence, which measures the difference in probability distributions over the action space produced by the two policies. Such an approach compares policies based upon the actions they take rather than their underlying parameters $\\theta$.\nIn this way, we can perform large policy updates while ensuring that the new policy does not produce actions that are significantly different from the old policy.\n5.5. Proximal Policy Optimization (PPO) We introduce proximal policy optimization, a family of policy optimization methods that use multiple epochs of stochastic gradient ascent to perform each policy update. These methods have the stability and reliability of trust-region methods but are much simpler to implement … applicable in more general settings, and have better overall performance. —— [6]\nTRPO has improved data efficiency, stability, and reliability compared to the VPG algorithm, but there are still limitations that need to be addressed.\nNamely, the algorithm is complicated, can only perform a single update each time new data is sampled from the environment, and is only applicable to certain problem setups.\nAiming to develop a better approach, authors in [6] propose Proximal Policy Optimization (PPO), another policy gradient algorithm that alternates between collecting data from the environment and performing several epochs of training over this sampled data. PPO shares the reliability of TRPO and is:\nMuch simpler More data efficient More generally applicable Similar to TRPO, we perform policy updates in PPO according to a surrogate objective. However, this surrogate objective has a “clipped” probability ratio, as shown in the equation below:\n$$ L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t \\left[ \\min(\\frac{\\pi_{\\theta}(o_t|q, o_{\u003c t})}{\\pi_{\\theta_{old}}(o_t|q, o_{\u003c t})}A_t, \\text{CLIP}(\\frac{\\pi_{\\theta}(o_t|q, o_{\u003c t})}{\\pi_{\\theta_{old}}(o_t|q, o_{\u003c t})}, 1-\\varepsilon, 1+\\varepsilon) A_t) \\right] $$The surrogate objective for PPO is expressed as a minimum of two values. The first value is the same surrogate objective from TRPO, while the second value is a “clipped” version of this objective that lies within a certain range. In practice, this expression is formulated such that there is no reward for moving the probability ratio beyond the interval $[1 - \\varepsilon, 1 + \\varepsilon]$, see the figure below:\nFrom [2]. In other words, PPO has no incentive for excessively large policy updates. Plus, by taking the minimum of the clipped and unclipped version of the surrogate objective, we only ignore excessive changes to the probability ratio if they improve the underlying objective. In the figure above, we see a basic depiction of this trend for both positive and negative values of the advantage function.\nTo understand PPO’s surrogate objective more intuitively, we should look at the figure below, which plots several objective functions as we interpolate between an old and updated policy obtained via PPO. In this figure, we see the KL divergence, the TRPO surrogate objective (labeled as CPI), the clipped surrogate objective, and the full PPO surrogate objective. From these plots, we can see that the PPO surrogate objective is a pessimistic/lower bound for the TRPO surrogate objective, where a penalty is incurred for having too large of a policy update.\nFrom [2]. While TRPO sets a hard constraint to avoid policy updates that are too large, PPO simply formulates the surrogate objective such that a penalty is incurred if the KL divergence is too large. Such an approach is much simpler, as we no longer have to solve a difficult, constrained optimization problem. Rather, we can compute PPO’s surrogate loss with only minor tweaks to the VPG algorithm.\nPPO has several benefits compared to TRPO. First, the implementation of PPO is much simpler compared to TRPO, as we can use automatic differentiation and gradient-based optimization techniques9 instead of deriving an (approximate) solution for a complex, constrained objective function. Additionally, while TRPO makes only a single policy update each time new data is collected, PPO performs multiple epochs of optimization via stochastic gradient ascent over the surrogate objective, which improves data efficiency.\nFinally, computing estimates of the advantage function (e.g., via Generalized Advantage Estimation (GAE)) typically requires that we learn a corresponding value function. In TRPO, we must learn this state-value function with a separate neural network. However, PPO—due to its compatibility with a wider scope of architectures (including those with parameter sharing) —— can train a joint network for policy and value functions by just adding an extra term to the loss function that computes the mean-squared error (MSE) between estimated and actual value function values.\n$$ L^{\\text{CLIP+VF}}(\\theta) = \\mathbb{E}_t \\left[ L^{\\text{CLIP}}(\\theta) - c_1 L^{\\text{VF}}(\\theta) \\right] $$where:\n$L^{\\text{CLIP}}(\\theta)$ is the PPO surrogate objective. $L^{\\text{VF}}(\\theta)$ is the MSE loss for the value function. $c_1$ is a hyperparameter that controls the weight of the value function loss in the overall loss function. 6. Group Relative Policy Optimization (GRPO) From [7]. Different from PPO, GRPO:\nRemoves the value function model. The policy model generates multiple outputs for each input, and the reward model calculates the reward for each output, and calculates the advantage scores after group computation. Removes the GAE, and changes the method to calculate KL. In PPO, we optimizes LLMs by maximizing the following objective function:\n$$ \\begin{align*} \\mathcal{J}_{\\text{PPO}}(\\theta) = \\mathbb{E}\u0026_{q \\sim P(Q), o \\sim \\pi_{\\theta_{\\text{old}}}(O|q)} \\left[ \\frac{1}{|o|} \\sum_{t=1}^{|o|} \\min \\left( \\frac{\\pi_{\\theta}(o_t|q, o_{\u003c t})}{\\pi_{\\theta_{\\text{old}}}(o_t|q, o_{\u003c t})} A_t, \\right. \\right. \\\\ \u0026 \\left. \\left. \\text{clip}\\left(\\frac{\\pi_{\\theta}(o_t|q, o_{\u003c t})}{\\pi_{\\theta_{\\text{old}}}(o_t|q, o_{\u003c t})}, 1-\\varepsilon, 1+\\varepsilon\\right) A_t \\right) \\right] \\end{align*} $$where:\n$\\pi_{\\theta}$ and $\\pi{\\theta_{\\text{old}}}$ are the current and old policy models; $q$, $o$ are questions and outputs sampled from the question dataset and the old policy $\\pi{\\theta_{\\text{old}}}$; $\\varepsilon$ is a clipping-related hyper-parameter introduced in PPO for stabilizing training; $A_t$ is the advantage, which is computed by applying Generalized Advantage Estimation (GAE), based on the rewards ${r_{\\geq t}}$ and a learned value function $V_{\\psi}$. Thus, in PPO, a value function needs to be trained alongside the policy model and to mitigate over-optimization of the reward model, the standard approach is to add a per-token KL penalty from a reference model in the reward at each token, i.e.:\n$$ r_t = r_{\\varphi}(q, o_{\\leq t}) - \\beta \\log \\frac{\\pi_{\\theta}(o_t|q, o_{\u003c t})}{\\pi_{ref}(o_t|q, o_{\u003c t})} $$There are several issues with PPO:\nAs the value function employed in PPO is typically another model of comparable size as the policy model, it brings a substantial memory and computational burden. During RL training, the value function is treated as a baseline in the calculation of the advantage for variance reduction. While in the LLM context, usually only the last token is assigned a reward score by the reward model, which may complicate the training of a value function that is accurate at each token. To address these issues, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\dots , o_G\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model by maximizing the following objective:\n$$ \\begin{align*} \\mathcal{J}_{\\text{GRPO}}(\\theta) = \\mathbb{E}\u0026[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] \\\\ \u0026\\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\left\\{ \\min\\left(\\frac{\\pi_{\\theta}(o_{i,t}|q, o_{i,\u003c t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t}|q, o_{i,\u003c t})} \\hat{A}_{i,t}, \\right.\\right. \\\\ \u0026\\left.\\left. \\text{clip}\\left(\\frac{\\pi_{\\theta}(o_{i,t}|q, o_{i,\u003c t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t}|q, o_{i,\u003c t})}, 1-\\varepsilon, 1+\\varepsilon\\right) \\hat{A}_{i,t}\\right) - \\beta D_{KL}[\\pi_{\\theta}||\\pi_{ref}] \\right\\} \\end{align*} $$where:\n$\\varepsilon$ and $\\beta$ are hyper-parameters; $\\hat{A}_{i,t}$ is the advantage calculated based on relative rewards of the outputs inside each group only. See here for verl’s implementation. For simplicity, given input scores (a tensor of padded rewards with shape (batch_size, response_len)), the algorithm first sums the rewards for each response, then normalizes the rewards in each group (a batch may contain multiple groups), and finally broadcasts and multiplies the rewards (of shape (batch_size, 1)) with reponse mask (of shape (batch_size, response_len)) to get the advantage That is to say, for each response of shape (1, response_len), the advantages are the same, except for the padded positions, which are 0. . Note that:\nInstead of adding KL penalty in the reward, GRPO regularizes by directly adding the KL divergence between the trained policy and the reference policy to the loss, avoiding complicating the calculation of $\\hat{A}_{i,t}$. Different from the KL penalty term used in [6], GRPO estimate the KL divergence with the following unbiased estimator: $$ \\mathbb{D}_{\\text{KL}}\\left[ \\pi_{\\theta}||\\pi_{ref}\\right] = \\frac{\\pi_{ref}(o_{t,t}|q, o_{t,\u003c t})}{\\pi_{\\theta}(o_{t,t}|q, o_{t,\u003c t})} - \\log \\frac{\\pi_{ref}(o_{t,t}|q, o_{t,\u003c t})}{\\pi_{\\theta}(o_{t,t}|q, o_{t,\u003c t})} - 1 $$ which is guaranteed to be positive. The code of the objective function, i.e., compute_policy_loss, is show here in verl. It can be seen that the implementation is a bit different from the equation above:\nThe $\\frac {\\pi_{\\theta}(o_{i,t}|q, o_{i,\u003c t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t}|q, o_{i,\u003c t})}$ term is replaced with $e^{\\log \\pi_{\\theta}(o_{i,t}|q, o_{i,\u003c t}) - \\log \\pi_{\\theta_{\\text{old}}}(o_{i,t}|q, o_{i,\u003c t})}$, which is more numerically stable, as shown in these lines in function compute_policy_loss. The kl penalty term is removed, and calculated separately in DataParallelPPOActor.update_policy, according to option use_kl_loss (which is default false but shoule be set to true for GRPO), as shown here. Both the policy loss and the KL loss would be passed to function agg_loss, which aggregates them to scalar values. The default method is token-mean, which is different from “the original GRPO paper takes the sample-level loss (seq-mean-token-mean), which may be unstable in long-CoT scenarios”. All GRPO example scripts provided in verl uses the default configuration “token-mean” for loss aggregation instead.\nBtw, token-mean will means the (policy gradient) loss across all the tokens in all the sequences in a mini-batch; An idea of DAPO? 7. DAPO: An Open-Source LLM Reinforcement Learning System at Scale [8] Clip-Higher, which promotes the diversity of the system and avoids entropy collapse; Configured here and used here in compute_policy_loss. Dynamic Sampling, which improves training efficiency and stability; Shown here in RayDPAOTrainer. Token-Level Policy Gradient Loss, which is critical in long-CoT RL scenarios; Configured here, which has been introduced in the above GRPO section. Overlong Reward Shaping, which reduces reward noise and stabilizes training; Configured here and used here in DapoRewardManager. Before defining the object function, it is worth noting that the KL term is excluded from the proposed algorithm.\nThe KL penalty term is used to regulate the divergence between the online policy and the frozen reference policy. In the RLHF scenario, the goal of RL is to align the model behavior without diverging too far from the initial model. However, during training the long-CoT reasoning model, the model distribution can diverge significantly from the initial model, thus this restriction is not necessary.\nThe objective function of DAPO is shown below:\n$$ \\begin{align*} \\mathcal{J}_{\\text{DAPO}}(\\theta) =\u0026 ~ \\mathbb{E}_{(q,a) \\sim \\mathcal{D}, \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(\\cdot|q)} \\\\ \u0026\\left[ \\frac{1}{\\sum_{i=1}^G |o_i|} \\sum_{i=1}^G \\sum_{t=1}^{|o_i|} \\min \\left( r_{i,t}(\\theta)\\hat{A}_{i,t}, \\text{clip}(r_{i,t}(\\theta), 1 - \\epsilon_{\\text{low}}, 1 + \\epsilon_{\\text{high}})\\hat{A}_{i,t} \\right) \\right] \\\\ \\text{s.t. } \u00260 \u003c \\left | \\{o_i \\mid \\text{is_equivalent}(a, o_i)\\} \\right | \u003c G, \\end{align*} $$where:\n$$ r_{i,t}(\\theta) = \\frac{\\pi_\\theta(o_{i,t} \\mid q, o_{i, \u003c t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t} \\mid q, o_{i, \u003c t})}, \\quad \\hat{A}_{i,t} = \\frac{R_i - \\text{mean}(\\{R_i\\}_{i=1}^G)}{\\text{std}(\\{R_i\\}_{i=1}^G)} $$8. Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning [9] Token entropy calculation:\n$$ \\begin{gather*} H_t := - \\sum_{j=1}^{V} p_{t,j} \\log p_{t,j} \\\\ \\text{ where } (p_{t,1}, \\cdots, p_{t,V}) = \\boldsymbol{p}_t = \\pi_{\\theta}(\\cdot \\mid q, o_{ \u003c t}) = \\text{Softmax}\\left(\\frac{\\mathbf{z}_t}{T}\\right) \\end{gather*} $$Here,\n$\\pi_{\\theta}$ denotes an LLM parameterized by $\\theta$ $q$ is the input query, and $o_{\u003c t} = (o_1, o_2, \\ldots, o_{t-1})$ represents the previously generated tokens $V$ is the vocabulary size $\\mathbf{z}_t \\in \\mathbb{R}^V$ denotes the pre-softmax logits at time step $t$ $\\boldsymbol{p}_t \\in \\mathbb{R}^V$ is the corresponding probability distribution over the vocabulary $T \\in \\mathbb{R}$ is the decoding temperature In off-policy settings, sequences are generated by a rollout policy $\\pi_{\\phi}$ while the training policy is $\\pi_{\\theta}$, with $\\phi \\neq \\theta$. The entropy is still calculated using $\\pi_{\\theta}$, as defined in Equation (1), to measure the uncertainty of the training policy in the given sequence.\nReferences Cameron R. Wolfe. Basics of Reinforcement Learning for LLMs. Cameron R. Wolfe. Proximal Policy Optimization (PPO): The Key to LLM Alignment. Achiam, Josh. Spinning Up in Deep RL. OpenAI, 2018. Touvron, Hugo, et al. “Llama 2: Open foundation and fine-tuned chat models.” arXiv preprint arXiv:2307.09288 (2023). Schulman, John, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. “Trust region policy optimization.” In International conference on machine learning, pp. 1889-1897. PMLR, 2015. Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. “Proximal policy optimization algorithms.” arXiv preprint arXiv:1707.06347 (2017). Shao, Zhihong, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang et al. “Deepseekmath: Pushing the limits of mathematical reasoning in open language models.” arXiv preprint arXiv:2402.03300 (2024). Yu, Qiying, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai et al. “Dapo: An open-source llm reinforcement learning system at scale.” arXiv preprint arXiv:2503.14476 (2025). Wang, Shenzhi, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang et al. “Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning.” arXiv preprint arXiv:2506.01939 (2025). ","wordCount":"6316","inLanguage":"en","datePublished":"2025-06-02T08:25:00+08:00","dateModified":"2025-09-12T21:48:31Z","author":[{"@type":"Person","name":"jamesnulliu"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://jamesnulliu.github.io/blogs/reinforcement-learning-for-llms/"},"publisher":{"@type":"Organization","name":"秋水·JamesNULLiu","logo":{"@type":"ImageObject","url":"https://jamesnulliu.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://jamesnulliu.github.io/ accesskey=h title="秋水·JamesNULLiu (Alt + H)">秋水·JamesNULLiu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://jamesnulliu.github.io/zh/ title=简体中文 aria-label=简体中文>简体中文</a></li></ul></div></div><ul id=menu><li><a href=https://jamesnulliu.github.io/ title=Home><span>Home</span></a></li><li><a href=https://jamesnulliu.github.io/about_me/ title="About Me"><span>About Me</span></a></li><li><a href=https://jamesnulliu.github.io/blogs/ title=Blogs><span>Blogs</span></a></li><li><a href=https://jamesnulliu.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://jamesnulliu.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://jamesnulliu.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://jamesnulliu.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://jamesnulliu.github.io/friends/ title=Friends><span>Friends</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://jamesnulliu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://jamesnulliu.github.io/blogs/>Blogs</a></div><h1 class="post-title entry-hint-parent">Reinforcement Learning for LLMs</h1><div class=post-meta><span title='2025-06-02 08:25:00 +0800 +0800'>Jun-02-2025</span>&nbsp;·&nbsp;30 min&nbsp;·&nbsp;6316 words&nbsp;·&nbsp;jamesnulliu</div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-basics aria-label="1. Basics">1. Basics</a></li><li><a href=#2-markov-decision-process-mdp aria-label="2. Markov Decision Process (MDP)">2. Markov Decision Process (MDP)</a><ul><li><a href=#21-concepts-and-definitions aria-label="2.1. Concepts and Definitions">2.1. Concepts and Definitions</a></li><li><a href=#22-a-classical-example aria-label="2.2. A Classical Example">2.2. A Classical Example</a></li><li><a href=#23-taxonomy-of-modern-rl-algorithms aria-label="2.3. Taxonomy of modern RL algorithms">2.3. Taxonomy of modern RL algorithms</a></li></ul></li><li><a href=#3-deep-q-leanring aria-label="3. Deep Q-Leanring">3. Deep Q-Leanring</a><ul><li><a href=#31-q-learning aria-label="3.1. Q-Learning">3.1. Q-Learning</a></li><li><a href=#32-deep-q-learning-dql aria-label="3.2. Deep Q-Learning (DQL)">3.2. Deep Q-Learning (DQL)</a></li></ul></li><li><a href=#4-policy-gradients aria-label="4. Policy Gradients">4. Policy Gradients</a><ul><li><a href=#41-value-functions-and-advantage-functions aria-label="4.1. Value Functions and Advantage Functions">4.1. Value Functions and Advantage Functions</a></li><li><a href=#42-policy-optimization aria-label="4.2. Policy Optimization">4.2. Policy Optimization</a></li><li><a href=#43-computing-the-policy-gradient-in-practice aria-label="4.3. Computing the Policy Gradient in Practice">4.3. Computing the Policy Gradient in Practice</a></li><li><a href=#44-vallina-poclicy-gradient-vpg-and-other-policy-gradients aria-label="4.4. Vallina Poclicy Gradient (VPG) and Other Policy Gradients">4.4. Vallina Poclicy Gradient (VPG) and Other Policy Gradients</a></li></ul></li><li><a href=#5-proximal-policy-optimization-ppo aria-label="5. Proximal Policy Optimization (PPO)">5. Proximal Policy Optimization (PPO)</a><ul><li><a href=#51-aligning-llms-with-rl aria-label="5.1. Aligning LLMs with RL">5.1. Aligning LLMs with RL</a></li><li><a href=#52-kullbackleibler-kl-divergence aria-label="5.2. Kullback–Leibler (KL) Divergence">5.2. Kullback–Leibler (KL) Divergence</a></li><li><a href=#53-trust-region-policy-optimization-trpo aria-label="5.3. Trust Region Policy Optimization (TRPO)">5.3. Trust Region Policy Optimization (TRPO)</a></li><li><a href=#54-trpo-vs-vpg-larger-policy-updates aria-label="5.4. TRPO vs. VPG: Larger Policy Updates">5.4. TRPO vs. VPG: Larger Policy Updates</a></li><li><a href=#55-proximal-policy-optimization-ppo aria-label="5.5. Proximal Policy Optimization (PPO)">5.5. Proximal Policy Optimization (PPO)</a></li></ul></li><li><a href=#6-group-relative-policy-optimization-grpo aria-label="6. Group Relative Policy Optimization (GRPO)">6. Group Relative Policy Optimization (GRPO)</a></li><li><a href=#7-dapo-an-open-source-llm-reinforcement-learning-system-at-scale-8 aria-label="7. DAPO: An Open-Source LLM Reinforcement Learning System at Scale [8]">7. DAPO: An Open-Source LLM Reinforcement Learning System at Scale [8]</a></li><li><a href=#8-beyond-the-8020-rule-high-entropy-minority-tokens-drive-effective-reinforcement-learning-for-llm-reasoning-9 aria-label="8. Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning [9]">8. Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning [9]</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{let e=null;const t=window.innerHeight+window.pageYOffset>=document.body.offsetHeight-100;if(t)e=elements[elements.length-1];else{let t=null,n=1/0;elements.forEach(e=>{const s=getOffsetTop(e)-window.pageYOffset;if(s<=window.innerHeight*.3){const o=Math.abs(s);o<n&&(n=o,t=e)}}),e=t||elements[0]}if(e&&e!==activeElement){if(activeElement){const t=encodeURI(activeElement.getAttribute("id")).toLowerCase(),e=document.querySelector(`.inner ul li a[href="#${t}"]`);e&&e.classList.remove("active")}activeElement=e;const n=encodeURI(activeElement.getAttribute("id")).toLowerCase(),t=document.querySelector(`.inner ul li a[href="#${n}"]`);t&&(t.classList.add("active"),document.getElementById("toc-container").classList.contains("wide")&&scrollTocToActiveItem(t))}},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}function scrollTocToActiveItem(e){const t=document.querySelector(".toc .inner");if(!t||!e)return;const n=t.getBoundingClientRect(),s=e.getBoundingClientRect(),o=n.height/2,i=s.top-n.top+t.scrollTop,a=i-o;t.scrollTo({top:Math.max(0,a),behavior:"smooth"})}</script><div class=post-content><h2 id=1-basics>1. Basics<a hidden class=anchor aria-hidden=true href=#1-basics>#</a></h2><ul><li>RLHF: Reinforcement Learning from Human Feedback</li><li>SFT: Supervised Fine-Tuning</li></ul><p>RL trains neural networks through <strong>trial</strong> and <strong>error</strong>. When finetuning a language model with RLHF, the model produces some text then receives a score/reward from a human annotator that captures the quality of that text. Then, we use RL to finetune the language model <strong>to generate outputs with high scores</strong>.</p><p>In this case, we cannot apply a loss function that trains the language model to maximize human preferences with supervised learning. This is because there’s no easy way to explain the score human give or connect it mathematically to the output of the neural network. In other words, we cannot backpropagate a loss applied to this score through the rest of the neural network. This would require that we are able to <strong>differentiate (i.e., compute the gradient of) the system that generates the score</strong>, which is a human that subjectively evaluates the generated text.</p><div class=image-container><img src=/imgs/blogs/reinforcement-learning-for-llms/rl-structure.webp alt class=image width=85%><div class=image-caption>The agent acts and receives rewards (and new states) from the environment.</div></div><p>Problems that are solved via RL tend to be structured in a similar format. Namely, we have an <strong>agent</strong> that is interacting with an <strong>environment</strong>; see the figure above. The agent has a state in the environment and produces actions, which can modify the current state, as output. As the agent interacts with the environment, it can receive both positive and negative rewards for its actions. <strong>The agent’s goal is to maximize the rewards that it receives, but there is not a reward associated with every action taken by the agent</strong>. Rather, rewards may have a long horizon, meaning that it takes several correct, consecutive actions to generate any positive reward.</p><h2 id=2-markov-decision-process-mdp>2. Markov Decision Process (MDP)<a hidden class=anchor aria-hidden=true href=#2-markov-decision-process-mdp>#</a></h2><h3 id=21-concepts-and-definitions>2.1. Concepts and Definitions<a hidden class=anchor aria-hidden=true href=#21-concepts-and-definitions>#</a></h3><p>Markov Decision Process (MDP) is a way to formulate the system described above more formally and mathematically. Within an MDP, we have <strong>states</strong>, <strong>actions</strong>, <strong>rewards</strong>, <strong>transitions</strong>, and a <strong>policy</strong>, as shown in the equation below:</p>$$
\begin{cases}
s \in S & \text{State} \\
a \in A & \text{Action} \\
r_s \in \mathbb{R} & \text{Reward} \\
\pi(a|s) & \text{Policy} \\
T(s_{t+1}|s_{t},a_{t}) & \text{Transition function} \\
\end{cases}
$$<p>States and actions have discrete values, while rewards are real numbers.</p><p>In an MDP, we define two types of functions: <strong>transition and policy functions</strong>. The policy takes a state as input, then outputs a probability distribution over possible actions.</p><blockquote><p>Notably, the action that is chosen only depends on the current state and not any state history that precedes it. This is a key property of an MDP, which make the assumption that the next action only depends upon the current state.</p></blockquote><p>Given this output, we can make a decision for the action to be taken from a current state, and the transition is then a function that outputs the next state based upon the prior state and chosen action. Using these components, the agent can interact with the environment in an iterative fashion, as the figure shown below.</p><div class=image-container><img src=/imgs/blogs/reinforcement-learning-for-llms/mdp-structure.webp alt class=image width=80%><div class=image-caption>Structure of an MDP.</div></div><ul><li>The policy describes how the agent chooses its next action given the current state.</li><li>The agent follows this strategy as it interacts with the environment.</li><li>The goal is to learn a policy that maximizes the reward that the agent receives from the environment.</li></ul><p>As the agent interacts with the environment, we form a <strong>trajectory</strong> ($\tau$) of <strong>states</strong> ($s$) and <strong>actions</strong> ($a$) that are chosen throughout this process. Then, given the <strong>reward</strong> ($r_s$) associated with each of these states, we get a total return ($R(\tau)$) given by the equation below, where $\gamma$ is the discount factor:</p>$$
\begin{cases}
\tau &= \{s_0, a_0, s_1, a_1, \dots, s_t, a_t\} \quad &\text{(Trajectory)} \\
R(\tau) &= \sum_t \gamma^t r_{s_t} \quad &\text{(Return)}
\end{cases}
$$<p>$R(\tau)$ is the summed reward across the agent&rsquo;s full trajectory, but <strong>rewards achieved at later time steps are exponentially discounted by the factor $\gamma$</strong>
<span class=sidenote-number><small class=sidenote>TL;DR. The fact that the discount rate is bounded to be smaller than 1 is a mathematical trick to make an infinite sum finite. This helps proving the convergence of certain algorithms. See <a href=https://stats.stackexchange.com/questions/221402/understanding-the-role-of-the-discount-factor-in-reinforcement-learning>Understanding the role of the discount factor in reinforcement learning</a>.</small>
</span>&mdash; This means that current rewards are more valuable than later rewards, due to both uncertainty and the simple fact that waiting to receive a reward is less desirable.</p><p><strong>The goal of RL is to train an agent that maximizes this return</strong>. As shown by the equation below, we can characterize this as finding a policy that maximizes the return over trajectories that are sampled from the final policy:</p><blockquote><p>Note that the policy is a probability distribution over actions at each time step given the current state, so the exact trajectory produced is not deterministic. Many different trajectories can be obtained depending upon how we sample actions from the policy.</p></blockquote>$$
\max_{\pi} ~ \mathbb{E}_{\tau \sim P_{\pi, T}} ~ R(\tau)
$$<p>where:</p><ul><li>$\max_{\pi}$ means to find the policy that yields the maximum return.</li><li>$\mathbb{E}_{\tau \sim P_{\pi, T}}$ means to take the expectation or average over trajectories randomly sampled from a certain policy $\pi$ and transition function $T$.</li><li>$R(\tau)$ is the return of the trajectory $\tau$.</li></ul><h3 id=22-a-classical-example>2.2. A Classical Example<a hidden class=anchor aria-hidden=true href=#22-a-classical-example>#</a></h3><p>A classical example of an MDP is a maze, where the agent is trying to find the optimal path to the goal.</p><div class=image-container><img src=/imgs/blogs/reinforcement-learning-for-llms/agent-in-maze.webp alt class=image width=70%><div class=image-caption>A simple traditional RL environment. The target is to train the agent to find the optimal (largest return) solution path.</div></div><ul><li><strong>States</strong>: The positions in the $2 \times 3$ maze. The positions can be represented as a one-hot vector.</li><li><strong>Actions</strong>: The possible moves (up, down, left, right) &mdash; This is the agent&rsquo;s (i.e., the model&rsquo;s) each-step output.</li><li><strong>Rewards</strong>: The agent receives a reward of $+10$ for reaching the goal and $-10$ for reaching the trap. The agent receives a reward of $0$ for all other states.</li><li><strong>Transition function</strong>: The agent can move to an adjacent state based on the chosen action, but it cannot move through walls. The transition function defines how the agent moves from one state to another based on the action taken.</li><li><strong>Policy</strong>: The agent&rsquo;s policy is a probability distribution over the possible actions given the current state. For example, if the agent is in the state (0, 0), it might have a policy that gives a high probability to moving right and a low probability to moving down.</li><li><strong>Trajectory</strong>: The sequence of states and actions taken by the agent as it navigates the maze.</li></ul><p>Like many problems that are solved with RL, this setup has an environment that is not differentiable (i.e., we can’t compute a gradient and train the model in a supervised fashion) and contains long-term dependencies, meaning that we might have to learn how to perform several sequential actions to get any reward.</p><h3 id=23-taxonomy-of-modern-rl-algorithms>2.3. Taxonomy of modern RL algorithms<a hidden class=anchor aria-hidden=true href=#23-taxonomy-of-modern-rl-algorithms>#</a></h3><div class=image-container><img src=/imgs/blogs/reinforcement-learning-for-llms/taxonomy-of-modern-rl-algorithms.webp alt class=image width=100%><div class=image-caption>Taxonomy of modern RL algorithms.</div></div><h2 id=3-deep-q-leanring>3. Deep Q-Leanring<a hidden class=anchor aria-hidden=true href=#3-deep-q-leanring>#</a></h2><h3 id=31-q-learning>3.1. Q-Learning<a hidden class=anchor aria-hidden=true href=#31-q-learning>#</a></h3><p>Q-Learning is a model-free RL algorithm, meaning that we don’t have to learn a model for the environment with which the agent interacts. The goal of Q-Learning is to <strong>learn the value of any action at a particular state</strong>.</p><p>There are three key concepts to mention here:</p><ol><li>Value $Q(s, a)$ corresponds to choosing action $a$ at current state $s$. The value not only contains the determined reward by taking action $a$ at state $s$, but also contains a <strong>discounted</strong> and <strong>recursive</strong> future Q-value of the next state $s'$ after taking action $max_a$ (i.e., the best action at state $s'$).</li><li>The higher a value $Q(s, a)$ is, the more valuable it is to take action $a$ at state $s$.</li><li>A look-up table (Q-table) must be maintained to store the Q values for each state-action pair.</li></ol><p>The algorithm first initialize all Q values as zero and pick an initial state with which to start the learning process. Then, iterate over the following steps:</p><ul><li>Pick an action to execute from the current state (using an $\varepsilon$-Greedy Policy).</li><li>Get a reward and next state from the (model-free) environment.</li><li>Update the Q value in the Q-table based on the Bellman equation.</li></ul><p>Here we show a simplified update method which derives from the Bellman Optimality Equation and defines $Q(s_t, a_t)$ recursively:</p>$$
Q(s_t, a_t) = r_t + \gamma \max_{a} Q(s_{t+1}, a)
$$<p>where:</p><ul><li>$Q(s_t, a_t)$ is the Q value of the current state $s_t$ and action $a_t$.</li><li>$r_t$ is the reward received after taking action $a_t$ at state $s_t$.</li><li>$\gamma$ is the discount factor.</li><li>$\max_{a} Q(s_{t+1}, a)$ is the maximum Q value of the next state $s_{t+1}$ over all possible actions $a$.</li></ul><h3 id=32-deep-q-learning-dql>3.2. Deep Q-Learning (DQL)<a hidden class=anchor aria-hidden=true href=#32-deep-q-learning-dql>#</a></h3><p>In DQL, Q-table is replaced by a neural network.</p><div class=image-container><img src=/imgs/blogs/reinforcement-learning-for-llms/dql-structure.webp alt class=image width=90%><div class=image-caption>Structure of an DQL.</div></div><p>In DQL, we have two neural networks: the Q network and the target network. These networks are identical, but the exact architecture they use depend upon the problem being solved7. To train these networks, we first gather data by interacting with the environment. This data is gathered using the current Q network with an ε-greedy policy. This process of gathering interaction data for training the Q network is referred to as experience replay; see above.</p><p>From here, we use data that has been collected to train the Q network. During each training iteration, we sample a batch of data and pass it through both the Q network and the target network. The Q network takes the current state as input and predicts the Q value of the action that is taken (i.e., predicted Q value), while the target network takes the next state as input and predicts the Q value of the best action that can be taken from that state8 (i.e., target Q value).</p><p>From here, we use the predicted Q value, the target Q value, and the observed reward to train the Q network with an MSE loss; see above. The target network is held fixed. Every several iterations, the weights of the Q network are copied to the target network, allowing this model to be updated as well. Then, we just repeat this process until the Q network converges. Notably, the dataset we obtain from experience replay is cumulative, meaning that we maintain all of the data we have observed from the environment throughout all iterations.</p><p>Why do we need the target network? The vanilla Q-learning framework leverages two Q values in its update rule: a (predicted) Q value for the current state-action pair and the (target) Q value of the best state-action pair for the next state. In DQL, we similarly have to generate both of these Q values. In theory, we could do this with a single neural network by making multiple passes through the Q network—one for the predicted Q value and one for the target Q value. However, the Q network’s weights are being updated at every training iteration, which would cause the target Q value to constantly fluctuate as the model is updated. To avoid this issue, we keep the target network separate and fixed, only updating its weights every several iterations to avoid creating a “moving target”.</p><p>This idea of using a separate network to produce a training target for another network—referred to as knowledge distillation [6]—is heavily utilized within deep learning. Furthermore, the idea of avoiding too much fluctuation in the weights of the teacher/target model has been addressed in this domain. For example, the mean teacher approach [7] updates the weights of the teacher model as an exponential moving average of the student network’s weights; see above. In this way, we can ensure a stable target is provided by the teacher during training.</p><h2 id=4-policy-gradients>4. Policy Gradients<a hidden class=anchor aria-hidden=true href=#4-policy-gradients>#</a></h2><p>In Policy Gradients, we will assume that our policy is a machine learning model (e.g., a deep neural network) with parameters $\theta$. This policy takes a state as input and predicts some distribution over the action space. We use this output to decide what action should be taken next within the MDP:</p>$$
\begin{cases}
s \in S & \text{State} \\
a \in A & \text{Action} \\
r_s \in \mathbb{R} & \text{Reward} \\
\pi_{\theta}(a|s) & \text{Policy} \\
T(s_{t+1}|s_{t},a_{t}) & \text{Transition function} \\
\end{cases}
$$<p>As our agent traverses the environment, it receives positive or negative reward signals for the actions it chooses and the states that it visits. Our goal is to learn a policy from these reward signals that maximizes total reward across an entire trajectory sampled from the policy. This idea is captured by the return, which sums the total rewards over an agent’s trajectory:</p>$$
\begin{cases}
\tau &= \{s_0, a_0, s_1, a_1, \dots, s_t, a_t\} \quad &\text{(Trajectory)} \\
R(\tau) &= \sum_t \gamma^t r_{s_t} \quad &\text{(Return)}
\end{cases}
$$<p>If $\gamma < 1$, then the return is <strong>Infinite-Horizon Discounted Return</strong>. If $\gamma = 1$, then the return is <strong>Finite-Horizon Return</strong>.</p><h3 id=41-value-functions-and-advantage-functions>4.1. Value Functions and Advantage Functions<a hidden class=anchor aria-hidden=true href=#41-value-functions-and-advantage-functions>#</a></h3><p>One final concept that will be especially relevant is that <strong>value functions</strong>. In RL, there are four basic value functions, all of which assume the infinite-horizon discounted return:</p>$$
\begin{align*}
V^{\pi}(s) &= \mathbb{E}_{\tau \sim (\pi,T)} [R(\tau)|s_0 = s] && \text{(On-Policy Value Function)} \\
Q^{\pi}(s, a) &= \mathbb{E}_{\tau \sim (\pi,T)} [R(\tau)|s_0 = s, a_0 = a] && \text{(On-Policy Action-Value Function)} \\
V^{*}(s) &= \max_{\pi} \mathbb{E}_{\tau \sim (\pi,T)} [R(\tau)|s_0 = s] && \text{(Optimal Value Function)} \\
Q^{*}(s, a) &= \max_{\pi} \mathbb{E}_{\tau \sim (\pi,T)} [R(\tau)|s_0 = s, a_0 = a] && \text{(Optimal Action-Value Function)}
\end{align*}
$$<ul><li><strong>On-Policy Value Functio</strong>: expected return if starting in state $s$ and act according to policy $\pi$ afterwards.</li><li><strong>On-Policy Action-Value Function</strong>: expected return if you start in state $s$, take some action $a$ (may not come from the current policy), and act according to policy $\pi$ afterwards.</li><li><strong>Optimal Value Function</strong>: expected return if you start in state $s$ and always act according to the optimal policy afterwards.</li><li><strong>Optimal Action-Value Function</strong>: expected return if you start in state $s$, take some action a (may not come from the current policy), and act according to the optimal policy afterwards.</li></ul><p>There is an important connection between the optimal policy in an environment and the optimal action-value function. Namely, the optimal policy selects the action in state $s$ that maximizes the value of the optimal action-value function.</p><p>Using the value functions described above, we can define a special type of function called an advantage function, which is heavily used in RL algorithms based on policy gradients:</p>$$
A^{\pi}(s, a) = Q^{\pi}(s, a) - V^{\pi}(s) \quad \text{(Advantage Function)}
$$<ul><li>$Q^{\pi}(s, a)$ on-policy action-value function.</li><li>$V^{\pi}(s)$ on-policy value function.</li></ul><p>Simply put, the advantage function characterizes <strong>how much better it is to take a certain action a relative to a randomly-selected action in state $s$ given a policy $\pi$</strong>. Here, we should notice that the advantage function can be derived using the on-policy value and action-value functions defined before, as these functions assume that the agent acts according to a randomly-selected action from the policy $\pi$.</p><blockquote class=quote><p>The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next. &mdash;&mdash; [3]</p></blockquote><h3 id=42-policy-optimization>4.2. Policy Optimization<a hidden class=anchor aria-hidden=true href=#42-policy-optimization>#</a></h3><p>During the learning process, we aim to find parameters $\theta$ for our policy that maximize the objective function below:</p>$$
\mathcal{J}(\pi_{\theta}) = \mathbb{E}_{\tau \sim (\pi_{\theta},T)} [R(\tau)]
$$<p>where:</p><ul><li>$\pi_{\theta}$ is the policy with network parameters $\theta$.</li><li>$\tau$ is the trajectory sampled from the policy $\pi_{\theta}$ and transition function $T$.</li><li>$T$ is the transition function that defines how the agent moves from one state to another based on the action taken.</li><li>$R(\tau)$ is the return of the trajectory $\tau$.</li></ul><p>In words, this objective function measures the expected return of trajectories sampled from our policy within the specified environment.</p><p>If we want to find parameters $\theta$ that maximize this objective function, one of the most fundamental techniques that we can use is gradient ascent, which iterates over parameters $\theta$ using the update rule shown below:</p>$$
\theta_{t+1} = \theta_t + \alpha \nabla_{\theta} \mathcal{J}(\pi_{\theta})|_{\theta_t}
$$<p>Do a lot of math to compute $\nabla_{\theta} \mathcal{J}(\pi_{\theta})$, and the final result of the basic policy gradient is:</p>$$
\nabla_{\theta} \mathcal{J}(\pi_{\theta}) = \mathbb{E}_{\tau \sim (\pi_{\theta}, T)} \left[ \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) R(\tau) \right]
$$<p>Now, we have an actual expression for the gradient of our objective function that we can use in gradient ascent! Plus, this expression only depends on the return of a trajectory and the gradient of the log probability of an action given our current state. As long as we instantiate our policy such that the gradient of action probabilities is computable (e.g., this is pretty easy to do if our policy is implemented as a neural network), we can easily derive both of these quantities.</p><h3 id=43-computing-the-policy-gradient-in-practice>4.3. Computing the Policy Gradient in Practice<a hidden class=anchor aria-hidden=true href=#43-computing-the-policy-gradient-in-practice>#</a></h3><p>In practice, we can estimate the value of this expectation by sampling a fixed number of trajectories, by:</p><ul><li>Sample several trajectories by letting the agent interact with the environment according to the current policy.</li><li>Estimate the policy gradient using an average of relevant quantities over the fixed number of sample trajectories.</li></ul><p>Then given a set of sampled trajectories $\mathcal{D} = \{\tau_0, \tau_1, \dots\}$, we can estimate the policy gradient $\overline{\nabla_{\theta} \mathcal{J}(\pi_{\theta})}$ as follows:</p>$$
\overline{\nabla_{\theta} \mathcal{J}(\pi_{\theta})} = \frac{1}{|\mathcal{D}|} \sum_{\tau \in \mathcal{D}} \sum_{t=0}^{T} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) R(\tau) \right]
$$<h3 id=44-vallina-poclicy-gradient-vpg-and-other-policy-gradients>4.4. Vallina Poclicy Gradient (VPG) and Other Policy Gradients<a hidden class=anchor aria-hidden=true href=#44-vallina-poclicy-gradient-vpg-and-other-policy-gradients>#</a></h3><p>Given:</p>$$
\nabla_{\theta} \mathcal{J}(\pi_{\theta}) = \mathbb{E} \left[ \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) \Psi_t \right]
$$<p>We have:</p><ol><li><strong>Basic Policy Gradient</strong>:
$$
\Psi_t = R(\tau)
$$
Sum of all (potentially discounted) rewards obtained along the entire trajectory.</li><li><strong>Reward-to-Go</strong>:
$$
\Psi_t = \sum_{i=t}^{T} r_{s_i, a_i}
$$
Rewards after the current action.</li><li><strong>Reward-to-Go with Baseline</strong>:
$$
\Psi_t = \sum_{i=t}^{T} r_{s_i, a_i} - b(s_i)
$$
A baseline function to our expression that only depends on the current state.</li><li><strong>Vallina Policy Gradient (VPG)</strong>:
$$
\Psi_t = A^{\pi_{\theta}}(s_t, a_t) = Q^{\pi_{\theta}}(s_t, a_t) - V^{\pi_{\theta}}(s_t)
$$
where $A^{\pi_{\theta}}(s_t, a_t)$ is the advantage function, $Q^{\pi_{\theta}}(s_t, a_t)$ is the action-value function, and $V^{\pi_{\theta}}(s_t)$ is the value function.</li></ol><p><strong>TODO</strong>: Why use VPG?</p><h2 id=5-proximal-policy-optimization-ppo>5. Proximal Policy Optimization (PPO)<a hidden class=anchor aria-hidden=true href=#5-proximal-policy-optimization-ppo>#</a></h2><ul><li>DQL: can only be applied in relatively simple environments.</li><li>VPG: has poor data efficiency and robustness, meaning that we must collect tons of data from our environment to eliminate noise within the policy gradient estimate.</li></ul><p>Motivation for TRPO and PPO:</p><ul><li>Generally applicable (i.e., to both discrete and continuous problems)</li><li>Data efficient</li><li>Robust (i.e., works without too much tuning)</li><li>Simple (i.e., not too difficult to understand/implement)</li></ul><p>TRPO satisfies the first two points outlined above, while PPO satisfies all four.</p><h3 id=51-aligning-llms-with-rl>5.1. Aligning LLMs with RL<a hidden class=anchor aria-hidden=true href=#51-aligning-llms-with-rl>#</a></h3><div class=image-container><img src=/imgs/blogs/reinforcement-learning-for-llms/align-procedure.webp alt class=image width=100%><div class="image-caption image-caption-center">Basic procedure for aligning LLMs.</div></div><p>After pre-training, the model perfroms next token prediction, but its output may be repetitive, uninteresting, or not useful. That&rsquo;s the reason alignment is needed.</p><p>Typically, we perform alignment by</p><ol><li>Selecting several alignment criteria (e.g., follow instructions, avoid harmful output, avoid hallucination, produce interesting/creative output, etc.)</li><li>Finetuning the model &mdash;&mdash; via SFT and RLHF &mdash;&mdash; to satisfy these criteria.</li><li>The final model can further finetuned and used to solve a downstream application via prompting (or in-context learning).</li></ol><div class=image-container><img src=/imgs/blogs/reinforcement-learning-for-llms/sft-and-rlhf.webp alt class=image width=100%><div class="image-caption image-caption-center">Procedure of SFT and RLHF.</div></div><p>As shown in the figure above, to apply RLHF:</p><ol><li>Prepare a set of prompts and generate several outputs for each prompt with the language model.</li><li>Ask a group of human annotators to rank/score the responses to each prompt according to our alignment criteria.</li><li>Use these ranked responses to train a reward model that predicts a human preference score from a language model’s response.</li><li>Use PPO (or other algorithms, e.g., VPG, TRPO) to finetune our language model to maximize the human preferences scores (predicted by the reward model) of its outputs.</li></ol><h3 id=52-kullbackleibler-kl-divergence>5.2. Kullback–Leibler (KL) Divergence<a hidden class=anchor aria-hidden=true href=#52-kullbackleibler-kl-divergence>#</a></h3><p>At the highest level, the Kullback-Leibler (KL) Divergence is just a method of comparing two probability distributions.</p><p>The idea of KL divergence has its roots in information theory and is highly related to the concept of entropy<span class=sidenote-number>
<small class=sidenote>According to Shannon&rsquo;s Source Coding Theorem, the optimal number of bits required to encode a message with probability $p(x)$ is given by $−\log_{2}{p(x)}$.<br>High probability event ($p(x) \approx 1$): $−log_2​(1)=0$. It takes very few bits to encode a highly probable event.<br>Low probability event ($p(x) \approx 0$): $−log_2​(p(x))$ is a large number. It takes many bits to encode a rare event.</small>
</span>:</p>$$
H=
\begin{cases}
-\mathbb{E} \left[ \log p(x) \right] &\text{(Continuous Case)} \\
-\sum_{i=1}^{N} p(x_i) \cdot \log p(x_i) &\text{(Discrete Case)}
\end{cases}
$$<p>In the equation above, we can see common formulations of entropy $H$ for a probability distribution $p$. Intuitively, the entropy value captures how much information is stored within a probability distribution &mdash;&mdash; a lower entropy means that you would need fewer bits to encode the information stored within $p$.</p><p>Instead of a single probability distribution $p$, the KL divergence considers two probability distributions: $p$ and $q$. Then, mirroring the above entropy formulation, we compute KL divergence by finding the expected difference in log probabilities between these two distributions:</p>$$
\begin{align*}
D_{\text{KL}}(p||q) &= H(p, q) - H(p) \\ &=
\begin{cases}
\mathbb{E} \left[ \log p(x) - \log q(x) \right] ~~~ \text{(Continuous Case)} \\
\sum_{i=1}^{N} p(x_i) \cdot \left( \log p(x_i) - \log q(x_i) \right) ~~~ \text{(Discrete Case)}
\end{cases}
\end{align*}
$$<p>where:</p><ul><li>$H(p,q)$: The average number of bits used with the approximate code.</li><li>$H(p)$: The minimum possible average number of bits used with the optimal code.</li><li>$D_{\text{KL}}(p||q)$: The penalty, or the expected number of extra bits &ldquo;wasted&rdquo; or &ldquo;lost&rdquo; per message due to the approximation.</li></ul><p>The KL divergence is commonly explained in the context of approximations. Namely, if we approximate $p$ with $q$, <strong>the KL divergence is the number of bits we would expect to lose by making this approximation</strong>.</p><p>KL divergence is heavily used across different domains of AI/ML research. For example, it is commonly used in loss functions for training neural networks, either as the core loss or as an added regularization term.</p><blockquote class=quote><p>The final reward function we use during optimization contains a [KL divergence] penalty term … we find this constraint is useful for training stability, and to reduce reward hacking.” &mdash;&mdash; [4]</p></blockquote><h3 id=53-trust-region-policy-optimization-trpo>5.3. Trust Region Policy Optimization (TRPO)<a hidden class=anchor aria-hidden=true href=#53-trust-region-policy-optimization-trpo>#</a></h3><p>VPG is limited by the fact that <strong>it can only perform a single policy update for each estimate of the policy gradient that is derived</strong>. Given that VPG is notoriously data inefficient, meaning that <strong>we have to sample a lot of data when deriving a policy update</strong>, performing multiple (or larger) updates may seem enticing. However, such an approach is not justified theoretically and, in practice, leads to policy updates that are too large, thus damaging performance.</p><p>Trust Region Policy Optimization (TRPO) [5] aims to solve the problem described above using an approach that is similar to VPG. At each step of the optimization process, however, we find the largest possible policy update that still improves performance. Simply put, TRPO allows us to learn faster by finding a reliable way to make larger policy updates that do not damage performance.</p><p>More specifically, we update the policy under a constraint—based on the KL divergence—that captures the distance between policies before and after the current update. Considering this constraint allows us to find a balance between update size and the amount of change to the underlying policy:</p>$$
\begin{equation*}
\begin{gathered}
\theta_{k+1} = \operatorname{argmax}_{\theta} \mathbb{E}_{(s,a) \sim (\pi_{\theta_k}, T)} \left[ \frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)} A^{\pi_{\theta_k}}(s,a) \right] \\
\text{such that } \overline{D}_{\text{KL}}(\theta||\theta_k) < \delta
\end{gathered}
\end{equation*}
$$<p>where:</p><ul><li>$\mathbb{E}_{(s,a) \sim (\pi_{\theta_k}, T)}$ is the expectation over state-action pairs sampled from the current policy $\pi_{\theta_k}$ and transition function $T$.</li><li>$\pi_{\theta}(a|s)$ is the probability of taking action $a$ in state $s$ according to the new policy $\pi_{\theta}$.</li><li>$\pi_{\theta_k}(a|s)$ is the probability of taking action $a$ in state $s$ according to the current policy $\pi_{\theta_k}$.</li><li>$A^{\pi_{\theta_k}}(s,a)$ is the advantage function for the current policy $\pi_{\theta_k}$.</li><li>$\overline{D}_{\text{KL}}(\theta||\theta_k)$ is the average KL divergence between the new policy $\pi_{\theta}$ and the current policy $\pi_{\theta_k}$.</li><li>$\delta$ is a hyperparameter that controls the maximum allowed change in the policy.</li></ul><p>Formulation of TRPO has several critical differences from VPG:</p><ul><li>The terms in the expectation are modified slightly to express the probability of a given action a as a ratio between old and updated policies.</li><li>The update has an added constraint based on the KL divergence between old and updated policies.</li><li>Instead of performing gradient ascent, we are solving a constrained maximization problem to generate each new policy</li></ul><p>The implementation of TRPO is similar to that of VPG. We allow our current policy to interact with the environment and collect data. From this observed data, we can compute the approximate update for TRPO as described above. Then, we can continue the process of collecting data and performing an update until we arrive at a policy that performs quite well.</p><p><strong>Because we are using the actual policy being trained to collect the data used to train it, TRPO is an on-policy reinforcement learning algorithm</strong>.</p><h3 id=54-trpo-vs-vpg-larger-policy-updates>5.4. TRPO vs. VPG: Larger Policy Updates<a hidden class=anchor aria-hidden=true href=#54-trpo-vs-vpg-larger-policy-updates>#</a></h3><p>As mentioned previously, the VPG algorithm is based upon gradient ascent, which &mdash;&mdash; by nature &mdash;&mdash; ensures that updates to the policy&rsquo;s parameters $\theta$ are not too large. In particular, we use a learning rate to perform updates with VPG, which can control the size of the update in the parameter space:</p>$$
\theta_{t+1} = \theta_t + \alpha \nabla_{\theta} J(\pi_{\theta})|_{\theta_t}
$$<p>Here, only the size of the update to $\theta$ is controlled, and so that the old and updated policies are close in the parameter space. However, small changes to $\theta$ may also drastically alter the policy, because ensuring that policy updates are small in the parameter space does not provide much of a guarantee on changes to the resulting policy.</p><p><strong>As a result, we are constrained to relatively small updates within the VPG algorithm &mdash;&mdash; larger or multiple updates could be harmful</strong>.</p><p>TRPO sidesteps this issue by considering the size of our policy update from an alternative viewpoint. Namely, we compare updated and old policies using the KL divergence, which measures the difference in probability distributions over the action space produced by the two policies. Such an approach compares policies based upon the actions they take rather than their underlying parameters $\theta$.</p><p>In this way, we can perform large policy updates while ensuring that the new policy does not produce actions that are significantly different from the old policy.</p><h3 id=55-proximal-policy-optimization-ppo>5.5. Proximal Policy Optimization (PPO)<a hidden class=anchor aria-hidden=true href=#55-proximal-policy-optimization-ppo>#</a></h3><blockquote class=quote><p>We introduce proximal policy optimization, a family of policy optimization methods that use multiple epochs of stochastic gradient ascent to perform each policy update. These methods have the stability and reliability of trust-region methods but are much simpler to implement &mldr; applicable in more general settings, and have better overall performance. &mdash;&mdash; [6]</p></blockquote><p>TRPO has improved data efficiency, stability, and reliability compared to the VPG algorithm, but there are still limitations that need to be addressed.</p><p>Namely, the algorithm is complicated, can only perform a single update each time new data is sampled from the environment, and is only applicable to certain problem setups.</p><p>Aiming to develop a better approach, authors in [6] propose Proximal Policy Optimization (PPO), another policy gradient algorithm that alternates between collecting data from the environment and performing several epochs of training over this sampled data. PPO shares the reliability of TRPO and is:</p><ol><li>Much simpler</li><li>More data efficient</li><li>More generally applicable</li></ol><p>Similar to TRPO, we perform policy updates in PPO according to a surrogate objective. However, this surrogate objective has a &ldquo;clipped&rdquo; probability ratio, as shown in the equation below:</p>$$
L^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min(\frac{\pi_{\theta}(o_t|q, o_{< t})}{\pi_{\theta_{old}}(o_t|q, o_{< t})}A_t, \text{CLIP}(\frac{\pi_{\theta}(o_t|q, o_{< t})}{\pi_{\theta_{old}}(o_t|q, o_{< t})}, 1-\varepsilon, 1+\varepsilon) A_t) \right]
$$<p>The surrogate objective for PPO is expressed as a minimum of two values. The first value is the same surrogate objective from TRPO, while the second value is a &ldquo;clipped&rdquo; version of this objective that lies within a certain range. In practice, this expression is formulated such that there is no reward for moving the probability ratio beyond the interval $[1 - \varepsilon, 1 + \varepsilon]$, see the figure below:</p><div class=image-container><img src=/imgs/blogs/reinforcement-learning-for-llms/prob-ratio-to-L-CLIP.png alt class=image width=100%><div class="image-caption image-caption-center">From [2].</div></div><p><strong>In other words, PPO has no incentive for excessively large policy updates. Plus, by taking the minimum of the clipped and unclipped version of the surrogate objective, we only ignore excessive changes to the probability ratio if they improve the underlying objective. In the figure above, we see a basic depiction of this trend for both positive and negative values of the advantage function.</strong></p><p>To understand PPO&rsquo;s surrogate objective more intuitively, we should look at the figure below, which plots several objective functions as we interpolate between an old and updated policy obtained via PPO. In this figure, we see the KL divergence, the TRPO surrogate objective (labeled as CPI), the clipped surrogate objective, and the full PPO surrogate objective. From these plots, we can see that the PPO surrogate objective is a pessimistic/lower bound for the TRPO surrogate objective, where a penalty is incurred for having too large of a policy update.</p><div class=image-container><img src=/imgs/blogs/reinforcement-learning-for-llms/different-objective-funcs.jpg alt class=image width=100%><div class="image-caption image-caption-center">From [2].</div></div><p>While TRPO sets a hard constraint to avoid policy updates that are too large, PPO simply formulates the surrogate objective such that a penalty is incurred if the KL divergence is too large. Such an approach is much simpler, as we no longer have to solve a difficult, constrained optimization problem. Rather, we can compute PPO’s surrogate loss with only minor tweaks to the VPG algorithm.</p><p>PPO has several benefits compared to TRPO. First, the implementation of PPO is much simpler compared to TRPO, as we can use automatic differentiation and gradient-based optimization techniques9 instead of deriving an (approximate) solution for a complex, constrained objective function. Additionally, while TRPO makes only a single policy update each time new data is collected, PPO performs multiple epochs of optimization via stochastic gradient ascent over the surrogate objective, which improves data efficiency.</p><p>Finally, computing estimates of the advantage function (e.g., via Generalized Advantage Estimation (GAE)) typically requires that we learn a corresponding value function. In TRPO, we must learn this state-value function with a separate neural network. However, PPO—due to its compatibility with a wider scope of architectures (including those with parameter sharing) &mdash;&mdash; can train a joint network for policy and value functions by just adding an extra term to the loss function that computes the mean-squared error (MSE) between estimated and actual value function values.</p>$$
L^{\text{CLIP+VF}}(\theta) = \mathbb{E}_t \left[ L^{\text{CLIP}}(\theta) - c_1 L^{\text{VF}}(\theta) \right]
$$<p>where:</p><ul><li>$L^{\text{CLIP}}(\theta)$ is the PPO surrogate objective.</li><li>$L^{\text{VF}}(\theta)$ is the MSE loss for the value function.</li><li>$c_1$ is a hyperparameter that controls the weight of the value function loss in the overall loss function.</li></ul><h2 id=6-group-relative-policy-optimization-grpo>6. Group Relative Policy Optimization (GRPO)<a hidden class=anchor aria-hidden=true href=#6-group-relative-policy-optimization-grpo>#</a></h2><div class=image-container><img src=/imgs/blogs/reinforcement-learning-for-llms/ppo-vs-grpo.png alt class=image width=100%><div class="image-caption image-caption-center">From [7].</div></div><p>Different from PPO, GRPO:</p><ol><li>Removes the value function model.</li><li>The policy model generates multiple outputs for each input, and the reward model calculates the reward for each output, and calculates the advantage scores after group computation.</li><li>Removes the GAE, and changes the method to calculate KL.</li></ol><p>In PPO, we optimizes LLMs by maximizing the following objective function:</p>$$
\begin{align*}
\mathcal{J}_{\text{PPO}}(\theta) = \mathbb{E}&_{q \sim P(Q), o \sim \pi_{\theta_{\text{old}}}(O|q)} \left[ \frac{1}{|o|} \sum_{t=1}^{|o|} \min \left( \frac{\pi_{\theta}(o_t|q, o_{< t})}{\pi_{\theta_{\text{old}}}(o_t|q, o_{< t})} A_t, \right. \right. \\
& \left. \left. \text{clip}\left(\frac{\pi_{\theta}(o_t|q, o_{< t})}{\pi_{\theta_{\text{old}}}(o_t|q, o_{< t})}, 1-\varepsilon, 1+\varepsilon\right) A_t \right) \right]
\end{align*}
$$<p>where:</p><ul><li>$\pi_{\theta}$ and $\pi{\theta_{\text{old}}}$ are the current and old policy models;</li><li>$q$, $o$ are questions and outputs sampled from the question dataset and the old policy $\pi{\theta_{\text{old}}}$;</li><li>$\varepsilon$ is a clipping-related hyper-parameter introduced in PPO for stabilizing training;</li><li>$A_t$ is the advantage, which is computed by applying Generalized Advantage Estimation (GAE), based on the rewards ${r_{\geq t}}$ and a learned value function $V_{\psi}$.</li></ul><p>Thus, in PPO, a value function needs to be trained alongside the policy model and to mitigate over-optimization of the reward model, the standard approach is to add a per-token KL penalty from a reference model in the reward at each token, i.e.:</p>$$
r_t = r_{\varphi}(q, o_{\leq t}) - \beta \log \frac{\pi_{\theta}(o_t|q, o_{< t})}{\pi_{ref}(o_t|q, o_{< t})}
$$<p>There are several issues with PPO:</p><ol><li>As the value function employed in PPO is typically another model of comparable size as the policy model, it brings a substantial memory and computational burden.</li><li>During RL training, the value function is treated as a baseline in the calculation of the advantage for variance reduction. While in the LLM context, usually only the last token is assigned a reward score by the reward model, which may complicate the training of a value function that is accurate at each token.</li></ol><p>To address these issues, for each question $q$, GRPO samples a group of outputs $\{o_1, o_2, \dots , o_G\}$ from the old policy $\pi_{\theta_{\text{old}}}$ and then optimizes the policy model by maximizing the following objective:</p>$$
\begin{align*}
\mathcal{J}_{\text{GRPO}}(\theta) = \mathbb{E}&[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O|q)] \\
&\frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left\{ \min\left(\frac{\pi_{\theta}(o_{i,t}|q, o_{i,< t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q, o_{i,< t})} \hat{A}_{i,t}, \right.\right. \\
&\left.\left. \text{clip}\left(\frac{\pi_{\theta}(o_{i,t}|q, o_{i,< t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q, o_{i,< t})}, 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i,t}\right) - \beta D_{KL}[\pi_{\theta}||\pi_{ref}] \right\}
\end{align*}
$$<p>where:</p><ul><li>$\varepsilon$ and $\beta$ are hyper-parameters;</li><li>$\hat{A}_{i,t}$ is the advantage calculated based on relative rewards of the outputs inside each group only. See <a href=https://github.com/jamesnulliu/verl/blob/archive-d9a6a31/verl/trainer/ppo/core_algos.py#L244-L308 target=_blank rel="noopener noreferrer">here</a> for verl&rsquo;s implementation. For simplicity, given input <code>scores</code> (a tensor of padded rewards with shape <code>(batch_size, response_len)</code>), the algorithm first sums the rewards for each response, then normalizes the rewards in each group (a batch may contain multiple groups), and finally broadcasts and multiplies the rewards (of shape <code>(batch_size, 1)</code>) with reponse mask (of shape <code>(batch_size, response_len)</code>) to get the advantage<span class=sidenote-number>
<small class=sidenote>That is to say, for each response of shape <code>(1, response_len)</code>, the advantages are the same, except for the padded positions, which are 0.</small>
</span>.</li></ul><p>Note that:</p><ol><li>Instead of adding KL penalty in the reward, GRPO regularizes by directly adding the KL divergence between the trained policy and the reference policy to the loss, avoiding complicating the calculation of $\hat{A}_{i,t}$.</li><li>Different from the KL penalty term used in [6], GRPO estimate the KL divergence with the following unbiased estimator:
$$
\mathbb{D}_{\text{KL}}\left[ \pi_{\theta}||\pi_{ref}\right] = \frac{\pi_{ref}(o_{t,t}|q, o_{t,< t})}{\pi_{\theta}(o_{t,t}|q, o_{t,< t})} - \log \frac{\pi_{ref}(o_{t,t}|q, o_{t,< t})}{\pi_{\theta}(o_{t,t}|q, o_{t,< t})} - 1
$$
which is guaranteed to be positive.</li></ol><p>The code of the objective function, i.e., <code>compute_policy_loss</code>, is show <a href=https://github.com/jamesnulliu/verl/blob/archive-d9a6a31/verl/trainer/ppo/core_algos.py#L722-L794 target=_blank rel="noopener noreferrer">here</a> in verl. It can be seen that the implementation is a bit different from the equation above:</p><ul><li>The $\frac {\pi_{\theta}(o_{i,t}|q, o_{i,< t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q, o_{i,< t})}$ term is replaced with $e^{\log \pi_{\theta}(o_{i,t}|q, o_{i,< t}) - \log \pi_{\theta_{\text{old}}}(o_{i,t}|q, o_{i,< t})}$, which is more numerically stable, as shown in <a href=https://github.com/jamesnulliu/verl/blob/archive-d9a6a31/verl/trainer/ppo/core_algos.py#L766-L769 target=_blank rel="noopener noreferrer">these lines</a> in function <code>compute_policy_loss</code>.</li><li>The kl penalty term is removed, and calculated separately in <code>DataParallelPPOActor.update_policy</code>, according to option <code>use_kl_loss</code> (which is default false but shoule be set to true for GRPO), as shown <a href=https://github.com/jamesnulliu/verl/blob/archive-d9a6a31/verl/workers/actor/dp_actor.py#L529-L539 target=_blank rel="noopener noreferrer">here</a>.</li><li>Both the policy loss and the KL loss would be passed to function <code>agg_loss</code>, which aggregates them to scalar values. The default method is <code>token-mean</code>, which is different from &ldquo;<em>the original GRPO paper takes the sample-level loss (<code>seq-mean-token-mean</code>), which may be unstable in long-CoT scenarios</em>&rdquo;.<blockquote class=quote><p>All GRPO example scripts provided in verl uses the default configuration &ldquo;token-mean&rdquo; for loss aggregation instead.</p></blockquote>Btw, <code>token-mean</code> will means the (policy gradient) loss across all the tokens in all the sequences in a mini-batch; <em>An idea of DAPO?</em></li></ul><h2 id=7-dapo-an-open-source-llm-reinforcement-learning-system-at-scale-8>7. DAPO: An Open-Source LLM Reinforcement Learning System at Scale [8]<a hidden class=anchor aria-hidden=true href=#7-dapo-an-open-source-llm-reinforcement-learning-system-at-scale-8>#</a></h2><ol><li>Clip-Higher, which promotes the diversity of the system and avoids entropy collapse; Configured <a href=https://github.com/jamesnulliu/verl/blob/archive-d9a6a31/recipe/dapo/run_dapo_qwen2.5_32b.sh#L76-L77 target=_blank rel="noopener noreferrer">here</a> and used <a href=https://github.com/jamesnulliu/verl/blob/archive-d9a6a31/verl/trainer/ppo/core_algos.py#L773-L779 target=_blank rel="noopener noreferrer">here</a> in <code>compute_policy_loss</code>.</li><li>Dynamic Sampling, which improves training efficiency and stability; Shown <a href=https://github.com/jamesnulliu/verl/blob/archive-d9a6a31/recipe/dapo/dapo_ray_trainer.py#L199-L257 target=_blank rel="noopener noreferrer">here</a> in <code>RayDPAOTrainer</code>.</li><li>Token-Level Policy Gradient Loss, which is critical in long-CoT RL scenarios; Configured <a href=https://github.com/jamesnulliu/verl/blob/archive-d9a6a31/recipe/dapo/run_dapo_qwen2.5_32b.sh#L99 target=_blank rel="noopener noreferrer">here</a>, which has been introduced in the above GRPO section.</li><li>Overlong Reward Shaping, which reduces reward noise and stabilizes training; Configured <a href=https://github.com/jamesnulliu/verl/blob/archive-d9a6a31/recipe/dapo/run_dapo_qwen2.5_32b.sh#L117-L119 target=_blank rel="noopener noreferrer">here</a> and used <a href=https://github.com/volcengine/verl/blob/4f80e465c2ec79ab9c3c30ec74b9745de61d0490/verl/workers/reward_manager/dapo.py#L100-L113 target=_blank rel="noopener noreferrer">here</a> in <code>DapoRewardManager</code>.</li></ol><p>Before defining the object function, it is worth noting that the KL term is excluded from the proposed algorithm.</p><blockquote class=quote><p>The KL penalty term is used to regulate the divergence between the online policy and the frozen reference policy. In the RLHF scenario, the goal of RL is to align the model behavior without diverging too far from the initial model. However, during training the long-CoT reasoning model, the model distribution can diverge significantly from the initial model, thus this restriction is not necessary.</p></blockquote><p>The objective function of DAPO is shown below:</p>$$
\begin{align*}
\mathcal{J}_{\text{DAPO}}(\theta) =& ~ \mathbb{E}_{(q,a) \sim \mathcal{D}, \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(\cdot|q)} \\
&\left[ \frac{1}{\sum_{i=1}^G |o_i|} \sum_{i=1}^G \sum_{t=1}^{|o_i|} \min \left( r_{i,t}(\theta)\hat{A}_{i,t}, \text{clip}(r_{i,t}(\theta), 1 - \epsilon_{\text{low}}, 1 + \epsilon_{\text{high}})\hat{A}_{i,t} \right) \right]
\\
\text{s.t. } &0 < \left | \{o_i \mid \text{is_equivalent}(a, o_i)\} \right | < G,
\end{align*}
$$<p>where:</p>$$
r_{i,t}(\theta) = \frac{\pi_\theta(o_{i,t} \mid q, o_{i, < t})}{\pi_{\theta_{\text{old}}}(o_{i,t} \mid q, o_{i, < t})}, \quad \hat{A}_{i,t} = \frac{R_i - \text{mean}(\{R_i\}_{i=1}^G)}{\text{std}(\{R_i\}_{i=1}^G)}
$$<h2 id=8-beyond-the-8020-rule-high-entropy-minority-tokens-drive-effective-reinforcement-learning-for-llm-reasoning-9>8. Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning [9]<a hidden class=anchor aria-hidden=true href=#8-beyond-the-8020-rule-high-entropy-minority-tokens-drive-effective-reinforcement-learning-for-llm-reasoning-9>#</a></h2><p>Token entropy calculation:</p>$$
\begin{gather*}
H_t := - \sum_{j=1}^{V} p_{t,j} \log p_{t,j} \\
\text{ where } (p_{t,1}, \cdots, p_{t,V}) = \boldsymbol{p}_t = \pi_{\theta}(\cdot \mid q, o_{ < t}) = \text{Softmax}\left(\frac{\mathbf{z}_t}{T}\right)
\end{gather*}
$$<p>Here,</p><ul><li>$\pi_{\theta}$ denotes an LLM parameterized by $\theta$</li><li>$q$ is the input query, and $o_{< t} = (o_1, o_2, \ldots, o_{t-1})$ represents the previously generated tokens</li><li>$V$ is the vocabulary size</li><li>$\mathbf{z}_t \in \mathbb{R}^V$ denotes the pre-softmax logits at time step $t$</li><li>$\boldsymbol{p}_t \in \mathbb{R}^V$ is the corresponding probability distribution over the vocabulary</li><li>$T \in \mathbb{R}$ is the decoding temperature</li></ul><p>In off-policy settings, sequences are generated by a rollout policy $\pi_{\phi}$ while the training policy is $\pi_{\theta}$, with $\phi \neq \theta$. The entropy is still calculated using $\pi_{\theta}$, as defined in Equation (1), to measure the uncertainty of the training policy in the given sequence.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li>Cameron R. Wolfe. <a href=https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning target=_blank rel="noopener noreferrer">Basics of Reinforcement Learning for LLMs</a>.</li><li>Cameron R. Wolfe. <a href=https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo target=_blank rel="noopener noreferrer">Proximal Policy Optimization (PPO): The Key to LLM Alignment</a>.</li><li>Achiam, Josh. <a href=https://spinningup.openai.com/en/latest/index.html target=_blank rel="noopener noreferrer">Spinning Up in Deep RL</a>. OpenAI, 2018.</li><li>Touvron, Hugo, et al. &ldquo;Llama 2: Open foundation and fine-tuned chat models.&rdquo; arXiv preprint arXiv:2307.09288 (2023).</li><li>Schulman, John, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. &ldquo;Trust region policy optimization.&rdquo; In International conference on machine learning, pp. 1889-1897. PMLR, 2015.</li><li>Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. &ldquo;Proximal policy optimization algorithms.&rdquo; arXiv preprint arXiv:1707.06347 (2017).</li><li>Shao, Zhihong, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang et al. &ldquo;Deepseekmath: Pushing the limits of mathematical reasoning in open language models.&rdquo; arXiv preprint arXiv:2402.03300 (2024).</li><li>Yu, Qiying, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai et al. &ldquo;Dapo: An open-source llm reinforcement learning system at scale.&rdquo; arXiv preprint arXiv:2503.14476 (2025).</li><li>Wang, Shenzhi, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang et al. &ldquo;Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning.&rdquo; arXiv preprint arXiv:2506.01939 (2025).</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://jamesnulliu.github.io/tags/rl4llm/>Rl4llm</a></li></ul><nav class=paginav><a class=next href=https://jamesnulliu.github.io/blogs/cuda-programming-notes-01-memory-coalescing/><span class=title>Next »</span><br><span>CUDA Programming Notes | 01: Memory Coalescing</span></a></nav></footer><script src=https://giscus.app/client.js data-repo=jamesnulliu/jamesnulliu.github.io data-repo-id=R_kgDOMPCQIw data-category=Announcements data-category-id=DIC_kwDOMPCQI84Cgb2t data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async></script></article></main><footer class=footer><span>© 2024-2025 JamesNULLiu</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>