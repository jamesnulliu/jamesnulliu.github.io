<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Deeplearning on 秋水·JamesNULLiu</title><link>https://jamesnulliu.github.io/categories/deeplearning/</link><description>Recent content in Deeplearning on 秋水·JamesNULLiu</description><generator>Hugo -- 0.148.2</generator><language>en</language><copyright>2024-2025 JamesNULLiu</copyright><lastBuildDate>Fri, 12 Sep 2025 22:39:56 +0000</lastBuildDate><atom:link href="https://jamesnulliu.github.io/categories/deeplearning/index.xml" rel="self" type="application/rss+xml"/><item><title>Reinforcement Learning for LLMs</title><link>https://jamesnulliu.github.io/blogs/reinforcement-learning-for-llms/</link><pubDate>Mon, 02 Jun 2025 08:25:00 +0800</pubDate><guid>https://jamesnulliu.github.io/blogs/reinforcement-learning-for-llms/</guid><description>&lt;h2 id="1-basics">1. Basics&lt;/h2>
&lt;ul>
&lt;li>RLHF: Reinforcement Learning from Human Feedback&lt;/li>
&lt;li>SFT: Supervised Fine-Tuning&lt;/li>
&lt;/ul>
&lt;p>RL trains neural networks through &lt;strong>trial&lt;/strong> and &lt;strong>error&lt;/strong>. When finetuning a language model with RLHF, the model produces some text then receives a score/reward from a human annotator that captures the quality of that text. Then, we use RL to finetune the language model &lt;strong>to generate outputs with high scores&lt;/strong>.&lt;/p>
&lt;p>In this case, we cannot apply a loss function that trains the language model to maximize human preferences with supervised learning. This is because there’s no easy way to explain the score human give or connect it mathematically to the output of the neural network. In other words, we cannot backpropagate a loss applied to this score through the rest of the neural network. This would require that we are able to &lt;strong>differentiate (i.e., compute the gradient of) the system that generates the score&lt;/strong>, which is a human that subjectively evaluates the generated text.&lt;/p></description><content:encoded><![CDATA[<h2 id="1-basics">1. Basics</h2>
<ul>
<li>RLHF: Reinforcement Learning from Human Feedback</li>
<li>SFT: Supervised Fine-Tuning</li>
</ul>
<p>RL trains neural networks through <strong>trial</strong> and <strong>error</strong>. When finetuning a language model with RLHF, the model produces some text then receives a score/reward from a human annotator that captures the quality of that text. Then, we use RL to finetune the language model <strong>to generate outputs with high scores</strong>.</p>
<p>In this case, we cannot apply a loss function that trains the language model to maximize human preferences with supervised learning. This is because there’s no easy way to explain the score human give or connect it mathematically to the output of the neural network. In other words, we cannot backpropagate a loss applied to this score through the rest of the neural network. This would require that we are able to <strong>differentiate (i.e., compute the gradient of) the system that generates the score</strong>, which is a human that subjectively evaluates the generated text.</p>
<div class="image-container">
    <img src="/imgs/blogs/reinforcement-learning-for-llms/rl-structure.webp" 
        alt="" 
        class="image" 
        width="85%"/>
    <div class="image-caption">
        The agent acts and receives rewards (and new states) from the environment.
    </div>
</div>
<p>Problems that are solved via RL tend to be structured in a similar format. Namely, we have an <strong>agent</strong> that is interacting with an <strong>environment</strong>; see the figure above. The agent has a state in the environment and produces actions, which can modify the current state, as output. As the agent interacts with the environment, it can receive both positive and negative rewards for its actions. <strong>The agent’s goal is to maximize the rewards that it receives, but there is not a reward associated with every action taken by the agent</strong>. Rather, rewards may have a long horizon, meaning that it takes several correct, consecutive actions to generate any positive reward.</p>
<h2 id="2-markov-decision-process-mdp">2. Markov Decision Process (MDP)</h2>
<h3 id="21-concepts-and-definitions">2.1. Concepts and Definitions</h3>
<p>Markov Decision Process (MDP) is a way to formulate the system described above more formally and mathematically. Within an MDP, we have <strong>states</strong>, <strong>actions</strong>, <strong>rewards</strong>, <strong>transitions</strong>, and a <strong>policy</strong>, as shown in the equation below:</p>
$$
\begin{cases}
s \in S & \text{State} \\
a \in A & \text{Action} \\
r_s \in \mathbb{R} & \text{Reward} \\
\pi(a|s) & \text{Policy} \\
T(s_{t+1}|s_{t},a_{t}) & \text{Transition function} \\
\end{cases}
$$<p>States and actions have discrete values, while rewards are real numbers.</p>
<p>In an MDP, we define two types of functions: <strong>transition and policy functions</strong>. The policy takes a state as input, then outputs a probability distribution over possible actions.</p>
<blockquote>
<p>Notably, the action that is chosen only depends on the current state and not any state history that precedes it. This is a key property of an MDP, which make the assumption that the next action only depends upon the current state.</p></blockquote>
<p>Given this output, we can make a decision for the action to be taken from a current state, and the transition is then a function that outputs the next state based upon the prior state and chosen action. Using these components, the agent can interact with the environment in an iterative fashion, as the figure shown below.</p>
<div class="image-container">
    <img src="/imgs/blogs/reinforcement-learning-for-llms/mdp-structure.webp" 
        alt="" 
        class="image" 
        width="80%"/>
    <div class="image-caption">
        Structure of an MDP.
    </div>
</div>
<ul>
<li>The policy describes how the agent chooses its next action given the current state.</li>
<li>The agent follows this strategy as it interacts with the environment.</li>
<li>The goal is to learn a policy that maximizes the reward that the agent receives from the environment.</li>
</ul>
<p>As the agent interacts with the environment, we form a <strong>trajectory</strong> ($\tau$) of <strong>states</strong> ($s$) and <strong>actions</strong> ($a$) that are chosen throughout this process. Then, given the <strong>reward</strong> ($r_s$) associated with each of these states, we get a total return ($R(\tau)$) given by the equation below, where $\gamma$ is the discount factor:</p>
$$
\begin{cases}
\tau &= \{s_0, a_0, s_1, a_1, \dots, s_t, a_t\} \quad &\text{(Trajectory)} \\ 
R(\tau) &= \sum_t \gamma^t r_{s_t} \quad &\text{(Return)}
\end{cases}
$$<p>$R(\tau)$ is the summed reward across the agent&rsquo;s full trajectory, but <strong>rewards achieved at later time steps are exponentially discounted by the factor $\gamma$</strong>
<span class="sidenote-number">
    <small class="sidenote">TL;DR. The fact that the discount rate is bounded to be smaller than 1 is a mathematical trick to make an infinite sum finite. This helps proving the convergence of certain algorithms. See <a href="https://stats.stackexchange.com/questions/221402/understanding-the-role-of-the-discount-factor-in-reinforcement-learning">Understanding the role of the discount factor in reinforcement learning</a>.</small>
</span>
&mdash; This means that current rewards are more valuable than later rewards, due to both uncertainty and the simple fact that waiting to receive a reward is less desirable.</p>
<p><strong>The goal of RL is to train an agent that maximizes this return</strong>. As shown by the equation below, we can characterize this as finding a policy that maximizes the return over trajectories that are sampled from the final policy:</p>
<blockquote>
<p>Note that the policy is a probability distribution over actions at each time step given the current state, so the exact trajectory produced is not deterministic. Many different trajectories can be obtained depending upon how we sample actions from the policy.</p></blockquote>
$$
\max_{\pi} ~ \mathbb{E}_{\tau \sim P_{\pi, T}} ~ R(\tau)
$$<p>where:</p>
<ul>
<li>$\max_{\pi}$ means to find the policy that yields the maximum return.</li>
<li>$\mathbb{E}_{\tau \sim P_{\pi, T}}$ means to take the expectation or average over trajectories randomly sampled from a certain policy $\pi$ and transition function $T$.</li>
<li>$R(\tau)$ is the return of the trajectory $\tau$.</li>
</ul>
<h3 id="22-a-classical-example">2.2. A Classical Example</h3>
<p>A classical example of an MDP is a maze, where the agent is trying to find the optimal path to the goal.</p>
<div class="image-container">
    <img src="/imgs/blogs/reinforcement-learning-for-llms/agent-in-maze.webp" 
        alt="" 
        class="image" 
        width="70%"/>
    <div class="image-caption">
        A simple traditional RL environment. The target is to train the agent to find the optimal (largest return) solution path.
    </div>
</div>
<ul>
<li><strong>States</strong>: The positions in the $2 \times 3$ maze. The positions can be represented as a one-hot vector.</li>
<li><strong>Actions</strong>: The possible moves (up, down, left, right) &mdash; This is the agent&rsquo;s (i.e., the model&rsquo;s) each-step output.</li>
<li><strong>Rewards</strong>: The agent receives a reward of $+10$ for reaching the goal and $-10$ for reaching the trap. The agent receives a reward of $0$ for all other states.</li>
<li><strong>Transition function</strong>: The agent can move to an adjacent state based on the chosen action, but it cannot move through walls. The transition function defines how the agent moves from one state to another based on the action taken.</li>
<li><strong>Policy</strong>: The agent&rsquo;s policy is a probability distribution over the possible actions given the current state. For example, if the agent is in the state (0, 0), it might have a policy that gives a high probability to moving right and a low probability to moving down.</li>
<li><strong>Trajectory</strong>: The sequence of states and actions taken by the agent as it navigates the maze.</li>
</ul>
<p>Like many problems that are solved with RL, this setup has an environment that is not differentiable (i.e., we can’t compute a gradient and train the model in a supervised fashion) and contains long-term dependencies, meaning that we might have to learn how to perform several sequential actions to get any reward.</p>
<h3 id="23-taxonomy-of-modern-rl-algorithms">2.3. Taxonomy of modern RL algorithms</h3>
<div class="image-container">
    <img src="/imgs/blogs/reinforcement-learning-for-llms/taxonomy-of-modern-rl-algorithms.webp" 
        alt="" 
        class="image" 
        width="100%"/>
    <div class="image-caption">
        Taxonomy of modern RL algorithms.
    </div>
</div>
<h2 id="3-deep-q-leanring">3. Deep Q-Leanring</h2>
<h3 id="31-q-learning">3.1. Q-Learning</h3>
<p>Q-Learning is a model-free RL algorithm, meaning that we don’t have to learn a model for the environment with which the agent interacts. The goal of Q-Learning is to <strong>learn the value of any action at a particular state</strong>.</p>
<p>There are three key concepts to mention here:</p>
<ol>
<li>Value $Q(s, a)$ corresponds to choosing action $a$ at current state $s$. The value not only contains the determined reward by taking action $a$ at state $s$, but also contains a <strong>discounted</strong> and <strong>recursive</strong> future Q-value of the next state $s'$ after taking action $max_a$ (i.e., the best action at state $s'$).</li>
<li>The higher a value $Q(s, a)$ is, the more valuable it is to take action $a$ at state $s$.</li>
<li>A look-up table (Q-table) must be maintained to store the Q values for each state-action pair.</li>
</ol>
<p>The algorithm first initialize all Q values as zero and pick an initial state with which to start the learning process. Then, iterate over the following steps:</p>
<ul>
<li>Pick an action to execute from the current state (using an $\varepsilon$-Greedy Policy).</li>
<li>Get a reward and next state from the (model-free) environment.</li>
<li>Update the Q value in the Q-table based on the Bellman equation.</li>
</ul>
<p>Here we show a simplified update method which derives from the Bellman Optimality Equation and defines $Q(s_t, a_t)$ recursively:</p>
$$
Q(s_t, a_t) = r_t + \gamma \max_{a} Q(s_{t+1}, a)
$$<p>where:</p>
<ul>
<li>$Q(s_t, a_t)$ is the Q value of the current state $s_t$ and action $a_t$.</li>
<li>$r_t$ is the reward received after taking action $a_t$ at state $s_t$.</li>
<li>$\gamma$ is the discount factor.</li>
<li>$\max_{a} Q(s_{t+1}, a)$ is the maximum Q value of the next state $s_{t+1}$ over all possible actions $a$.</li>
</ul>
<h3 id="32-deep-q-learning-dql">3.2. Deep Q-Learning (DQL)</h3>
<p>In DQL, Q-table is replaced by a neural network.</p>
<div class="image-container">
    <img src="/imgs/blogs/reinforcement-learning-for-llms/dql-structure.webp" 
        alt="" 
        class="image" 
        width="90%"/>
    <div class="image-caption">
        Structure of an DQL.
    </div>
</div>
<p>In DQL, we have two neural networks: the Q network and the target network. These networks are identical, but the exact architecture they use depend upon the problem being solved7. To train these networks, we first gather data by interacting with the environment. This data is gathered using the current Q network with an ε-greedy policy. This process of gathering interaction data for training the Q network is referred to as experience replay; see above.</p>
<p>From here, we use data that has been collected to train the Q network. During each training iteration, we sample a batch of data and pass it through both the Q network and the target network. The Q network takes the current state as input and predicts the Q value of the action that is taken (i.e., predicted Q value), while the target network takes the next state as input and predicts the Q value of the best action that can be taken from that state8 (i.e., target Q value).</p>
<p>From here, we use the predicted Q value, the target Q value, and the observed reward to train the Q network with an MSE loss; see above. The target network is held fixed. Every several iterations, the weights of the Q network are copied to the target network, allowing this model to be updated as well. Then, we just repeat this process until the Q network converges. Notably, the dataset we obtain from experience replay is cumulative, meaning that we maintain all of the data we have observed from the environment throughout all iterations.</p>
<p>Why do we need the target network? The vanilla Q-learning framework leverages two Q values in its update rule: a (predicted) Q value for the current state-action pair and the (target) Q value of the best state-action pair for the next state. In DQL, we similarly have to generate both of these Q values. In theory, we could do this with a single neural network by making multiple passes through the Q network—one for the predicted Q value and one for the target Q value. However, the Q network’s weights are being updated at every training iteration, which would cause the target Q value to constantly fluctuate as the model is updated. To avoid this issue, we keep the target network separate and fixed, only updating its weights every several iterations to avoid creating a “moving target”.</p>
<p>This idea of using a separate network to produce a training target for another network—referred to as knowledge distillation [6]—is heavily utilized within deep learning. Furthermore, the idea of avoiding too much fluctuation in the weights of the teacher/target model has been addressed in this domain. For example, the mean teacher approach [7] updates the weights of the teacher model as an exponential moving average of the student network’s weights; see above. In this way, we can ensure a stable target is provided by the teacher during training.</p>
<h2 id="4-policy-gradients">4. Policy Gradients</h2>
<p>In Policy Gradients, we will assume that our policy is a machine learning model (e.g., a deep neural network) with parameters $\theta$. This policy takes a state as input and predicts some distribution over the action space. We use this output to decide what action should be taken next within the MDP:</p>
$$
\begin{cases}
s \in S & \text{State} \\
a \in A & \text{Action} \\
r_s \in \mathbb{R} & \text{Reward} \\
\pi_{\theta}(a|s) & \text{Policy} \\
T(s_{t+1}|s_{t},a_{t}) & \text{Transition function} \\
\end{cases}
$$<p>As our agent traverses the environment, it receives positive or negative reward signals for the actions it chooses and the states that it visits. Our goal is to learn a policy from these reward signals that maximizes total reward across an entire trajectory sampled from the policy. This idea is captured by the return, which sums the total rewards over an agent’s trajectory:</p>
$$
\begin{cases}
\tau &= \{s_0, a_0, s_1, a_1, \dots, s_t, a_t\} \quad &\text{(Trajectory)} \\ 
R(\tau) &= \sum_t \gamma^t r_{s_t} \quad &\text{(Return)}
\end{cases}
$$<p>If $\gamma < 1$, then the return is <strong>Infinite-Horizon Discounted Return</strong>. If $\gamma = 1$, then the return is <strong>Finite-Horizon Return</strong>.</p>
<h3 id="41-value-functions-and-advantage-functions">4.1. Value Functions and Advantage Functions</h3>
<p>One final concept that will be especially relevant is that <strong>value functions</strong>. In RL, there are four basic value functions, all of which assume the infinite-horizon discounted return:</p>
$$
\begin{align*}
V^{\pi}(s) &= \mathbb{E}_{\tau \sim (\pi,T)} [R(\tau)|s_0 = s] && \text{(On-Policy Value Function)} \\
Q^{\pi}(s, a) &= \mathbb{E}_{\tau \sim (\pi,T)} [R(\tau)|s_0 = s, a_0 = a] && \text{(On-Policy Action-Value Function)} \\
V^{*}(s) &= \max_{\pi} \mathbb{E}_{\tau \sim (\pi,T)} [R(\tau)|s_0 = s] && \text{(Optimal Value Function)} \\
Q^{*}(s, a) &= \max_{\pi} \mathbb{E}_{\tau \sim (\pi,T)} [R(\tau)|s_0 = s, a_0 = a] && \text{(Optimal Action-Value Function)}
\end{align*}
$$<ul>
<li><strong>On-Policy Value Functio</strong>: expected return if starting in state $s$ and act according to policy $\pi$ afterwards.</li>
<li><strong>On-Policy Action-Value Function</strong>: expected return if you start in state $s$, take some action $a$ (may not come from the current policy), and act according to policy $\pi$ afterwards.</li>
<li><strong>Optimal Value Function</strong>: expected return if you start in state $s$ and always act according to the optimal policy afterwards.</li>
<li><strong>Optimal Action-Value Function</strong>: expected return if you start in state $s$, take some action a (may not come from the current policy), and act according to the optimal policy afterwards.</li>
</ul>
<p>There is an important connection between the optimal policy in an environment and the optimal action-value function. Namely, the optimal policy selects the action in state $s$ that maximizes the value of the optimal action-value function.</p>
<p>Using the value functions described above, we can define a special type of function called an advantage function, which is heavily used in RL algorithms based on policy gradients:</p>
$$
A^{\pi}(s, a) = Q^{\pi}(s, a) - V^{\pi}(s) \quad \text{(Advantage Function)}
$$<ul>
<li>$Q^{\pi}(s, a)$ on-policy action-value function.</li>
<li>$V^{\pi}(s)$ on-policy value function.</li>
</ul>
<p>Simply put, the advantage function characterizes <strong>how much better it is to take a certain action a relative to a randomly-selected action in state $s$ given a policy $\pi$</strong>. Here, we should notice that the advantage function can be derived using the on-policy value and action-value functions defined before, as these functions assume that the agent acts according to a randomly-selected action from the policy $\pi$.</p>
<blockquote class="quote"><p>The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next. &mdash;&mdash; [3]</p></blockquote>
<h3 id="42-policy-optimization">4.2. Policy Optimization</h3>
<p>During the learning process, we aim to find parameters $\theta$ for our policy that maximize the objective function below:</p>
$$
\mathcal{J}(\pi_{\theta}) = \mathbb{E}_{\tau \sim (\pi_{\theta},T)} [R(\tau)]
$$<p>where:</p>
<ul>
<li>$\pi_{\theta}$ is the policy with network parameters $\theta$.</li>
<li>$\tau$ is the trajectory sampled from the policy $\pi_{\theta}$ and transition function $T$.</li>
<li>$T$ is the transition function that defines how the agent moves from one state to another based on the action taken.</li>
<li>$R(\tau)$ is the return of the trajectory $\tau$.</li>
</ul>
<p>In words, this objective function measures the expected return of trajectories sampled from our policy within the specified environment.</p>
<p>If we want to find parameters $\theta$ that maximize this objective function, one of the most fundamental techniques that we can use is gradient ascent, which iterates over parameters $\theta$ using the update rule shown below:</p>
$$
\theta_{t+1} = \theta_t + \alpha \nabla_{\theta} \mathcal{J}(\pi_{\theta})|_{\theta_t}
$$<p>Do a lot of math to compute $\nabla_{\theta} \mathcal{J}(\pi_{\theta})$, and the final result of the basic policy gradient is:</p>
$$
\nabla_{\theta} \mathcal{J}(\pi_{\theta}) = \mathbb{E}_{\tau \sim (\pi_{\theta}, T)} \left[ \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) R(\tau) \right]
$$<p>Now, we have an actual expression for the gradient of our objective function that we can use in gradient ascent! Plus, this expression only depends on the return of a trajectory and the gradient of the log probability of an action given our current state. As long as we instantiate our policy such that the gradient of action probabilities is computable (e.g., this is pretty easy to do if our policy is implemented as a neural network), we can easily derive both of these quantities.</p>
<h3 id="43-computing-the-policy-gradient-in-practice">4.3. Computing the Policy Gradient in Practice</h3>
<p>In practice, we can estimate the value of this expectation by sampling a fixed number of trajectories, by:</p>
<ul>
<li>Sample several trajectories by letting the agent interact with the environment according to the current policy.</li>
<li>Estimate the policy gradient using an average of relevant quantities over the fixed number of sample trajectories.</li>
</ul>
<p>Then given a set of sampled trajectories $\mathcal{D} = \{\tau_0, \tau_1, \dots\}$, we can estimate the policy gradient $\overline{\nabla_{\theta} \mathcal{J}(\pi_{\theta})}$ as follows:</p>
$$
\overline{\nabla_{\theta} \mathcal{J}(\pi_{\theta})} = \frac{1}{|\mathcal{D}|} \sum_{\tau \in \mathcal{D}} \sum_{t=0}^{T} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) R(\tau) \right]
$$<h3 id="44-vallina-poclicy-gradient-vpg-and-other-policy-gradients">4.4. Vallina Poclicy Gradient (VPG) and Other Policy Gradients</h3>
<p>Given:</p>
$$
\nabla_{\theta} \mathcal{J}(\pi_{\theta}) = \mathbb{E} \left[ \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) \Psi_t \right]
$$<p>We have:</p>
<ol>
<li><strong>Basic Policy Gradient</strong>:
$$
   \Psi_t = R(\tau)
   $$
Sum of all (potentially discounted) rewards obtained along the entire trajectory.</li>
<li><strong>Reward-to-Go</strong>:
$$
   \Psi_t = \sum_{i=t}^{T} r_{s_i, a_i}
   $$
Rewards after the current action.</li>
<li><strong>Reward-to-Go with Baseline</strong>:
$$
   \Psi_t = \sum_{i=t}^{T} r_{s_i, a_i} - b(s_i)
   $$
A baseline function to our expression that only depends on the current state.</li>
<li><strong>Vallina Policy Gradient (VPG)</strong>:
$$
   \Psi_t = A^{\pi_{\theta}}(s_t, a_t) = Q^{\pi_{\theta}}(s_t, a_t) - V^{\pi_{\theta}}(s_t)
   $$
where $A^{\pi_{\theta}}(s_t, a_t)$ is the advantage function, $Q^{\pi_{\theta}}(s_t, a_t)$ is the action-value function, and $V^{\pi_{\theta}}(s_t)$ is the value function.</li>
</ol>
<p><strong>TODO</strong>: Why use VPG?</p>
<h2 id="5-proximal-policy-optimization-ppo">5. Proximal Policy Optimization (PPO)</h2>
<ul>
<li>DQL: can only be applied in relatively simple environments.</li>
<li>VPG: has poor data efficiency and robustness, meaning that we must collect tons of data from our environment to eliminate noise within the policy gradient estimate.</li>
</ul>
<p>Motivation for TRPO and PPO:</p>
<ul>
<li>Generally applicable (i.e., to both discrete and continuous problems)</li>
<li>Data efficient</li>
<li>Robust (i.e., works without too much tuning)</li>
<li>Simple (i.e., not too difficult to understand/implement)</li>
</ul>
<p>TRPO satisfies the first two points outlined above, while PPO satisfies all four.</p>
<h3 id="51-aligning-llms-with-rl">5.1. Aligning LLMs with RL</h3>
<div class="image-container">
    <img src="/imgs/blogs/reinforcement-learning-for-llms/align-procedure.webp" 
        alt="" 
        class="image" 
        width="100%"/>
    <div class="image-caption image-caption-center">
        Basic procedure for aligning LLMs.
    </div>
</div>
<p>After pre-training, the model perfroms next token prediction, but its output may be repetitive, uninteresting, or not useful. That&rsquo;s the reason alignment is needed.</p>
<p>Typically, we perform alignment by</p>
<ol>
<li>Selecting several alignment criteria (e.g., follow instructions, avoid harmful output, avoid hallucination, produce interesting/creative output, etc.)</li>
<li>Finetuning the model &mdash;&mdash; via SFT and RLHF &mdash;&mdash; to satisfy these criteria.</li>
<li>The final model can further finetuned and used to solve a downstream application via prompting (or in-context learning).</li>
</ol>
<div class="image-container">
    <img src="/imgs/blogs/reinforcement-learning-for-llms/sft-and-rlhf.webp" 
        alt="" 
        class="image" 
        width="100%"/>
    <div class="image-caption image-caption-center">
        Procedure of SFT and RLHF.
    </div>
</div>
<p>As shown in the figure above, to apply RLHF:</p>
<ol>
<li>Prepare a set of prompts and generate several outputs for each prompt with the language model.</li>
<li>Ask a group of human annotators to rank/score the responses to each prompt according to our alignment criteria.</li>
<li>Use these ranked responses to train a reward model that predicts a human preference score from a language model’s response.</li>
<li>Use PPO (or other algorithms, e.g., VPG, TRPO) to finetune our language model to maximize the human preferences scores (predicted by the reward model) of its outputs.</li>
</ol>
<h3 id="52-kullbackleibler-kl-divergence">5.2. Kullback–Leibler (KL) Divergence</h3>
<p>At the highest level, the Kullback-Leibler (KL) Divergence is just a method of comparing two probability distributions.</p>
<p>The idea of KL divergence has its roots in information theory and is highly related to the concept of entropy<span class="sidenote-number">
    <small class="sidenote">According to Shannon&rsquo;s Source Coding Theorem, the optimal number of bits required to encode a message with probability $p(x)$ is given by $−\log_{2}{p(x)}$.<br>
High probability event ($p(x) \approx 1$): $−log_2​(1)=0$. It takes very few bits to encode a highly probable event.<br>
Low probability event ($p(x) \approx 0$): $−log_2​(p(x))$ is a large number. It takes many bits to encode a rare event.</small>
</span>:</p>
$$
H=
\begin{cases}
-\mathbb{E} \left[ \log p(x) \right]      &\text{(Continuous Case)}  \\
-\sum_{i=1}^{N} p(x_i) \cdot \log p(x_i)  &\text{(Discrete Case)}
\end{cases}
$$<p>In the equation above, we can see common formulations of entropy $H$ for a probability distribution $p$. Intuitively, the entropy value captures how much information is stored within a probability distribution &mdash;&mdash; a lower entropy means that you would need fewer bits to encode the information stored within $p$.</p>
<p>Instead of a single probability distribution $p$, the KL divergence considers two probability distributions: $p$ and $q$. Then, mirroring the above entropy formulation, we compute KL divergence by finding the expected difference in log probabilities between these two distributions:</p>
$$
\begin{align*}
D_{\text{KL}}(p||q) &= H(p, q) - H(p) \\  &=
\begin{cases}
\mathbb{E} \left[ \log p(x) - \log q(x) \right] ~~~ \text{(Continuous Case)} \\
\sum_{i=1}^{N} p(x_i) \cdot \left( \log p(x_i) - \log q(x_i) \right) ~~~ \text{(Discrete Case)}
\end{cases}
\end{align*}
$$<p>where:</p>
<ul>
<li>$H(p,q)$: The average number of bits used with the approximate code.</li>
<li>$H(p)$: The minimum possible average number of bits used with the optimal code.</li>
<li>$D_{\text{KL}}(p||q)$: The penalty, or the expected number of extra bits &ldquo;wasted&rdquo; or &ldquo;lost&rdquo; per message due to the approximation.</li>
</ul>
<p>The KL divergence is commonly explained in the context of approximations. Namely, if we approximate $p$ with $q$, <strong>the KL divergence is the number of bits we would expect to lose by making this approximation</strong>.</p>
<p>KL divergence is heavily used across different domains of AI/ML research. For example, it is commonly used in loss functions for training neural networks, either as the core loss or as an added regularization term.</p>
<blockquote class="quote"><p>The final reward function we use during optimization contains a [KL divergence] penalty term … we find this constraint is useful for training stability, and to reduce reward hacking.” &mdash;&mdash; [4]</p></blockquote>
<h3 id="53-trust-region-policy-optimization-trpo">5.3. Trust Region Policy Optimization (TRPO)</h3>
<p>VPG is limited by the fact that <strong>it can only perform a single policy update for each estimate of the policy gradient that is derived</strong>. Given that VPG is notoriously data inefficient, meaning that <strong>we have to sample a lot of data when deriving a policy update</strong>, performing multiple (or larger) updates may seem enticing. However, such an approach is not justified theoretically and, in practice, leads to policy updates that are too large, thus damaging performance.</p>
<p>Trust Region Policy Optimization (TRPO) [5] aims to solve the problem described above using an approach that is similar to VPG. At each step of the optimization process, however, we find the largest possible policy update that still improves performance. Simply put, TRPO allows us to learn faster by finding a reliable way to make larger policy updates that do not damage performance.</p>
<p>More specifically, we update the policy under a constraint—based on the KL divergence—that captures the distance between policies before and after the current update. Considering this constraint allows us to find a balance between update size and the amount of change to the underlying policy:</p>
$$
\begin{equation*}
\begin{gathered}
\theta_{k+1} = \operatorname{argmax}_{\theta} \mathbb{E}_{(s,a) \sim (\pi_{\theta_k}, T)} \left[ \frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)} A^{\pi_{\theta_k}}(s,a) \right] \\
\text{such that } \overline{D}_{\text{KL}}(\theta||\theta_k) < \delta
\end{gathered}
\end{equation*}
$$<p>where:</p>
<ul>
<li>$\mathbb{E}_{(s,a) \sim (\pi_{\theta_k}, T)}$ is the expectation over state-action pairs sampled from the current policy $\pi_{\theta_k}$ and transition function $T$.</li>
<li>$\pi_{\theta}(a|s)$ is the probability of taking action $a$ in state $s$ according to the new policy $\pi_{\theta}$.</li>
<li>$\pi_{\theta_k}(a|s)$ is the probability of taking action $a$ in state $s$ according to the current policy $\pi_{\theta_k}$.</li>
<li>$A^{\pi_{\theta_k}}(s,a)$ is the advantage function for the current policy $\pi_{\theta_k}$.</li>
<li>$\overline{D}_{\text{KL}}(\theta||\theta_k)$ is the average KL divergence between the new policy $\pi_{\theta}$ and the current policy $\pi_{\theta_k}$.</li>
<li>$\delta$ is a hyperparameter that controls the maximum allowed change in the policy.</li>
</ul>
<p>Formulation of TRPO has several critical differences from VPG:</p>
<ul>
<li>The terms in the expectation are modified slightly to express the probability of a given action a as a ratio between old and updated policies.</li>
<li>The update has an added constraint based on the KL divergence between old and updated policies.</li>
<li>Instead of performing gradient ascent, we are solving a constrained maximization problem to generate each new policy</li>
</ul>
<p>The implementation of TRPO is similar to that of VPG. We allow our current policy to interact with the environment and collect data. From this observed data, we can compute the approximate update for TRPO as described above. Then, we can continue the process of collecting data and performing an update until we arrive at a policy that performs quite well.</p>
<p><strong>Because we are using the actual policy being trained to collect the data used to train it, TRPO is an on-policy reinforcement learning algorithm</strong>.</p>
<h3 id="54-trpo-vs-vpg-larger-policy-updates">5.4. TRPO vs. VPG: Larger Policy Updates</h3>
<p>As mentioned previously, the VPG algorithm is based upon gradient ascent, which &mdash;&mdash; by nature &mdash;&mdash; ensures that updates to the policy&rsquo;s parameters $\theta$ are not too large. In particular, we use a learning rate to perform updates with VPG, which can control the size of the update in the parameter space:</p>
$$
\theta_{t+1} = \theta_t + \alpha \nabla_{\theta} J(\pi_{\theta})|_{\theta_t}
$$<p>Here, only the size of the update to $\theta$ is controlled, and so that the old and updated policies are close in the parameter space. However, small changes to $\theta$ may also drastically alter the policy, because ensuring that policy updates are small in the parameter space does not provide much of a guarantee on changes to the resulting policy.</p>
<p><strong>As a result, we are constrained to relatively small updates within the VPG algorithm &mdash;&mdash; larger or multiple updates could be harmful</strong>.</p>
<p>TRPO sidesteps this issue by considering the size of our policy update from an alternative viewpoint. Namely, we compare updated and old policies using the KL divergence, which measures the difference in probability distributions over the action space produced by the two policies. Such an approach compares policies based upon the actions they take rather than their underlying parameters $\theta$.</p>
<p>In this way, we can perform large policy updates while ensuring that the new policy does not produce actions that are significantly different from the old policy.</p>
<h3 id="55-proximal-policy-optimization-ppo">5.5. Proximal Policy Optimization (PPO)</h3>
<blockquote class="quote"><p>We introduce proximal policy optimization, a family of policy optimization methods that use multiple epochs of stochastic gradient ascent to perform each policy update. These methods have the stability and reliability of trust-region methods but are much simpler to implement &hellip; applicable in more general settings, and have better overall performance. &mdash;&mdash; [6]</p></blockquote>
<p>TRPO has improved data efficiency, stability, and reliability compared to the VPG algorithm, but there are still limitations that need to be addressed.</p>
<p>Namely, the algorithm is complicated, can only perform a single update each time new data is sampled from the environment, and is only applicable to certain problem setups.</p>
<p>Aiming to develop a better approach, authors in [6] propose Proximal Policy Optimization (PPO), another policy gradient algorithm that alternates between collecting data from the environment and performing several epochs of training over this sampled data. PPO shares the reliability of TRPO and is:</p>
<ol>
<li>Much simpler</li>
<li>More data efficient</li>
<li>More generally applicable</li>
</ol>
<p>Similar to TRPO, we perform policy updates in PPO according to a surrogate objective. However, this surrogate objective has a &ldquo;clipped&rdquo; probability ratio, as shown in the equation below:</p>
$$
L^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min(\frac{\pi_{\theta}(o_t|q, o_{< t})}{\pi_{\theta_{old}}(o_t|q, o_{< t})}A_t, \text{CLIP}(\frac{\pi_{\theta}(o_t|q, o_{< t})}{\pi_{\theta_{old}}(o_t|q, o_{< t})}, 1-\varepsilon, 1+\varepsilon) A_t) \right]
$$<p>The surrogate objective for PPO is expressed as a minimum of two values. The first value is the same surrogate objective from TRPO, while the second value is a &ldquo;clipped&rdquo; version of this objective that lies within a certain range. In practice, this expression is formulated such that there is no reward for moving the probability ratio beyond the interval $[1 - \varepsilon, 1 + \varepsilon]$, see the figure below:</p>
<div class="image-container">
    <img src="/imgs/blogs/reinforcement-learning-for-llms/prob-ratio-to-L-CLIP.png" 
        alt="" 
        class="image" 
        width="100%"/>
    <div class="image-caption image-caption-center">
        From [2].
    </div>
</div>
<p><strong>In other words, PPO has no incentive for excessively large policy updates. Plus, by taking the minimum of the clipped and unclipped version of the surrogate objective, we only ignore excessive changes to the probability ratio if they improve the underlying objective. In the figure above, we see a basic depiction of this trend for both positive and negative values of the advantage function.</strong></p>
<p>To understand PPO&rsquo;s surrogate objective more intuitively, we should look at the figure below, which plots several objective functions as we interpolate between an old and updated policy obtained via PPO. In this figure, we see the KL divergence, the TRPO surrogate objective (labeled as CPI), the clipped surrogate objective, and the full PPO surrogate objective. From these plots, we can see that the PPO surrogate objective is a pessimistic/lower bound for the TRPO surrogate objective, where a penalty is incurred for having too large of a policy update.</p>
<div class="image-container">
    <img src="/imgs/blogs/reinforcement-learning-for-llms/different-objective-funcs.jpg" 
        alt="" 
        class="image" 
        width="100%"/>
    <div class="image-caption image-caption-center">
        From [2].
    </div>
</div>
<p>While TRPO sets a hard constraint to avoid policy updates that are too large, PPO simply formulates the surrogate objective such that a penalty is incurred if the KL divergence is too large. Such an approach is much simpler, as we no longer have to solve a difficult, constrained optimization problem. Rather, we can compute PPO’s surrogate loss with only minor tweaks to the VPG algorithm.</p>
<p>PPO has several benefits compared to TRPO. First, the implementation of PPO is much simpler compared to TRPO, as we can use automatic differentiation and gradient-based optimization techniques9 instead of deriving an (approximate) solution for a complex, constrained objective function. Additionally, while TRPO makes only a single policy update each time new data is collected, PPO performs multiple epochs of optimization via stochastic gradient ascent over the surrogate objective, which improves data efficiency.</p>
<p>Finally, computing estimates of the advantage function (e.g., via Generalized Advantage Estimation (GAE)) typically requires that we learn a corresponding value function. In TRPO, we must learn this state-value function with a separate neural network. However, PPO—due to its compatibility with a wider scope of architectures (including those with parameter sharing) &mdash;&mdash; can train a joint network for policy and value functions by just adding an extra term to the loss function that computes the mean-squared error (MSE) between estimated and actual value function values.</p>
$$
L^{\text{CLIP+VF}}(\theta) = \mathbb{E}_t \left[ L^{\text{CLIP}}(\theta) - c_1 L^{\text{VF}}(\theta) \right]
$$<p>where:</p>
<ul>
<li>$L^{\text{CLIP}}(\theta)$ is the PPO surrogate objective.</li>
<li>$L^{\text{VF}}(\theta)$ is the MSE loss for the value function.</li>
<li>$c_1$ is a hyperparameter that controls the weight of the value function loss in the overall loss function.</li>
</ul>
<h2 id="6-group-relative-policy-optimization-grpo">6. Group Relative Policy Optimization (GRPO)</h2>
<div class="image-container">
    <img src="/imgs/blogs/reinforcement-learning-for-llms/ppo-vs-grpo.png" 
        alt="" 
        class="image" 
        width="100%"/>
    <div class="image-caption image-caption-center">
        From [7].
    </div>
</div>
<p>Different from PPO, GRPO:</p>
<ol>
<li>Removes the value function model.</li>
<li>The policy model generates multiple outputs for each input, and the reward model calculates the reward for each output, and calculates the advantage scores after group computation.</li>
<li>Removes the GAE, and changes the method to calculate KL.</li>
</ol>
<p>In PPO, we optimizes LLMs by maximizing the following objective function:</p>
$$
\begin{align*}
\mathcal{J}_{\text{PPO}}(\theta) = \mathbb{E}&_{q \sim P(Q), o \sim \pi_{\theta_{\text{old}}}(O|q)} \left[ \frac{1}{|o|} \sum_{t=1}^{|o|} \min \left( \frac{\pi_{\theta}(o_t|q, o_{< t})}{\pi_{\theta_{\text{old}}}(o_t|q, o_{< t})} A_t, \right. \right. \\
& \left. \left. \text{clip}\left(\frac{\pi_{\theta}(o_t|q, o_{< t})}{\pi_{\theta_{\text{old}}}(o_t|q, o_{< t})}, 1-\varepsilon, 1+\varepsilon\right) A_t \right) \right]
\end{align*}
$$<p>where:</p>
<ul>
<li>$\pi_{\theta}$ and $\pi{\theta_{\text{old}}}$ are the current and old policy models;</li>
<li>$q$, $o$ are questions and outputs sampled from the question dataset and the old policy $\pi{\theta_{\text{old}}}$;</li>
<li>$\varepsilon$ is a clipping-related hyper-parameter introduced in PPO for stabilizing training;</li>
<li>$A_t$ is the advantage, which is computed by applying Generalized Advantage Estimation (GAE), based on the rewards ${r_{\geq t}}$ and a learned value function $V_{\psi}$.</li>
</ul>
<p>Thus, in PPO, a value function needs to be trained alongside the policy model and to mitigate over-optimization of the reward model, the standard approach is to add a per-token KL penalty from a reference model in the reward at each token, i.e.:</p>
$$
r_t = r_{\varphi}(q, o_{\leq t}) - \beta \log \frac{\pi_{\theta}(o_t|q, o_{< t})}{\pi_{ref}(o_t|q, o_{< t})}
$$<p>There are several issues with PPO:</p>
<ol>
<li>As the value function employed in PPO is typically another model of comparable size as the policy model, it brings a substantial memory and computational burden.</li>
<li>During RL training, the value function is treated as a baseline in the calculation of the advantage for variance reduction. While in the LLM context, usually only the last token is assigned a reward score by the reward model, which may complicate the training of a value function that is accurate at each token.</li>
</ol>
<p>To address these issues, for each question $q$, GRPO samples a group of outputs $\{o_1, o_2, \dots , o_G\}$ from the old policy $\pi_{\theta_{\text{old}}}$ and then optimizes the policy model by maximizing the following objective:</p>
$$
\begin{align*}
\mathcal{J}_{\text{GRPO}}(\theta) = \mathbb{E}&[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O|q)] \\
&\frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left\{ \min\left(\frac{\pi_{\theta}(o_{i,t}|q, o_{i,< t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q, o_{i,< t})} \hat{A}_{i,t}, \right.\right. \\
&\left.\left. \text{clip}\left(\frac{\pi_{\theta}(o_{i,t}|q, o_{i,< t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q, o_{i,< t})}, 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i,t}\right) - \beta D_{KL}[\pi_{\theta}||\pi_{ref}] \right\}
\end{align*}
$$<p>where:</p>
<ul>
<li>$\varepsilon$ and $\beta$ are hyper-parameters;</li>
<li>$\hat{A}_{i,t}$ is the advantage calculated based on relative rewards of the outputs inside each group only. See <a href="https://github.com/jamesnulliu/verl/blob/archive-d9a6a31/verl/trainer/ppo/core_algos.py#L244-L308" target="_blank" rel="noopener noreferrer">here</a> for verl&rsquo;s implementation. For simplicity, given input <code>scores</code> (a tensor of padded rewards with shape <code>(batch_size, response_len)</code>), the algorithm first sums the rewards for each response, then normalizes the rewards in each group (a batch may contain multiple groups), and finally broadcasts and   multiplies the rewards (of shape <code>(batch_size, 1)</code>) with reponse mask (of shape <code>(batch_size, response_len)</code>) to get the advantage<span class="sidenote-number">
    <small class="sidenote">That is to say, for each response of shape <code>(1, response_len)</code>, the advantages are the same, except for the padded positions, which are 0.</small>
</span>.</li>
</ul>
<p>Note that:</p>
<ol>
<li>Instead of adding KL penalty in the reward, GRPO regularizes by directly adding the KL divergence between the trained policy and the reference policy to the loss, avoiding complicating the calculation of $\hat{A}_{i,t}$.</li>
<li>Different from the KL penalty term used in [6], GRPO estimate the KL divergence with the following unbiased estimator:
$$
   \mathbb{D}_{\text{KL}}\left[ \pi_{\theta}||\pi_{ref}\right] = \frac{\pi_{ref}(o_{t,t}|q, o_{t,< t})}{\pi_{\theta}(o_{t,t}|q, o_{t,< t})} - \log \frac{\pi_{ref}(o_{t,t}|q, o_{t,< t})}{\pi_{\theta}(o_{t,t}|q, o_{t,< t})} - 1
   $$
which is guaranteed to be positive.</li>
</ol>
<p>The code of the objective function, i.e., <code>compute_policy_loss</code>, is show <a href="https://github.com/jamesnulliu/verl/blob/archive-d9a6a31/verl/trainer/ppo/core_algos.py#L722-L794" target="_blank" rel="noopener noreferrer">here</a> in verl. It can be seen that the implementation is a bit different from the equation above:</p>
<ul>
<li>The $\frac {\pi_{\theta}(o_{i,t}|q, o_{i,< t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q, o_{i,< t})}$ term is replaced with $e^{\log \pi_{\theta}(o_{i,t}|q, o_{i,< t}) - \log \pi_{\theta_{\text{old}}}(o_{i,t}|q, o_{i,< t})}$, which is more numerically stable, as shown in <a href="https://github.com/jamesnulliu/verl/blob/archive-d9a6a31/verl/trainer/ppo/core_algos.py#L766-L769" target="_blank" rel="noopener noreferrer">these lines</a> in function <code>compute_policy_loss</code>.</li>
<li>The kl penalty term is removed, and calculated separately in <code>DataParallelPPOActor.update_policy</code>, according to option <code>use_kl_loss</code> (which is default false but shoule be set to true for GRPO), as shown <a href="https://github.com/jamesnulliu/verl/blob/archive-d9a6a31/verl/workers/actor/dp_actor.py#L529-L539" target="_blank" rel="noopener noreferrer">here</a>.</li>
<li>Both the policy loss and the KL loss would be passed to function <code>agg_loss</code>, which aggregates them to scalar values. The default method is <code>token-mean</code>, which is different from &ldquo;<em>the original GRPO paper takes the sample-level loss (<code>seq-mean-token-mean</code>), which may be unstable in long-CoT scenarios</em>&rdquo;.
<blockquote class="quote"><p>All GRPO example scripts provided in verl uses the default configuration &ldquo;token-mean&rdquo; for loss aggregation instead.</p></blockquote>
Btw, <code>token-mean</code> will means the (policy gradient) loss across all the tokens in all the sequences in a mini-batch; <em>An idea of DAPO?</em></li>
</ul>
<h2 id="7-dapo-an-open-source-llm-reinforcement-learning-system-at-scale-8">7. DAPO: An Open-Source LLM Reinforcement Learning System at Scale [8]</h2>
<ol>
<li>Clip-Higher, which promotes the diversity of the system and avoids entropy collapse; Configured <a href="https://github.com/jamesnulliu/verl/blob/archive-d9a6a31/recipe/dapo/run_dapo_qwen2.5_32b.sh#L76-L77" target="_blank" rel="noopener noreferrer">here</a> and used <a href="https://github.com/jamesnulliu/verl/blob/archive-d9a6a31/verl/trainer/ppo/core_algos.py#L773-L779" target="_blank" rel="noopener noreferrer">here</a> in <code>compute_policy_loss</code>.</li>
<li>Dynamic Sampling, which improves training efficiency and stability; Shown <a href="https://github.com/jamesnulliu/verl/blob/archive-d9a6a31/recipe/dapo/dapo_ray_trainer.py#L199-L257" target="_blank" rel="noopener noreferrer">here</a> in <code>RayDPAOTrainer</code>.</li>
<li>Token-Level Policy Gradient Loss, which is critical in long-CoT RL scenarios; Configured <a href="https://github.com/jamesnulliu/verl/blob/archive-d9a6a31/recipe/dapo/run_dapo_qwen2.5_32b.sh#L99" target="_blank" rel="noopener noreferrer">here</a>, which has been introduced in the above GRPO section.</li>
<li>Overlong Reward Shaping, which reduces reward noise and stabilizes training; Configured <a href="https://github.com/jamesnulliu/verl/blob/archive-d9a6a31/recipe/dapo/run_dapo_qwen2.5_32b.sh#L117-L119" target="_blank" rel="noopener noreferrer">here</a> and used <a href="https://github.com/volcengine/verl/blob/4f80e465c2ec79ab9c3c30ec74b9745de61d0490/verl/workers/reward_manager/dapo.py#L100-L113" target="_blank" rel="noopener noreferrer">here</a> in <code>DapoRewardManager</code>.</li>
</ol>
<p>Before defining the object function, it is worth noting that the KL term is excluded from the proposed algorithm.</p>
<blockquote class="quote"><p>The KL penalty term is used to regulate the divergence between the online policy and the frozen reference policy. In the RLHF scenario, the goal of RL is to align the model behavior without diverging too far from the initial model. However, during training the long-CoT reasoning model, the model distribution can diverge significantly from the initial model, thus this restriction is not necessary.</p></blockquote>
<p>The objective function of DAPO is shown below:</p>
$$
\begin{align*}
    \mathcal{J}_{\text{DAPO}}(\theta) =& ~ \mathbb{E}_{(q,a) \sim \mathcal{D}, \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(\cdot|q)} \\
    &\left[ \frac{1}{\sum_{i=1}^G |o_i|} \sum_{i=1}^G \sum_{t=1}^{|o_i|} \min \left( r_{i,t}(\theta)\hat{A}_{i,t}, \text{clip}(r_{i,t}(\theta), 1 - \epsilon_{\text{low}}, 1 + \epsilon_{\text{high}})\hat{A}_{i,t} \right) \right]
    \\
    \text{s.t. } &0 < \left | \{o_i \mid \text{is_equivalent}(a, o_i)\} \right | < G,
\end{align*}
$$<p>where:</p>
$$
r_{i,t}(\theta) = \frac{\pi_\theta(o_{i,t} \mid q, o_{i, < t})}{\pi_{\theta_{\text{old}}}(o_{i,t} \mid q, o_{i, < t})}, \quad \hat{A}_{i,t} = \frac{R_i - \text{mean}(\{R_i\}_{i=1}^G)}{\text{std}(\{R_i\}_{i=1}^G)}
$$<h2 id="8-beyond-the-8020-rule-high-entropy-minority-tokens-drive-effective-reinforcement-learning-for-llm-reasoning-9">8. Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning [9]</h2>
<p>Token entropy calculation:</p>
$$
\begin{gather*}
H_t := - \sum_{j=1}^{V} p_{t,j} \log p_{t,j} \\
\text{ where } (p_{t,1}, \cdots, p_{t,V}) = \boldsymbol{p}_t = \pi_{\theta}(\cdot \mid q, o_{ < t}) = \text{Softmax}\left(\frac{\mathbf{z}_t}{T}\right)
\end{gather*}
$$<p>Here,</p>
<ul>
<li>$\pi_{\theta}$ denotes an LLM parameterized by $\theta$</li>
<li>$q$ is the input query, and $o_{< t} = (o_1, o_2, \ldots, o_{t-1})$ represents the previously generated tokens</li>
<li>$V$ is the vocabulary size</li>
<li>$\mathbf{z}_t \in \mathbb{R}^V$ denotes the pre-softmax logits at time step $t$</li>
<li>$\boldsymbol{p}_t \in \mathbb{R}^V$ is the corresponding probability distribution over the vocabulary</li>
<li>$T \in \mathbb{R}$ is the decoding temperature</li>
</ul>
<p>In off-policy settings, sequences are generated by a rollout policy $\pi_{\phi}$ while the training policy is $\pi_{\theta}$, with $\phi \neq \theta$. The entropy is still calculated using $\pi_{\theta}$, as defined in Equation (1), to measure the uncertainty of the training policy in the given sequence.</p>
<h2 id="references">References</h2>
<ol>
<li>Cameron R. Wolfe. <a href="https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning" target="_blank" rel="noopener noreferrer">Basics of Reinforcement Learning for LLMs</a>.</li>
<li>Cameron R. Wolfe. <a href="https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo" target="_blank" rel="noopener noreferrer">Proximal Policy Optimization (PPO): The Key to LLM Alignment</a>.</li>
<li>Achiam, Josh. <a href="https://spinningup.openai.com/en/latest/index.html" target="_blank" rel="noopener noreferrer">Spinning Up in Deep RL</a>. OpenAI, 2018.</li>
<li>Touvron, Hugo, et al. &ldquo;Llama 2: Open foundation and fine-tuned chat models.&rdquo; arXiv preprint arXiv:2307.09288 (2023).</li>
<li>Schulman, John, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. &ldquo;Trust region policy optimization.&rdquo; In International conference on machine learning, pp. 1889-1897. PMLR, 2015.</li>
<li>Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. &ldquo;Proximal policy optimization algorithms.&rdquo; arXiv preprint arXiv:1707.06347 (2017).</li>
<li>Shao, Zhihong, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang et al. &ldquo;Deepseekmath: Pushing the limits of mathematical reasoning in open language models.&rdquo; arXiv preprint arXiv:2402.03300 (2024).</li>
<li>Yu, Qiying, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai et al. &ldquo;Dapo: An open-source llm reinforcement learning system at scale.&rdquo; arXiv preprint arXiv:2503.14476 (2025).</li>
<li>Wang, Shenzhi, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang et al. &ldquo;Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning.&rdquo; arXiv preprint arXiv:2506.01939 (2025).</li>
</ol>
]]></content:encoded></item><item><title>Arithmetic Intensity Estimation of Large Language Models</title><link>https://jamesnulliu.github.io/blogs/arithmetic-intensity-estimation-of-large-language-models/</link><pubDate>Thu, 13 Mar 2025 17:38:00 +0800</pubDate><guid>https://jamesnulliu.github.io/blogs/arithmetic-intensity-estimation-of-large-language-models/</guid><description>This blog post discusses the arithmetic intensity of large language models and how it affects the performance of these models.</description><content:encoded><![CDATA[<h2 id="1-estimating-total-flops">1. Estimating Total FLOPs</h2>
<p>We only consider the FLOPs of Transformer layers, excluding the embedding layer and the output layer.</p>
<ul>
<li>
<p><strong>Attention</strong>:</p>
<ol>
<li>Each projection for Q, K and V is matmul of input <code>(B, S, H)</code> and weight <code>(H, H)</code>, yielding <code>(B, S, H)</code>:<br>
$$
    \text{FLOPs} = 3 \times (2 \times B \times S \times H \times H) = 6 \times B \times S \times H^2
    $$</li>
<li>$S = QK^T$, matmul of $Q$ <code>(B, S, H)</code> and $K^T$ <code>(B, H, S)</code>, yielding <code>(B, S, S)</code>:<br>
$$
    \text{FLOPs} = 2 \times B \times S \times S \times H = 2 \times B \times S^2 \times H
    $$</li>
<li>$L = S \cdot V$, matmul of $S$ <code>(B, S, S)</code> and $V$ <code>(B, S, H)</code>, yielding <code>(B, S, H)</code>:<br>
$$
    \text{FLOPs} = 2 \times B \times S \times H \times S
    $$</li>
<li>$O = L \cdot W_O$, matmul of $L$ <code>(B, S, H)</code> and $W_O$ <code>(H, H)</code>, yielding <code>(B, S, H)</code>:<br>
$$
    \text{FLOPs} = 2 \times B \times S \times H^2
    $$</li>
<li>Total attention FLOPs per Transformer layer:
$$
    \text{FLOPs} = 8 \times B \times S \times H^2 + 4 \times B \times S^2 \times H
    $$</li>
</ol>
</li>
<li>
<p><strong>Feed-Forward Networek</strong><br>
Typically 2 linear layers, one mapping <code>(B, S, H)</code> to <code>(B, S, 4H)</code> and the other mapping <code>(B, S, 4H)</code> to <code>(B, S, H)</code>:</p>
<ul>
<li>
<p>Total FFN FLOPs per Transformer layer:</p>
$$
    \begin{align*}
    \text{FLOPs} &= 2 \times B \times S \times H \times (4 \times H) + 2 \times B \times S \times (4 \times H) \times H \\
                 &= 16 \times B \times S \times H^2
    \end{align*}
    $$</li>
</ul>
</li>
<li>
<p><strong>Total FLOPs: $N$ Layers of Transformer</strong><br>
Each Transformer layer consists of an attention mechanism and a feed-forward network</p>
<ul>
<li>
<p>When prefilling, the total FLOPs is:</p>
$$
    \text{FLOPs}_\text{total} = N (24 B S H^2 + 4 B S^2 H)
    $$</li>
<li>
<p>When decoding, suppose the input is of shape <code>(B, Si, H)</code> and KV cache is of shape <code>(B, Sc, H)</code>, the total FLOPs is:</p>
$$
    \text{FLOPs}_\text{total} = N (24 B S_i H^2 + 4 B S_i S_c H)
    $$</li>
</ul>
</li>
</ul>
<h2 id="2-estimating-total-bytes-transfered">2. Estimating Total Bytes Transfered</h2>
<p>In FP16, each parameter or activation element is 2 bytes.</p>
<p>Data transferred includes <strong>loading model weights</strong> and <strong>handling activations</strong>.</p>
<p>Suppose we have a $Z$-B-fp16 model and $N$ Transformer layers, each with input size <code>(B, S, H)</code>.</p>
<ul>
<li>
<p><strong>Model Weights</strong><br>
A $Z$-B-fp16 model has $Z \times 10^9$ <code>fp16</code> parameters, each 2 bytes:</p>
$$
  \text{Bytes}_\text{weights} = Z \times 10^9 \times 2 ~ \text{Bytes} = 2 \times Z ~ \text{GBytes}
  $$<p>In an optimized GPU inference, weights are typically loaded into high-bandwidth memory (HBM) once and reused, so we assume $2Z$ GB is read once per forward pass.</p>
</li>
<li>
<p><strong>Activations</strong></p>
<ul>
<li>For each Transfomer layer, input and output activations are of shape <code>(B, S, H)</code>, and each element is 2 bytes in <code>fp16</code>:
$$
      \text{Bytes}_\text{act-layer} = B \times S \times H \times 2 ~ \text{Bytes}
      $$</li>
<li>For $N$ layers, activations are computed sequentially. Since each layer’s output becomes the next layer’s input (read once, written once):<br>
$$
    \begin{align*}
    \text{Bytes}_\text{act-total} &= 2 \times N \times  \text{Bytes}_\text{act-layer} ~ \text{Bytes} \\
                                   &= 4 \times N \times B \times S \times H ~ \text{Bytes}
    \end{align*}
    $$</li>
</ul>
</li>
<li>
<p><strong>KV Caches</strong><br>
When decoding, each Transformer layer would load cached K and V both of shape <code>(B, Sc, H)</code>. After decoding, the new K and V of shape <code>(B, Si, H)</code> are computed and cached for the next layer. So the bytes transfered for one forward pass is:</p>
$$
  \text{Bytes}_\text{KV} = N \times (B \times S_c \times H + 2 \times B \times S_i \times H) \times 2 ~ \text{Bytes}
  $$</li>
<li>
<p><strong>Total Data Transferred</strong></p>
<ul>
<li>
<p>When prefilling, the total bytes transferred is:</p>
$$
    \begin{align*}
    \text{Bytes}_\text{total} &= \text{Bytes}_\text{weights} + \text{Bytes}_\text{act-total} \\
                              &= 2 Z \text{e}^9 + 4 N B S H ~ \text{Bytes}
    \end{align*}
    $$</li>
<li>
<p>When decoding, suppose cached sequence length is $S_c$ and the input sequence length is $S_i$, the total bytes transferred is:</p>
$$
    \begin{align*}
    \text{Bytes}_\text{total} &= \text{Bytes}_\text{weights} + \text{Bytes}_\text{act-total} + \text{Bytes}_\text{KV} \\
                              &= 2 Z \text{e}^{9} + 8 N B S_i H + 2 N B S_c H ~ \text{Bytes}
    \end{align*}
    $$</li>
</ul>
</li>
</ul>
<h2 id="3-arithmetic-intensity">3. Arithmetic Intensity</h2>
<p>When prefilling, there is no cached K and V, so the arithmetic intensity is:</p>
$$
\begin{align*}
\text{Arithmetic Intensity} &= \text{FLOPs}_\text{total} / \text{Bytes}_\text{total} \\
                            &= \frac{N (24 B S H^2 + 4 B S^2 H)}{2 Z 10^9 + 4 N B S H}
\end{align*}
$$<p>When decoding, suppose cached sequence length is $S_c$ and the input sequence length is $S_i$ , then the arithmetic intensity is:</p>
$$
\begin{align*}
\text{Arithmetic Intensity} &= \text{FLOPs}_\text{total} / \text{Bytes}_\text{total} \\
                            &= \frac{N (24 B S_i H^2 + 4 B S_i S_c H)}{2 Z 10^9 + 8 N B S_i H + 2 N B S_c H}
\end{align*}
$$<h2 id="4-roofline-model">4. Roofline Model</h2>
<div class="image-container">
    <img src="/imgs/blogs/arithmetic-intensity-estimation-of-large-language-models/roofline_model.png" 
        alt="" 
        class="image" 
        width="80%"/>
    <div class="image-caption">
        Roofline Model. If the arithmetic intensity is on the right side of the machine balance, the performance compute-bound. If it is on the left side, the performance is memory-bound.
    </div>
</div>
<p>A100-80GB has the following hardware `specifications:</p>
<ul>
<li><strong>Peak FLOPs</strong> ($\pi$): $312 \times 10^{12}$ FLOPs/s</li>
<li><strong>Memory Bandwidth</strong> ($\beta$): $2039 \times 10^9$ B/s</li>
<li><strong>Machine Balance</strong> ($I_{max}$): $312 \times 10^{12} / (2039 \times 10^9) \approx 153$ FLOPs/Byte</li>
</ul>
<p>Here are two examples of arithmetic intensity estimation:</p>
<ul>
<li>See: <a href="https://www.geogebra.org/calculator/uqzhngtf" target="_blank" rel="noopener noreferrer">
    Arithmetic Intensity for Prefilling
</a>
</li>
<li>See: <a href="https://www.geogebra.org/calculator/tkkekjdb" target="_blank" rel="noopener noreferrer">
    Arithmetic Intensity for Speculative Decoding
</a>
</li>
</ul>
<h2 id="5-discussion-tensor-parallelism">5. Discussion: Tensor Parallelism</h2>
<p>If the model is split across multiple GPUs using TP, the hidden size <code>H</code> and the model weight is divided by the number of GPUs.</p>
]]></content:encoded></item><item><title>A Brief Talk on Speculative Decoding</title><link>https://jamesnulliu.github.io/blogs/a-brief-talk-on-speculative-decoding/</link><pubDate>Fri, 21 Feb 2025 01:14:06 +0800</pubDate><guid>https://jamesnulliu.github.io/blogs/a-brief-talk-on-speculative-decoding/</guid><description>A brief talk on speculative decoding in large language models.</description><content:encoded><![CDATA[<iframe src="https://docs.google.com/spreadsheets/d/e/2PACX-1vS8hnf4AA8xVJCNKTACvm4H_Lnu6kXtfB7tdL4Iv90OcsuXBnMs87XaVll4Dz0XhmXbjvbjKIeu8k3r/pubhtml" frameborder="0" width="100%" height="400"></iframe>
<h2 id="1-introduction-to-speculative-decoding">1. Introduction to Speculative Decoding</h2>
<p>Given a score model <code>S</code> (for example, LLAMA-3-70B) and a draft model <code>D</code> (for example, LLAMA-3-7B), the process of <strong>speculative decoding</strong> can be described as follows:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">input_ids</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># (seq_len,)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># D generates tokens[seq_len, ..., seq_len + k]</span>
</span></span><span class="line"><span class="cl">    <span class="n">draft_outputs</span> <span class="o">=</span> <span class="n">D</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>  <span class="c1"># (k,)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Given tokens[seq_len - 1, ..., seq_len + k], S generates real </span>
</span></span><span class="line"><span class="cl">    <span class="c1"># prediction for tokens[seq_len, ..., seq_len + k, seq_len + k + 1] with</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># one forward pass.</span>
</span></span><span class="line"><span class="cl">    <span class="n">score_outputs</span> <span class="o">=</span> <span class="n">S</span><span class="p">(</span><span class="n">cat</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">draft_outputs</span><span class="p">))</span>  <span class="c1"># (k + 1,)</span>
</span></span><span class="line"><span class="cl">    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">verify</span><span class="p">(</span><span class="n">draft_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">score_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">            <span class="k">break</span>
</span></span><span class="line"><span class="cl">        <span class="n">input_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">draft_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">input_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> 
</span></span></code></pre></td></tr></table>
</div>
</div><div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/sd-workflow-in-vllm-k-3-p-1.png" 
        alt="" 
        class="image" 
        width="85%"/>
    <div class="image-caption">
        Speculative decoding workflow in vLLM (k=3, top-p=1). k=3 indicates that the draft model generates 3 tokens per forward pass, and top-p=1 means that for each token, only 1 candidate is proposed. As shown in the picture, at prefill statge, input sequence would first be fed into both draft and score models to acquire kv caches. The output of draft model at this stage is omitted. Then, T5 is fed into draft model to generate proposed T6&#39;, T7&#39;, and T8&#39;. To verify these tokens, T5, T6&#39;, T7&#39; and T8&#39; are fed into the score model to get T6, T7*, T8* and T9* in one forward pass. Note that here T6 must be correct because it is generated by T5 through the score model; However, T7*, T8* and T9* are not guaranteed to be correct. The final step is to verify T6&#39;, T7&#39; and T8&#39; to see if T7*, T8* and T9* are correct. For example, if T6&#39; and T7&#39; is correct, then the final accepted tokens would be T6&#39;, T7&#39; and T8&#39;, which means the socore model generates 3 tokens in one forward pass.
    </div>
</div>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/sd-workflow-in-vllm-k-1-p-1.png" 
        alt="" 
        class="image" 
        width="80%"/>
    <div class="image-caption">
        Workflow of spuculative decoing in vLLM (k=1, top-p=1). Like the previous picture, if T6&#39; is correct, then the final accepted tokens would be T6&#39; and T7*, one generated by the draft model and the other by the score model. The score model generates 2 tokens in one forward pass.
    </div>
</div>
<h2 id="2-how-speculative-decoding-works-in-vllm">2. How Speculative Decoding Works in vLLM</h2>
<p>In vLLM, speculative decoding is integrated with the system&rsquo;s continuous batching architecture, where different requests are processed together in a single batch, enabling higher throughput. vLLM uses two key components to implement this:</p>
<ul>
<li><strong>Draft Runner</strong>: This runner is responsible for executing <strong>the smaller proposer model</strong> to propose candidate tokens.</li>
<li><strong>Target Runner</strong>: The target runner verifies the tokens by running <strong>the larger scorer model</strong>.</li>
</ul>
<p>vLLM&rsquo;s system is optimized to handle this process efficiently, allowing speculative decoding to work seamlessly with continuous batching, which increases the overall system performance.</p>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/sd-in-vllm.png" 
        alt="" 
        class="image" 
        width=""/>
    <div class="image-caption">
        Diagram illustrating how the draft and target runners interact within the vLLM batching system.
    </div>
</div>
<p>To implement speculative decoding in vLLM, two crucial components had to be modified:</p>
<ul>
<li><strong>Scheduler</strong>: The scheduler was adjusted to handle multiple token slots within a single forward pass, enabling the simultaneous generation and verification of several tokens.</li>
<li><strong>Memory Manager</strong>: The memory manager now handles the KV cache for both the draft and scorer models, ensuring smooth processing during speculative decoding.</li>
</ul>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/vllm-sd-system-archi.png" 
        alt="" 
        class="image" 
        width="80%"/>
    <div class="image-caption">
        System architecture of speculative decoding in vLLM. 
    </div>
</div>
<h2 id="3-types-of-speculative-decoding-supported-in-vllm">3. Types of Speculative Decoding Supported in vLLM</h2>
<h3 id="31-draft-model-based-speculative-decoding">3.1. Draft Model-Based Speculative Decoding</h3>
<p>This is the most commonly used form of speculative decoding, where a smaller model predicts the next tokens, and a larger model verifies them. A common example would be using a Llama 68M model to predict tokens for a Llama 2 70B model. This approach requires careful selection of the draft model to balance accuracy and overhead.</p>
<p>Choosing the correct draft model is essential for maximizing the efficiency of speculative decoding. The draft model needs to be small enough to avoid creating significant overhead but still accurate enough to provide a meaningful performance boost.</p>
<p>However, <strong>selecting the right draft model</strong> can be challenging. For example, in models like Llama 3, finding a suitable draft model is difficult due to differences in vocabulary size. Speculative decoding requires that the draft and target models <strong>share the same vocabulary</strong>, and in some cases, this can limit the use of speculative decoding. Therefore, in the following sections, we introduce several draft-model free speculative decoding methods.</p>
<h3 id="32-prompt-lookup-decoding">3.2. Prompt Lookup Decoding</h3>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/prompt-lookup-decoding.png" 
        alt="" 
        class="image" 
        width="80%"/>
    <div class="image-caption">
        An example of prompt lookup decoding. Given the prompt, we build all 2-grams as the lookup key. The values are the three tokens following the lookup key. During generation, we will check if the current 2-gram matches any key. If so, we will propose the following tokens with the value.
    </div>
</div>
<p>Otherwise known as n-gram matching, this approach is effective for use cases like summarization and question-answering, where there is a significant overlap between the prompt and the answer. Instead of using a small model to propose tokens, the system speculates based on the information already available in the prompt. This works particularly well when the large model repeats parts of the prompt in its answers.</p>
<h2 id="4-medusa">4. MEDUSA</h2>
<h3 id="41-roadmap">4.1. Roadmap</h3>
<ol>
<li><a href="https://github.com/vllm-project/vllm/issues/1023" target="_blank" rel="noopener noreferrer">
    [vllm][ISSUE] | Can vLLM support medusa head? #1023
</a>
</li>
<li><a href="https://github.com/vllm-project/vllm/issues/1171" target="_blank" rel="noopener noreferrer">
    [vllm][ISSUE] | [Discussion] Will vLLM consider using Speculative Sampling to accelerating LLM decoding? #1171
</a>
</li>
<li><a href="https://github.com/vllm-project/vllm/pull/4978" target="_blank" rel="noopener noreferrer">
    [vllm][PR] | [Speculative Decoding] Medusa Implementation with Top-1 proposer #4978
</a>
</li>
</ol>
<h3 id="41-medusa-heads">4.1. <strong>MEDUSA Heads</strong></h3>
<p>MEDUSA heads are additional decoding heads appended to the last hidden states of the original model.</p>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/medusa.png" 
        alt="" 
        class="image" 
        width="70%"/>
    <div class="image-caption">
        Three heads are used to propose tokens for the following three positions. Head 1 is proposing [&#34;is&#34;, &#34;\&#39;&#34;, &#34;the&#34;] for the first position. Head 2 is proposing [&#34;difficult&#34;, &#34;is&#34;, &#34;\&#39;&#34;] for the second position. Head 3 is proposing [&#34;not&#34;, &#34;difficult&#34;, &#34;a&#34;] for the third position. NOTE: All heads take the output of the last transformer block as the input.
    </div>
</div>
<p>Specifically, given the original model’s last hidden states $h_t$ at position $t$, we add $K$ decoding heads to $h_t$. The $k$-th head is used to predict the token in the $(t + k + 1)$-th position of the next tokens (the original language model head is used to predict the $(t + 1)$-th position).</p>
$$
\begin{aligned}
p_{t}^{(k)} & =\mathrm{softmax}\left(W_{2}^{(k)}\cdot\left(\mathrm{SiLU}(W_{1}^{(k)}\cdot h_{t})+h_{t}\right)\right), \\
 & \mathrm{where~}W_{2}^{(k)}\in\mathbb{R}^{d\times V},W_{1}^{(k)}\in\mathbb{R}^{d\times d}.
\end{aligned}
$$<p>Unlike a draft model, MEDUSA heads are trained in conjunction with the original backbone model, which can remain frozen during training (MEDUSA-1) or be trained together (MEDUSA-2).</p>
<h3 id="42-tree-attention">4.2. <strong>Tree Attention</strong></h3>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/tree-attn.png" 
        alt="" 
        class="image" 
        width="70%"/>
    <div class="image-caption">
        
    </div>
</div>
<p>The top-2 predictions from the first MEDUSA head and the top-3 from the second result in a total of $2 \times 3 = 6$ candidates. Each of these candidates corresponds to a distinct branch within the tree structure.</p>
<p>To guarantee that each token only accesses its predecessors, an attention mask is devised that exclusively permits attention flow from the current token back to its antecedent tokens.</p>
<h2 id="5-eagle">5. EAGLE</h2>
<h3 id="51-roadmap">5.1. Roadmap</h3>
<ol>
<li><a href="https://github.com/vllm-project/vllm/pull/6830" target="_blank" rel="noopener noreferrer">
    [vllm][PR] |  [Speculative Decoding] EAGLE Implementation with Top-1 proposer #6830
</a>
</li>
</ol>
<h3 id="52-detailed-process">5.2. Detailed Process</h3>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/eagle-compare.png" 
        alt="" 
        class="image" 
        width="80%"/>
    <div class="image-caption">
        A comparison of the methods for drafting the fourth and fifth tokens, t4 and t5. t (represented by blue blocks) denotes tokens, and f (orange blocks) signifies the features, with subscripts indicating their positions in the sequence.  The red border indicates the predictions of the draft model. For simplicity, the n in the n-gram for Lookahead, as shown in the figure, has been set to 2.
    </div>
</div>
<p>This link is a Feishu drawboard to show the detailed process of speculative decoding with EAGLE in vLLM:</p>
<ul>
<li><a href="https://ncnqdau83tum.feishu.cn/docx/PliBdWWPWohaClxAagjcZqcZnMe?from=from_copylink" target="_blank" rel="noopener noreferrer">
    Speculative Decoding with EAGLE in vLLM
</a>
</li>
</ul>
<h2 id="6-deepseekmtp">6. DeepseekMTP</h2>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/deepseekmtp-structure.png" 
        alt="" 
        class="image" 
        width="90%"/>
    <div class="image-caption">
        Structure of DeepseekMTP. This figure also demonstrates the training process of draft models, which are fed with continuous tokens and corresponding masks to predict the next tokens for each position. This process is similar to the pre-training process of the larger scorer model.
    </div>
</div>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/deepseekmtp-compute-graph.png" 
        alt="" 
        class="image" 
        width="100%"/>
    <div class="image-caption">
        Compute graph of DeepseekMTP.
    </div>
</div>
<h2 id="7-discussion">7. Discussion</h2>
<h3 id="71-performance-insights-speedups-and-trade-offs">7.1. Performance Insights, Speedups, and Trade-offs</h3>
<blockquote>
<p>Ref: <a href="https://blog.vllm.ai/2024/10/17/spec-decode.html#speculative-decoding-performance-insights-speedups-and-trade-offs" target="_blank" rel="noopener noreferrer">
    [vllm] | How Speculative Decoding Boosts vLLM Performance by up to 2.8x
</a>
</p></blockquote>
<p>Speculative decoding offers significant performance benefits in <strong>low-QPS (queries per second)</strong> environments. For example, in testing on the ShareGPT dataset, vLLM demonstrated up to a 1.5x speedup in token generation when using draft model-based speculative decoding. Similarly, prompt lookup decoding has shown speedups of up to 2.8x when applied to summarization datasets, such as CNN/DailyMail.</p>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/sd-performance-low-qps.png" 
        alt="" 
        class="image" 
        width="75%"/>
    <div class="image-caption">
        Performance comparison showing spec decode delivering up to 1.5x Speedup at QPS=1 Llama3-70B on ShareGPT with 4xH100 using draft model (turboderp/Qwama-0.5B-Instruct) and up to 2.8x Speedup at QPS=1 Llama3-70B on CNN Dailymail with 4xH100 using n-grams.
    </div>
</div>
<p>However, in <strong>high-QPS environments</strong>, speculative decoding may introduce performance trade-offs. The extra compute required to propose and verify tokens can sometimes slow down the system when it is already compute-bound, as seen when the number of requests per second increases. In such cases, the overhead of speculative decoding can outweigh its benefits, leading to reduced performance.</p>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/sd-performance-high-qps.png" 
        alt="" 
        class="image" 
        width=""/>
    <div class="image-caption">
        As high QPS, we see 1.4x slowdown Llama3-70B on ShareGPT with 4xH100, 1.8x slowdown Llama3-70B on CNN Dailymail with 4xH100
    </div>
</div>
<h3 id="72-why-exactly-is-batch-expansion-inefficient">7.2. Why exactly is batch expansion inefficient?</h3>
<blockquote>
<p>Ref: <a href="https://docs.google.com/document/d/1T-JaS2T1NRfdP51qzqpyakoCXxSXTtORppiwaj5asxA/edit?tab=t.0#heading=h.71imqkdaug8g" target="_blank" rel="noopener noreferrer">
    Optimizing attention for spec decode can reduce latency / increase throughput
</a>
</p></blockquote>
<p>Looking at Llama2 architecture, each component has the following algorithmic complexity wrt speculative tokens and sequence length. The baseline is non-speculative decoding, so factors such as d_model are ignored as they are the same in either case.</p>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/llama2-sd-complexity.png" 
        alt="" 
        class="image" 
        width=""/>
    <div class="image-caption">
        
    </div>
</div>
<p>Each of these scales linearly with number of speculative tokens, except for attention, which scales by <code>num_spec_tokens * seq_len</code>. This means that for large batch sizes and/or large speculative trees and/or large sequence lengths, attention will be the computational bottleneck.</p>
<p>To optimize the attention operation, the key is that components of the attention operation are duplicated when scoring different speculative tokens given the same prefix sequence:</p>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/sd-attn-opt.png" 
        alt="" 
        class="image" 
        width=""/>
    <div class="image-caption">
        
    </div>
</div>
<p>Speaking theoretically, we can optimize attention for speculative scoring by reducing redundant <code>QK^T</code> computations + loads and <code>Softmax(...)V</code> loads:</p>
<ul>
<li>Share K loads for common tokens</li>
<li>Share K*Q compute for common tokens</li>
<li>Share V loads for common tokens</li>
</ul>
<p>We should experimentally verify this analysis: one weakness is that <code>Softmax(...)V</code> computation is still <code>O(num_spec_tokens * seq_len)</code>.</p>
<h2 id="references">References</h2>
<ol>
<li><a href="https://docs.vllm.ai/en/latest/features/spec_decode.html" target="_blank" rel="noopener noreferrer">
    [vllm] | Speculative Decoding
</a>
</li>
<li><a href="https://blog.vllm.ai/2024/10/17/spec-decode.html" target="_blank" rel="noopener noreferrer">
    [vllm] | How Speculative Decoding Boosts vLLM Performance by up to 2.8x
</a>
</li>
<li><a href="https://blog.vllm.ai/2024/10/17/spec-decode.html#how-to-use-speculative-decoding-in-vllm" target="_blank" rel="noopener noreferrer">
    [vllm] | How to Use Speculative Decoding in vLLM
</a>
.</li>
<li><a href="https://github.com/vllm-project/vllm/pull/4978" target="_blank" rel="noopener noreferrer">
    [vllm][PR] | [Speculative Decoding] Medusa Implementation with Top-1 proposer #4978
</a>
</li>
<li><a href="https://pytorch.org/blog/hitchhikers-guide-speculative-decoding" target="_blank" rel="noopener noreferrer">
    A Hitchhiker&#39;s Guide to Speculative Decoding
</a>
</li>
<li><a href="https://docs.google.com/document/d/1Z9TvqzzBPnh5WHcRwjvK2UEeFeq5zMZb5mFE8jR0HCs/edit?tab=t.0#heading=h.1fjfb0donq5a" target="_blank" rel="noopener noreferrer">
    [vllm] | What is lookahead scheduling in vLLM?
</a>
</li>
<li><a href="https://docs.google.com/document/d/1T-JaS2T1NRfdP51qzqpyakoCXxSXTtORppiwaj5asxA/edit?tab=t.0#heading=h.kk7dq05lc6q8" target="_blank" rel="noopener noreferrer">
    Optimizing attention for spec decode can reduce latency / increase throughput
</a>
</li>
<li><a href="https://github.com/vllm-project/vllm/issues/4565" target="_blank" rel="noopener noreferrer">
    [vllm][ISSUE] | [RFC]: Automate Speculative Decoding #4565
</a>
</li>
<li><a href="https://huggingface.co/blog/dynamic_speculation_lookahead" target="_blank" rel="noopener noreferrer">
    [HF] | Faster Assisted Generation with Dynamic Speculation
</a>
</li>
</ol>
]]></content:encoded></item><item><title>Create A LibTorch Project</title><link>https://jamesnulliu.github.io/blogs/create-a-libtorch-project/</link><pubDate>Mon, 23 Dec 2024 01:00:00 +0800</pubDate><guid>https://jamesnulliu.github.io/blogs/create-a-libtorch-project/</guid><description>How to create a LibTorch project.</description><content:encoded><![CDATA[<h2 id="0-introduction">0. Introduction</h2>
<p>These days I am reading <a href="https://www.elsevier.com/books/programming-massively-parallel-processors/kirk/978-0-12-811986-0" target="_blank" rel="noopener noreferrer">
    Programming Massively Parallel Processors: A Hands-on Approach, 4th Edition
</a>
, and created a <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors" target="_blank" rel="noopener noreferrer">
    project
</a>
 to store my notes as I learn.</p>
<p>One of the most important parts in the book is writing <strong>cuda kernels</strong>, so I decided to build all kernels into shared libraries and test those implementations both in C++ and Python.</p>
<p>I generated my project using <a href="https://github.com/jamesnulliu/VSC-Python-Project-Template" target="_blank" rel="noopener noreferrer">
    this template
</a>
 specifically tailored for the similar scenario, but still met some problems such as conflicts when linking libtorch and gtest 🤯.</p>
<p><strong>So the purpose of this blog is to provide a concise guide to:</strong></p>
<ol>
<li>Build a C++, CUDA and LibTorch library, test it with gtest.</li>
<li>Load the library into torch, call the operaters in Python.</li>
<li>Resolve problems when linking all the libraries.</li>
</ol>
<blockquote>
<p>⚠️<strong>WARNING</strong><br>
Find some tutorials on how to use cmake and vcpkg before reading this blog.</p></blockquote>
<h2 id="1-environment-and-quick-start">1. Environment and Quick Start</h2>
<p>Check <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/tree/c648649/README.md" target="_blank" rel="noopener noreferrer">
    README.md
</a>
 of the project repository.</p>
<h2 id="2-create-a-c-cuda-and-libtorch-project">2. Create a C++, CUDA and LibTorch Project</h2>
<p>I put all C++ codes in &ldquo;<a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/tree/96685ab/csrc" target="_blank" rel="noopener noreferrer">
    ./csrc/
</a>
&rdquo; and build them with cmake. The intermediate files should be generated in &ldquo;./build/&rdquo; and that is just about using some command-line arguments, see <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/scripts/build.sh#L42" target="_blank" rel="noopener noreferrer">
    this line
</a>
.</p>
<p>Vcpkg is used to manage the dependencies of the project. I am not going to teach you how to use vcpkg in this blog, but I will mention some pitfalls I met when using it.</p>
<blockquote>
<p>😍️ I really enjoy building C++ projects with cmake and vcpkg. Have a try if you haven&rsquo;t used them before.</p></blockquote>
<h3 id="21-how-to-link-against-libtorch">2.1. How to Link against LibTorch</h3>
<p>Since you have installed pytorch in <a href="#1-environment">
    1. Environment
</a>
, now you already have libtorch installed in your conda environment. Run this command, and you will get the cmake prefix path of libtorch:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">python -c <span class="s2">&#34;import torch;print(torch.utils.cmake_prefix_path)&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>To integrate libtorch into cmake, I create <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/cmake/utils/run-python.cmake" target="_blank" rel="noopener noreferrer">
    this file
</a>
 and <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/cmake/libraries/libtorch.cmake" target="_blank" rel="noopener noreferrer">
    this file
</a>
 to find libtorch in the current project and use them <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/CMakeLists.txt#L27" target="_blank" rel="noopener noreferrer">
    here
</a>
.</p>
<p>Now you can link your targets against libtorch simply like what I do <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/CMakeLists.txt#L19" target="_blank" rel="noopener noreferrer">
    here
</a>
.</p>
<blockquote>
<p>📝<strong>NOTE</strong><br>
When you link your target against <code>${TORCH_LIBRARIES}</code>, cuda libraries are being linked automatically, which means you don&rsquo;t have to find and link cuda using something like I write <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab6/csrc/cmake/libraries/libcuda.cmake" target="_blank" rel="noopener noreferrer">
    here
</a>
</p></blockquote>
<h3 id="22-cmake-and-vcpkg-configuration">2.2. CMake and VCPKG Configuration</h3>
<p>Currently, I am planning to use the C/C++ packages listed in <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/vcpkg.json" target="_blank" rel="noopener noreferrer">
    this file
</a>
. I load the packages with <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/CMakeLists.txt#L30-L36" target="_blank" rel="noopener noreferrer">
    these lines in &#34;./csrc/CMakeLists.txt&#34;
</a>
 . Then I link those packages to my targets <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/CMakeLists.txt#L20-L21" target="_blank" rel="noopener noreferrer">
    here
</a>
 and <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/test/CMakeLists.txt#L11-L13" target="_blank" rel="noopener noreferrer">
    here
</a>
.</p>
<blockquote>
<p>📝<strong>NOTE</strong><br>
<code>libtorch &lt; 2.6</code> is compiled with <code>_GLIBCXX_USE_CXX11_ABI=0</code> to use legacy ABI before C++11, which conflicts with the packages managed by vcpkg in default. Consequentially, you have to create a custom vcpkg triplet to control the behaviors when vcpkg actually build the packages. The triplet file is <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/cmake/vcpkg-triplets/x64-linux.cmake" target="_blank" rel="noopener noreferrer">
    here
</a>
 and is enabled by <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/scripts/build.sh#L47-L48" target="_blank" rel="noopener noreferrer">
    these lines
</a>
 when building the C++ part.</p></blockquote>
<p>I also set <code>CMAKE_CXX_SCAN_FOR_MODULES</code> to <code>OFF</code> on <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/cmake/compilers/cxx-compiler-configs.cmake#L15" target="_blank" rel="noopener noreferrer">
    this line
</a>
 because some compile errors occurs. This is a temporary solution but I am not planning to use modules from C++20 in this project, so just ignoring it.</p>
<h3 id="23-write-and-register-custom-torch-operators">2.3. Write and Register Custom Torch Operators</h3>
<p>In order to register a custom torch <strong>operator</strong>, basically what you need to do next is to write a <strong>function</strong> that usually takes several <code>torch::Tensor</code> as input and returns a <code>torch::Tensor</code> as output, and then register this function to torch.</p>
<p>For example, I implement <code>pmpp::ops::cpu::launchVecAdd</code> in <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/ops/vecAdd/op.cpp" target="_blank" rel="noopener noreferrer">
    this cpp file
</a>
 and <code>pmpp::ops::cuda::launchVecAdd</code> in <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/ops/vecAdd/op.cu" target="_blank" rel="noopener noreferrer">
    this cu file
</a>
 and provide the corresponding torch implentations <code>pmpp::ops::cpu::vectorAddImpl</code> and <code>pmpp::ops::cuda::vectorAddImpl</code> in <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/ops/vecAdd/torch_impl.cpp" target="_blank" rel="noopener noreferrer">
    this file
</a>
.</p>
<blockquote>
<p>🤔 I didn&rsquo;t add any of those function declarations in hpp files under &ldquo;./include&rdquo; because I don&rsquo;t think they should be exposed to the users of the library. For the testing part, I will get and test the functions using <code>torch::Dispatcher</code> which aligns with the operaters invoked in python.</p></blockquote>
<p>To register these implementations as an operater into pytorch, see <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/ops/torch_bind.cpp#L10" target="_blank" rel="noopener noreferrer">
    this line
</a>
, <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/ops/torch_bind.cpp#L22" target="_blank" rel="noopener noreferrer">
    this line
</a>
, and <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/ops/torch_bind.cpp#L32" target="_blank" rel="noopener noreferrer">
    this line
</a>
, where I:</p>
<ol>
<li>Define a python function <code>vector_add</code> with signature: <code>vector_add(Tensor a, Tensor b) -&gt; Tensor</code>.</li>
<li>Register the CPU implementation of the function.</li>
<li>Register the CUDA implementation of the function.</li>
</ol>
<p>Now <code>vector_add</code> is a custom torch operator which can be called in both C++ and Python. All you need to do is to build these codes into a shared library like what I did <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/CMakeLists.txt#L7" target="_blank" rel="noopener noreferrer">
    here in cmake
</a>
.</p>
<h3 id="24-test-the-custom-torch-operators-in-c">2.4. Test the Custom Torch Operators in C++</h3>
<p>As long as a custom torch operator is registered, normally one or multiple shared libraries will be generated. For C++ users, you should link your executable target against libtorch and the generated shared libraries so that those registered operators can be called.</p>
<p>Since I have linked <code>libPmppTorchOps</code> against libtorch as <code>PUBLIC</code> in <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/CMakeLists.txt#L18" target="_blank" rel="noopener noreferrer">
    this line
</a>
, the test target will link against libtorch automatically as long as it links against <code>libPmppTorchOps</code>, see <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/test/CMakeLists.txt#L10" target="_blank" rel="noopener noreferrer">
    this line
</a>
.</p>
<blockquote>
<p>📝<strong>NOTE</strong><br>
You may be confused about why <code>-Wl,--no-as-needed</code> is added before <code>${PROJECT_NAMESPACE}pmpp-torch-ops</code>. This is because the shared libraries are not directly used in the test target (an operator is register in the library but not called directly in the executable), and the linker will not link against them by default. This flag will force the linker to link against the shared libraries even if they are not directly used.</p></blockquote>
<p>The registered operators can be dispatched in a not-so-intuitional way 🤣 based on the official documentation, see <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/test/OpTest/vecAdd.cpp#L14-L17" target="_blank" rel="noopener noreferrer">
    here
</a>
.</p>
<p>Now the only thing is to test the operators in C++ using gtest, but this is not the focus of this blog. So let&rsquo;s move on to the next part.</p>
<h2 id="3-create-and-package-a-python-project">3. Create and Package a Python Project</h2>
<h3 id="31-pyprojecttoml-and-setuppy">3.1. <code>pyproject.toml</code> and <code>setup.py</code></h3>
<p>In modern python, pyproject.toml is a de-facto standard configuration file for packaging, and in this project, setuptools is used as the build backend because I believe it is the most popular one and is easy to cooperate with cmake.</p>
<p>Particularly, &ldquo;<a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/pyproject.toml" target="_blank" rel="noopener noreferrer">
    ./pyproject.toml
</a>
&rdquo; and &ldquo;<a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/setup.py" target="_blank" rel="noopener noreferrer">
    ./setup.py
</a>
&rdquo; defines what will happen when you run <code>pip install .</code> in the root directory of the project. I created <code>CMakeExtention</code> and <code>CMakeBuild</code> (<a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/setup.py#L23-L68" target="_blank" rel="noopener noreferrer">
    here
</a>
) and pass them to <code>setup</code> function (<a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/setup.py#L92-L105" target="_blank" rel="noopener noreferrer">
    here
</a>
) so that the C++ library <code>libPmppTorchOps</code> (under &ldquo;./csrc/&rdquo;) will be built and installed before installing the python package.</p>
<p>You can easily understand what I did by reading the source code of these two files, and there is one more thing I want to mention.</p>
<p>Based on <a href="/blogs/create-a-libtorch-project/#2-create-a-c-cuda-and-libtorch-project">2. Create a C++, CUDA and LibTorch Project</a>, you should find that the generated shared library is under <code>./build/lib</code> ending with <code>.so</code> on linux or <code>.dll</code> on windows. Additionally, I added an install procedure <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/setup.py#L62-L68" target="_blank" rel="noopener noreferrer">
    here
</a>
 which will copy the shared libraries to &ldquo;./src/pmpp/_torch_ops&rdquo;.</p>
<blockquote>
<p>Note that &ldquo;<a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/tree/96685ab/src/pmpp" target="_blank" rel="noopener noreferrer">
    ./src/pmpp
</a>
&rdquo; is already an existing directory being the root of the actual python package, and &ldquo;./src/pmpp/_torch_ops&rdquo; will be created automatically while installing the shared libraries.</p></blockquote>
<p>The problem is, when packaging the python project, only the directory containing &ldquo;__init__.py&rdquo; will be considered as a package (or module), and I don&rsquo;t want to add this file to &ldquo;./src/pmpp/_torch_ops&rdquo; due to my mysophobia 😷. Therefore, I used <code>find_namespace_packages</code> instead of <code>find_packages</code> and specified <code>package_data</code> to include the shared libraries <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/setup.py#L106-L108" target="_blank" rel="noopener noreferrer">
    here
</a>
.</p>
<h3 id="32-install-the-python-package">3.2. Install the Python Package</h3>
<p>If you are planning to build your libraries with dependencies listed <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/pyproject.toml#L26-L31" target="_blank" rel="noopener noreferrer">
    here
</a>
 while installing the python project, I don&rsquo;t really suggest installing it in an isolated python environment (which is the default behavior of setuptools). All packages listed <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/pyproject.toml#L2" target="_blank" rel="noopener noreferrer">
    here
</a>
 have to be re-installed and in our case you need to at least append <code>torch</code> to that list.</p>
<p>Alternatively, try this command, which will directly use the torch installed in current conda environment:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">pip install --no-build-isolation -v .
</span></span></code></pre></div><h3 id="33-test-the-custom-torch-operators-in-python">3.3. Test the Custom Torch Operators in Python</h3>
<p>As long as you have the shared libraries built in <a href="#2-create-a-c-cuda-and-libtorch-project">
    2. Create a C&#43;&#43;, CUDA and LibTorch Project
</a>
, all you need to do is to use <code>torch.ops.load_library</code> to load the shared libraries and call the registered operators.</p>
<p>I write this process into &ldquo;<a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/src/pmpp/__init__.py" target="_blank" rel="noopener noreferrer">
    src/pmpp/__init__.py
</a>
&rdquo;, so the time you import <code>pmpp</code> in python, your custom torch operators will be ready to use. See <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/test/test.py" target="_blank" rel="noopener noreferrer">
    this file
</a>
 for an example of testing the operators.</p>
]]></content:encoded></item><item><title>Dive into Paged Attention</title><link>https://jamesnulliu.github.io/blogs/dive-into-paged-attention/</link><pubDate>Mon, 07 Oct 2024 12:00:00 +0800</pubDate><guid>https://jamesnulliu.github.io/blogs/dive-into-paged-attention/</guid><description>Dive into the paged attention mechanism of vLLM.</description><content:encoded><![CDATA[<h2 id="1-why-attentions--only-depends-on">1. Why Attention&rsquo;s $O_i$ only depends on $Q_i$</h2>
<p>The Attention formula is:</p>
$$
O=Attention(Q,K,V)=softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$<p>Assume $Q=\begin{bmatrix}Q_0\\Q_1\end{bmatrix}$, $K=\begin{bmatrix}K_0\\K_1\end{bmatrix}$</p>
<p>Then:</p>
$$
O=softmax(\frac{\begin{bmatrix}Q_0K_0^T&Q_0K_1^T\\Q_1K_0^T&Q_1K_1^T\end{bmatrix}}{\sqrt{d_k}})V
$$<p>Let:</p>
$$
A=\begin{bmatrix}A_0\\A_1\end{bmatrix}=\begin{bmatrix}Q_0K_0^T&Q_0K_1^T\\Q_1K_0^T&Q_1K_1^T\end{bmatrix},f(x)=\frac{softmax(x)}{\sqrt{d_k}}
$$<p>At this point, $A_1$ only depends on $Q_1$ and is independent of $Q_0$, so:</p>
$$
\begin{bmatrix}O_0\\O_1\end{bmatrix}=O=\begin{bmatrix}f(A_0)\\f(A_1)\end{bmatrix}V=\begin{bmatrix}f(A_0)V\\f(A_1)V\end{bmatrix}
$$<p>Therefore, $O_i$ only depends on $A_i$, and according to the definition of $A$, $A_i$ only depends on $Q_i$, meaning:</p>
<p>The $i$-th output of the Attention matrix only depends on the $i$-th $Q$ and is independent of previous $Q$s.</p>
<p><strong>Summary</strong>:</p>
<ul>
<li>When predicting the next token, we only need to calculate the corresponding <code>Q_new</code> for the new token and perform attention calculation with the previously cached <code>K_cache</code> and <code>V_cache</code>.</li>
<li>The new <code>K_new</code> and <code>V_new</code> will be added to the cache to provide the foundation for the next token generation.</li>
<li>This process avoids repeated calculations for all historical tokens, greatly improving efficiency.</li>
</ul>
<h2 id="2-kv-cache-incremental-process">2. KV Cache Incremental Process</h2>
<p>Example code:</p>
<p>&ldquo;<a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/cf690614d004aa647aefccb8db3eac83255cb99e/src/pmpp/models/attention.py">Learning-Programming-Massively-Parallel-Processors/src/pmpp/models/attention.py</a>&rdquo;</p>
<h3 id="21-prefilling-initial-input-complete-sequence-calculation">2.1. Prefilling: Initial Input (Complete Sequence) Calculation</h3>
<ul>
<li>For the initial input sequence <code>(seq_len, vocab_size)</code>, we obtain <code>Q</code>, <code>K</code>, and <code>V</code> through linear transformations, all with shape <code>(seq_len, embed_dim)</code> (<em>see <a href="">this</a></em>).</li>
<li>Using <code>Q</code> and <code>K</code> to calculate attention scores through dot product, then combining with <code>V</code> to compute the output <code>(seq_len, embed_dim)</code> (<em>see <a href="">this</a></em>), this is the first complete calculation for the initial sequence.</li>
</ul>
<h3 id="22-decoding-incremental-calculation-when-predicting-next-token">2.2. Decoding: Incremental Calculation When Predicting Next Token:</h3>
<p>When predicting the next token, there&rsquo;s no need to perform complete <code>Q</code>, <code>K</code>, <code>V</code> calculations for the entire sequence. Instead, only an incremental calculation for the newly generated token is required. The process is as follows:</p>
<ol>
<li><strong>Input New Token</strong>: Take the generated token from last round as input sequence, obtain <code>Q_new</code>, <code>K_new</code> and <code>V_new</code> through linear transformation.</li>
<li><strong>Update KV Cache</strong>: <code>K_new</code> and <code>V_new</code> are added to the end of <code>K_cache</code> and <code>V_cache</code>, making them a pair of <code>(kv_len + 1, embed_dim)</code> vectors.</li>
<li><strong>Attention Calculation with Updated <code>K_cache</code> and <code>V_cache</code></strong>: Use <code>Q_new</code> to perform attention calculation with updated <code>K_cache</code> and <code>V_cache</code>. <code>Q_new</code> can directly perform dot product with <code>K_cache</code> to get attention scores, then combine with <code>V_cache</code> to get new output.</li>
<li><strong>Output</strong>: The output after attention calculation has shape <code>(1, embed_dim)</code>, which is the newly generated token.</li>
</ol>
<h2 id="3-paged-attention-in-vllm">3. Paged Attention in vllm</h2>
<h3 id="31-motivation-memory-wastes">3.1. Motivation: Memory Wastes</h3>
<p><img alt="memory-wastes.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/memory-wastes.png"></p>
<p>The above figure shows possible memory waste scenarios. The main issue is that we don&rsquo;t know where the EOS (end of sequence) token is. Random memory allocation may lead to significant memory fragmentation, resulting in reduced throughput.</p>
<h3 id="32-solution-managing-caches-with-pages">3.2. Solution: Managing Caches with Pages</h3>
<p><img alt="paged-attention-animation.webp" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/paged-attention-animation.webp"></p>
<p>The above figure demonstrates how vLLM manages memory using Paged Attention.</p>
<p>In simple terms, before inference begins, vLLM allocates two long Tensors (<code>k_cache</code> and <code>v_cache</code>) for each Decoder Layer, dividing these Tensors into continuous equal-length PA blocks (each row in the figure represents one PA Block). Each PA Block can store K or V cache for <code>BLOCK_SIZE</code> tokens (each token&rsquo;s shape can be recognized as <code>(num_heads, head_size)</code>).</p>
<p>Therefore, the shapes of <code>k_cache</code> and <code>v_cache</code> can be recognized as <code>(num_blocks, block_size, num_heads, head_size)</code>.</p>
<p>For a continuous sequence, PA blocks are allocated before the prefilling stage, and during inference:</p>
<ul>
<li>When computing prompt attention, the input K and V are first stored in <code>k_cache</code> and <code>v_cache</code> according to PA blocks; then attention is calculated using the entire QKV.</li>
<li>When computing new tokens, Q and the block table are used to calculate attention during the decode phase; at this point, the memory access is to the PA blocks in <code>k_cache</code> and <code>v_cache</code>.</li>
</ul>
<h2 id="5-paged-attention-kernel-in-details">5. Paged Attention Kernel in Details</h2>
<blockquote>
<p>References:</p>
<ul>
<li><a href="https://docs.vllm.ai/en/latest/dev/kernel/paged_attention.html">vLLM Paged Attention</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/673284781">vLLM皇冠上的明珠：深入浅出理解PagedAttention CUDA实现</a></li>
</ul></blockquote>
<p>The general structure of the Paged Attention kernel is as follows:</p>
<p><img alt="pa-cal.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-cal.png"></p>
<h3 id="51-输入输出输出分析和参数说明">5.1. 输入输出输出分析和参数说明</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Grid: (num_heads, num_seqs, 1).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">template</span><span class="o">&lt;</span>
</span></span><span class="line"><span class="cl"><span class="k">typename</span> <span class="n">scalar_t</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">HEAD_SIZE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">NUM_THREADS</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">PARTITION_SIZE</span> <span class="o">=</span> <span class="mi">0</span><span class="o">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="n">__device__</span> <span class="kt">void</span> <span class="n">paged_attention_kernel</span><span class="p">(</span>
</span></span><span class="line"><span class="cl"><span class="p">...</span> <span class="c1">// Other side args.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="n">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">out</span><span class="p">,</span>       <span class="c1">// [num_seqs, num_heads, max_num_partitions, head_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="n">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">q</span><span class="p">,</span>         <span class="c1">// [num_seqs, num_heads, head_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="n">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">k_cache</span><span class="p">,</span>   <span class="c1">// [num_blocks, num_kv_heads, head_size/x, block_size, x]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="n">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">v_cache</span><span class="p">,</span>   <span class="c1">// [num_blocks, num_kv_heads, head_size, block_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">...</span> <span class="c1">// Other side args.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">)</span>
</span></span></code></pre></div><p>模板参数说明:</p>
<ul>
<li><code>scalar_t</code> 元素类型 (实际代码中还有 <code>cache_t</code> 表示 KV cache 的元素类型).</li>
<li><code>HEAD_SIZE</code> 每个 head 中元素数量.</li>
<li><code>BLOCK_SIZE</code> 每个 PA block 中的 token 数量.
<blockquote>
<ol>
<li>KV cache 被存储在不同 PA blocks. 每个 PA block 存储一个 head 中 <code>BLOCK_SIZE</code> 个 token.<br>
例如, 若 <code>BLOCK_SIZE=16</code>, <code>HEAD_SIZE=128</code>, 则一个  PA block 能存储一个 head 的 <code>16 * 128 = 2048</code> 个元素.</li>
<li>每个 PA block 可能只包含一部分的 context tokens.</li>
<li>从 page 角度看, KV cache 是若干个 page 的集合;</li>
</ol></blockquote>
</li>
<li><code>NUM_THREADS</code> 每个 CUDA thread block 中 thread 的数量.</li>
<li><code>PARTITION_SIZE</code> 参与 TP 的 GPU 数量, 默认 0 表示单卡. (以下都以单卡为例说明)</li>
</ul>
<p>额外的一些参数:</p>
<ul>
<li><code>num_seqs</code>: 本次推理请求 sequence 数目.
<blockquote>
<p>由于这个 kernel 只处理 decode 阶段单 query attention, 所以实际上每个 sequence 只有一个 query token.</p></blockquote>
</li>
<li><code>num_heads</code>: Q 的 head 数目</li>
<li><code>num_kv_heads</code>: KV 的 head 数目, 对于 MHA 其值和 <code>num_heads</code> 相同; 如果是 GQA, MQA 则 <code>num_kv_heads</code> 小于 <code>num_head</code>.</li>
<li><code>head_size</code>: 即 <code>HEAD_SIZE</code></li>
<li><code>k_cache: (num_blocks, num_kv_heads, head_size/x, block_size, x)</code>, 其中 <code>x</code> 表示 <code>THREAD_GROUP_SIZE * VEC_SIZE</code> 的大小 (后面会细说).</li>
</ul>
<p>下面结合 GPU architecture 初步分析一下参数.</p>
<p><img alt="gpu-archi.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/gpu-archi.png"></p>
<p>🧐 <strong>为什么要分 thread group?</strong></p>
<ul>
<li>因为当一个 cuda block 要取的数据比较少的时候 (计算 QK), 一个 thread group 分别一次取 Q 和 K 中 16B; 当一个 cuda block 要取的数据比较多的时候 (计算 LV), 一个 thread 取 16B.</li>
</ul>
<h3 id="52shared-memory-q_vecs-的写入">5.2.Shared Memory: <code>q_vecs</code> 的写入</h3>
<p>从 kernel 中的第一个申请的 shared memory 开始说.</p>
<blockquote>
<p>关于 shared memeory:</p>
<ol>
<li>在 kernel 中申请的 shared memory 被当前 cuda block 中的所有 thread 共享.</li>
<li>shared memory 的作用是为了减少 global memory 的访问次数，提高访存效率.</li>
</ol></blockquote>
<p>以下代码申请了一块 shared memroy 被整个 CUDA Block 中所有 kernel 共享:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__shared__</span> <span class="n">Q_vec</span> <span class="n">q_vecs</span><span class="p">[</span><span class="n">THREAD_GROUP_SIZE</span><span class="p">][</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span></code></pre></div><p>首先, <code>q_vecs</code> 覆盖了 Q 中 <code>head_size</code> 个元素 - 这也是一个 cuda block 需要处理的数据量.</p>
<p>接着再说两个维度的参数的意思:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_ELEMS_PER_THREAD</span> <span class="o">=</span> <span class="n">HEAD_SIZE</span> <span class="o">/</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_VECS_PER_THREAD</span> <span class="o">=</span> <span class="n">NUM_ELEMS_PER_THREAD</span> <span class="o">/</span> <span class="n">VEC_SIZE</span><span class="p">;</span>
</span></span></code></pre></div><ul>
<li><code>THREAD_GROUP_SIZE</code>: 每个 thread group 中的 thread 数量. 注意, 一个 cuda block 中有 <code>NUM_THREADS</code> 个 thread, <code>NUM_THREAD_GROUPS</code> 个 thread group. <code>THREAD_GROUP_SIZE = MAX(WARP_SIZE/BLOCK_SIZE, 1)</code>.</li>
<li><code>NUM_VECS_PER_THREAD</code>: <code>HEAD_SIZE</code> 能被分成多少个 16B. (这个变量这么命名的理由是后面读取 K 的时候每个 thread 会往自己的寄存器内读 <code>NUM_VECS_PER_THREAD</code> 个 k_vec.)</li>
</ul>
<blockquote>
<p>证明: <code>q_vecs</code> 覆盖 Q 的一个 head, 并且 <code>NUM_VECS_PER_THREAD</code> 表示 Q 的一个 head 被分成多少个 16B.<br>
=&gt; <code>THREAD_GROUP_SIZE</code> * <code>VEC_SIZE</code> = 16B / <code>sizeof(scalar_t)</code>;<br>
=&gt; <code>NUM_VECS_PER_THREAD</code> * 16B / <code>sizeof(scalar_t)</code> = <code>HEAD_SIZE</code>;</p></blockquote>
<p>然后看 load Q 的代码, 建议结合下面的图一起看:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl">  <span class="c1">// Load Q to shmem
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_group_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREAD_GROUPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">vec_idx</span> <span class="o">=</span> <span class="n">thread_group_offset</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">q_vecs</span><span class="p">[</span><span class="n">thread_group_offset</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">        <span class="o">*</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="n">Q_vec</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">q_ptr</span> <span class="o">+</span> <span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><ul>
<li><code>thread_group_idx</code> 表示当前 thread 属于当前 cuda block 中第几个 thread group.</li>
<li><code>thread_group_offset</code> 表示当前 thread 在当前 thread group 中是第几个 thread.</li>
</ul>
<p><img alt="pa-load-q.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-load-q.png"></p>
<p>上图展示了循环具体是怎么跑的.</p>
<ul>
<li>一个紫色箭头表示一个 thread group.</li>
<li><code>NUM_VECS_PER_THREAD</code> 表示 <code>HEAD_SIZE</code> 能被分成多少个 16B.</li>
<li>实际读取 Q 的内存时, 所有 thread group 从 Q 的起始位置紧密排列, 根据图上看的话一共有 <code>NUM_THREAD_GROUPS</code> 个紫色箭头.</li>
<li>所有 thread group 读取一次 Q 并存入 <code>q_vecs</code> 对应循环中的一次迭代; 因此下次迭代 thread group 需要向后偏移 <code>NUM_THREAD_GROUPS</code> 个位置 (例如 <code>i</code> 从 1 变为 7).</li>
<li>此外, 读一次 16B 对应一个 thread 来说自然也是取一个 VEC.</li>
<li>对应到 kernel 编写, 还需要计算当前 thread 具体读取哪个 vec; 因此得到 <code>vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE</code>.</li>
</ul>
<blockquote>
<p>🤔 这里会不会有 bank conflict?</p></blockquote>
<p>总之现在我们把 <code>(1, head_size)</code> 大小的元素读到了 cuda block 共享的 shared memory <code>q_vecs</code> 中.</p>
<h3 id="53-读取-k-cache-并计算-qk">5.3. 读取 K Cache 并计算 QK</h3>
<p>现在从 cuda block 的角度看, 当前 block 已经获得了自己要算的 Q 中的一个 head (形状为 <code>(1, head_size)</code>), 接下来就是计算 Q 和 K 的点积.</p>
<p>点积过程是把当前 block 拥有的 Q head 和整个 K Cache (迭代地) 进行点积运算. 参考下图:</p>
<p><img alt="pa-cal-kq-01.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-cal-kq-01.png"></p>
<p>QK 乘积实际上被暂存在 <code>logits</code> (也是一块 shared memory) 中, 之后会被用来计算 softmax.</p>
<p>😇 看下循环的具体代码吧:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Loop 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Physical block calculation ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// Loop 2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Offset calculation ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">K_vec</span> <span class="n">k_vecs</span><span class="p">[</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Loop 3
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Load K to `k_vecs` ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="kt">float</span> <span class="n">qk</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">Qk_dot</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span> <span class="n">THREAD_GROUP_SIZE</span><span class="o">&gt;::</span><span class="n">dot</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                        <span class="n">q_vecs</span><span class="p">[</span><span class="n">thread_group_offset</span><span class="p">],</span> <span class="n">k_vecs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Add the ALiBi bias if slopes are given.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">qk</span> <span class="o">+=</span> <span class="p">(</span><span class="n">alibi_slope</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">?</span> <span class="n">alibi_slope</span> <span class="o">*</span> <span class="p">(</span><span class="n">token_idx</span> <span class="o">-</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">thread_group_offset</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Store the partial reductions to shared memory.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// Mask
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// Update the max value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>先说第一个循环, 其中比较重要的几个参数定义如下:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// [start_block_idx, end_block_idx) is the range of blocks to process.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">int</span> <span class="n">start_block_idx</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">    <span class="n">USE_PARTITIONING</span> <span class="o">?</span> <span class="n">partition_idx</span> <span class="o">*</span> <span class="nl">num_blocks_per_partition</span> <span class="p">:</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="c1">// If not using partitioning, `end_block_idx` should be equal to `num_seq_blocks`.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">int</span> <span class="n">end_block_idx</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">    <span class="n">MIN</span><span class="p">(</span><span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">num_blocks_per_partition</span><span class="p">,</span> <span class="n">num_seq_blocks</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="c1">// Number of blocks to process.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">int</span> <span class="n">num_blocks</span> <span class="o">=</span> <span class="n">end_block_idx</span> <span class="o">-</span> <span class="n">start_block_idx</span><span class="p">;</span>
</span></span></code></pre></div><p>用文字描述就是:</p>
<ul>
<li><code>blk_idx</code> 表示当前 thread 所在 warp 需要处理的 PA block 的在 <code>block_table</code> 中索引 (逻辑上的索引).</li>
<li><code>start_block_idx</code> 和 <code>end_block_idx</code> 表示当前 cuda block 需要处理的 block 范围.</li>
<li><code>num_blocks</code> 表示当前 cuda block 需要处理的 block 数量.</li>
<li><code>NUM_WARPS</code> 表示当前 cuda block 中 warp 的数量. 一个 warp 包含 32 个 thread.</li>
<li><code>warp_idx</code> 表示当前 warp 在当前 cuda block 中的索引.</li>
</ul>
<p>说人话就是每个 warp 处理一个 PA block, 一开始 cuda block 中的所有 warp 紧密地指向最前面的 <code>NUM_WARPS</code> 个 PA block, 每次循环所有 warp 向后偏移 <code>NUM_WARPS</code> 个 PA block 的长度. 参考下图:</p>
<p><img alt="pa-cal-kq-02.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-cal-kq-02.png"></p>
<blockquote>
<p>🔔 这里再回顾一下, 一个 PA block 里存放了 <code>BLOCK_SIZE</code> 个 token 的 K 或 V cache.</p></blockquote>
<p>所以说这个循环和上面读取 Q 的循环一个尿性🤮, 不过是以 warp 的粒度处理数据;</p>
<p>进入了第一个循环内部, 第一步当然是计算当前 thread 对应的 warp 应该计算哪个 PA block (物理上的索引), 因此得到了 <code>physical_block_number</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">const</span> <span class="kt">int64_t</span> <span class="n">physical_block_number</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">    <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">block_table</span><span class="p">[</span><span class="n">block_idx</span><span class="p">]);</span>
</span></span></code></pre></div><hr>
<p>然后解释第二个循环, 第二个循环的整体目标就是让当前 warp 计算好自己负责的 PA block 中 <code>BLOCK_SIZE</code> 个 token 的 QK 乘积.</p>
<p>先看一下 <code>i</code> 的上界:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">    <span class="n">DIVIDE_ROUND_UP</span><span class="p">(</span><span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">WARP_SIZE</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="c1">// Loop 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Loop 2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></div><p>从 kernel 角度看, 每个 thread 需要辅助当前 warp 计算自己负责的一整个 PA block (包含 <code>BLOCK_SIZE</code> 个 token), 而我们把这个过程拆分为 Loop 2 中的 <code>NUM_TOKEN_PER_THREAD_GROUP</code> (也就是 <code>ceil(BLOCK_SIZE / 32)</code>) 次循环;</p>
<p>说人话就是<strong>一个 thread group 对应一个 token 中的一个 head</strong>, 如果 BLOCK SIZE 太大了后面每个 thread 向后偏移 <code>i * WARP_SIZE</code> 个 token 继续狠狠算🤣.</p>
<p>也因此第二个循环内部一上来先计算了几个偏移量, 并且申请了 thread 内部私有的 <code>k_vecs</code> 数组:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">const</span> <span class="kt">int</span> <span class="n">physical_block_offset</span> <span class="o">=</span> <span class="p">(</span><span class="n">thread_group_idx</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">WARP_SIZE</span><span class="p">)</span> <span class="o">%</span> <span class="n">BLOCK_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">const</span> <span class="kt">int</span> <span class="n">token_idx</span> <span class="o">=</span> <span class="n">block_idx</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">physical_block_offset</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">K_vec</span> <span class="n">k_vecs</span><span class="p">[</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span></code></pre></div><ul>
<li><code>thread_group_idx</code> 表示当前 thread group 在整个 cuda block 中的索引.</li>
<li>☢️ 一个 thread group 在一次循环中负责 fetch 一个 PA block 中 K cache 的一个 token 中<strong>自己负责的 head</strong>.</li>
<li>☢️ 一个 thread group 负责计算一个 qk 值; 这个值显然是由一个 Q head 和一个 K head 点积得到的.</li>
<li><code>physical_block_offset</code> 表示当前要算的 token 在当前 PA block 中的偏移量 (注意和前面的 <code>physical_block_number</code> 区分).</li>
<li>加 <code>i * WARP_SIZE</code> 的原因是如果 <code>BLOCK_SIZE</code> 大于 32, 那么一个 warp 要多次循环才能处理完一个 PA block 中的所有 token, 对应 <code>thread_group_idx</code> 需要做偏移.</li>
<li><code>token_idx</code> 表示当前要算的 token 在整个 seq 的 KV cache 中的索引.</li>
<li><code>k_vecs</code> 中能存放 <code>NUM_VECS_PER_THREAD</code> 个 VEC, 而一整个 thread group 中所有的 thread 的 <code>k_vecs</code> 合起来才能组成一个 K 的 head (推导参考上面 Q 的 😇). 这就是为什么后面算 QK 的时候要 reduce.</li>
</ul>
<p>🤔 <strong>看到这里读者可能有一个问题: 一个 token 的 K cache 应该对应多个 head, 为什么上面说一个 thread group 只负责一个 head?</strong><br>
答: 因为实际计算的时候, 一个 cuda block 只负责计算一个 head, 对应到 K Cache 乃至后面 V Cache 的位置也是一样的.</p>
<blockquote>
<p>这里额外说一下, 读 K 的 head 的一个目标应该是在尽量少的 register 中装下一个 head 的所有元素, 这样后续和 shared memory 中的 Q 做点乘并规约的速度更快. 假设一个 head 有 128 个 float16, 则占用 256B, 而 A100 中一个 thread 最多能有 255 个 32-bit register (也就是 1020B), 此时可以认为一个 thread 能装下一个 head 的所有元素.<br>
但是由于目前 PA kernel 在 <code>BLOCK_SIZE</code> 为 16 的情况下 <code>THREAD_GROUP_SIZE</code> 等于 2, 因此一个 thread 只会装一个 head 的一半元素, 这样可能会导致 register 的使用率不高.</p></blockquote>
<hr>
<p>接着进入第三个循环, 目的是让 thread group 从 K cache 中读一个 head, 并存入 <code>k_vecs</code> 中:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// x == THREAD_GROUP_SIZE * VEC_SIZE
</span></span></span><span class="line"><span class="cl"><span class="c1">// Each thread group fetches x elements from the key at a time.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">16</span> <span class="o">/</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">cache_t</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="c1">//...
</span></span></span><span class="line"><span class="cl"><span class="c1">// Loop 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Loop 2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">K_vec</span> <span class="n">k_vecs</span><span class="p">[</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Loop 3
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">const</span> <span class="n">cache_t</span><span class="o">*</span> <span class="n">k_ptr</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">                <span class="n">k_cache</span> <span class="o">+</span> <span class="n">physical_block_number</span> <span class="o">*</span> <span class="n">kv_block_stride</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">                <span class="n">kv_head_idx</span> <span class="o">*</span> <span class="n">kv_head_stride</span> <span class="o">+</span> <span class="n">physical_block_offset</span> <span class="o">*</span> <span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="k">const</span> <span class="kt">int</span> <span class="n">vec_idx</span> <span class="o">=</span> <span class="n">thread_group_offset</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="k">const</span> <span class="kt">int</span> <span class="n">offset1</span> <span class="o">=</span> <span class="p">(</span><span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">)</span> <span class="o">/</span> <span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="k">const</span> <span class="kt">int</span> <span class="n">offset2</span> <span class="o">=</span> <span class="p">(</span><span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">)</span> <span class="o">%</span> <span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// if Fp8KVCacheDataType::kAuto
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">k_vecs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="o">*</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="n">K_vec</span><span class="o">*&gt;</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">              <span class="n">k_ptr</span> <span class="o">+</span> <span class="n">offset1</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">offset2</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></div><p>老规矩, 先看 <code>j</code>, 本质就是从 0 迭代到 <code>NUM_VECS_PER_THREAD</code>, 每次迭代当前 thread 读取一个 VEC 存入 <code>k_vecs</code> 中.</p>
<blockquote>
<p>🔔 回顾:</p>
<ol>
<li><code>NUM_VECS_PER_THREAD</code> 表示一个 head 被分成多少个 16B.</li>
<li><code>k_cache</code> 的 shape 为 <code>(num_blocks, num_kv_heads, head_size/x, block_size, x)</code>.</li>
</ol></blockquote>
<p>其中的 <code>x</code> 表示一个 thread group 需要读取的元素数量 (<code>VEC_SIZE</code> * <code>THREAD_GROUP_SIZE</code>); 因此作者将 K Cache 的 layout 的最后一维设置为 <code>x</code> 其实也是方便后续 thread group 对 K cache 的读取.</p>
<p>下图具体展示了寻址的过程:</p>
<p><img alt="pa-cal-kq-03.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-cal-kq-03.png"></p>
<p>其中:</p>
<ul>
<li>在 MHSA 中, <code>num_kv_heads</code> 等于 <code>num_heads</code>; 而在 GQA, MQA 中, <code>num_kv_heads</code> 小于 <code>num_heads</code>.</li>
<li>(1) 负责找到当前 thread 属于的 warp 要处理哪个 PA block.</li>
<li>(2) 负责找到当前 thread 要计算的 head 在 K cache 中的位置. 这个 head 的索引和 Q 中 head 的索引在 MHSA 中相同.</li>
<li>(3) 负责找到当前 thread group 要计算的 token 在当前 PA block 中的位置.</li>
<li>(5) 负责找到当前 thread 在需要读取的 head (蓝色长方体) 中 x 的偏移, 通过 <code>j</code> 进行迭代读取. <strong>每次循环 thread group 中的所有 thread 取一个 x.</strong></li>
<li>(6) 负责找到当前 thread 在 thread gruop 中读取的 x 中 VEC 的偏移; thread 一次读取一个 VEC.</li>
</ul>
<p>🤔 <strong>为什么 (5) 在实际寻址时需要 <code>* BLOCK_SIZE * x</code> ?</strong><br>
答: 这是根据 <code>k_cache</code> 的 layout 得到的 stride. 同理 (3) <code>* x</code> 也是 stride.</p>
<p>第 3 个循环结束时当前 warp 负责的每个 token 中需要的 K cache head 已经全被加载入 thread 本地的 <code>k_vecs</code> 中了.</p>
<p>由于一个 thread group 的 <code>k_vecs</code> 才能真正组成一个 head, 在退回第二个循环进行 QK dot 的时候, 需要做个 reduction, 具体的范围就是 <code>THREAD_GROUP_SIZE</code> 个 thread:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Loop 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Loop 2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">K_vec</span> <span class="n">k_vecs</span><span class="p">[</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Loop 3
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="kt">float</span> <span class="n">qk</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">Qk_dot</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span> <span class="n">THREAD_GROUP_SIZE</span><span class="o">&gt;::</span><span class="n">dot</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                             <span class="n">q_vecs</span><span class="p">[</span><span class="n">thread_group_offset</span><span class="p">],</span> <span class="n">k_vecs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></div><p>计算完 <code>qk</code> 后, 由当前 thread group 中第一个 (offset 为 0) 的 thread 对自己刚才算出来的 <code>qk</code> 进行 mask, 顺便看看如果没有 mask 掉, 把 <code>qk_max</code> 赋值为 <code>qk</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">thread_group_offset</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Store the partial reductions to shared memory.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// NOTE(woosuk): It is required to zero out the masked logits.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">bool</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">token_idx</span> <span class="o">&gt;=</span> <span class="n">seq_len</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">[</span><span class="n">token_idx</span> <span class="o">-</span> <span class="n">start_token_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">?</span> <span class="mf">0.f</span> <span class="o">:</span> <span class="n">qk</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Update the max value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">qk_max</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">?</span> <span class="nl">qk_max</span> <span class="p">:</span> <span class="n">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">qk</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>🧐 <strong>为什么要做 mask?</strong></p>
<ul>
<li>因为一个 seq 的最后一个 PA block 可能覆盖不满 <code>BLOCK_SIZE</code> 个 token. 这里的 mask 就是把那部分 qk 置零.</li>
</ul>
<h3 id="54-softmax">5.4. Softmax</h3>
<p>我勒个 QK 啊, 总算算完了, 锐克 five 都要被抽清仓了. 页意丁真, 鉴定为开算 softmax.</p>
<p>主要步骤就是广播然后算, 算 softmax 需要知道每个 head 对应的 qk 的最大值. 由于一个 cuda block 负责的就是一个 head, 对于这个 head 上面的计算步骤一共算了 <code>cache_len</code>个 token 的 qk, 因此需要做一个 cuda block 范围的规约, 找到其中最大的 qk 值.</p>
<p>先在 warp 层面规约.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__shared__</span> <span class="kt">float</span> <span class="n">red_smem</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">NUM_WARPS</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="c1">// Perform reduction across the threads in the same warp to get the
</span></span></span><span class="line"><span class="cl"><span class="c1">// max qk value for each &#34;warp&#34; (not across the thread block yet).
</span></span></span><span class="line"><span class="cl"><span class="c1">// The 0-th thread of each thread group already has its max qk value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">WARP_SIZE</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">mask</span> <span class="o">&gt;=</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span> <span class="n">mask</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">qk_max</span> <span class="o">=</span> <span class="n">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">VLLM_SHFL_XOR_SYNC</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">mask</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">lane</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">red_smem</span><span class="p">[</span><span class="n">warp_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">qk_max</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">__syncthreads</span><span class="p">();</span>
</span></span></code></pre></div><ul>
<li><code>red_smem</code> 是之前申请的 shared memory.</li>
<li><code>VLLM_SHFL_XOR_SYNC</code> 是一个 warp 内的 shuffle 操作, 具体来说, 在每次循环时, 每个 thread 和自己相距 <code>mask</code> 位置的线程交换数据 (交换来的数据通过 <code>fmaxf</code> 比较), 并且 <code>mask</code> 会逐渐减半, 直到 <code>THREAD_GROUP_SIZE</code> 为止.</li>
<li><code>lane</code> 表示当前 warp 中的线程索引.</li>
</ul>
<p>接着再对每个 warp 的最大值进行规约, 由于每个 warp 的最大值都被存入了 <code>red_smem</code> 中, 所以只需要再次进行 shuffle 操作即可.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// TODO(woosuk): Refactor this part.
</span></span></span><span class="line"><span class="cl"><span class="c1">// Get the max qk value for the sequence.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">qk_max</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">&lt;</span> <span class="n">NUM_WARPS</span> <span class="o">?</span> <span class="n">red_smem</span><span class="p">[</span><span class="n">lane</span><span class="p">]</span> <span class="o">:</span> <span class="o">-</span><span class="n">FLT_MAX</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">NUM_WARPS</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">mask</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">mask</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">qk_max</span> <span class="o">=</span> <span class="n">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">VLLM_SHFL_XOR_SYNC</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">mask</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>此时, 第 1 个线程的 <code>qk_max</code> 就是当前 cuda block 中所有 warp 中最大的 qk 值. 将其广播给所有线程:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Broadcast the max qk value to all threads.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">qk_max</span> <span class="o">=</span> <span class="n">VLLM_SHFL_SYNC</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
</span></span></code></pre></div><p>在获得了 <code>qk_max</code> 后, 就可以计算 softmax 了:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Get the sum of the exp values.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">float</span> <span class="n">exp_sum</span> <span class="o">=</span> <span class="mf">0.f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_tokens</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREADS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">__expf</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">qk_max</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">exp_sum</span> <span class="o">+=</span> <span class="n">val</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">exp_sum</span> <span class="o">=</span> <span class="n">block_sum</span><span class="o">&lt;</span><span class="n">NUM_WARPS</span><span class="o">&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="n">red_smem</span><span class="p">[</span><span class="n">NUM_WARPS</span><span class="p">],</span> <span class="n">exp_sum</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Compute softmax.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">float</span> <span class="n">inv_sum</span> <span class="o">=</span> <span class="n">__fdividef</span><span class="p">(</span><span class="mf">1.f</span><span class="p">,</span> <span class="n">exp_sum</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="n">f</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_tokens</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREADS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*=</span> <span class="n">inv_sum</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">__syncthreads</span><span class="p">();</span>
</span></span></code></pre></div><h3 id="55-lv-logits--value">5.5. LV (Logits * Value)</h3>
<p><img alt="pa-cal.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-cal.png"></p>
<p>上图展示了 LV 的计算过程, 主要区别是由于要计算 Logits 的 shape 可以表示为 <code>(num_heads, num_seqs, cache_len)</code>, 而 V 的 shape 可以表示为 <code>(num_heads, cache_len, head_size)</code>, 因此 LV 的矩阵乘法中, 每计算一个元素需要读取 logits 的一行和 V 的一列进行计算.</p>
<p>此时, 一个 cuda block 的职责从 &ldquo;自 Q 中读取一个 head&rdquo; 转变为 &ldquo;计算 output 中的一个 head&rdquo;.</p>
<p>🧐 <strong>为什么在计算 LV 时, 去掉了 thread group 的概念, 每个 thread 都被设定为每次读取 16B?</strong></p>
<ul>
<li>因为现在每计算一个元素, 需要的访存量更大, 因此给每个 thread 分配了更多的数据读取量. 也就是说, <code>V_VEC_SIZE</code> 比 <code>VEC_SIZE</code> 更大.</li>
</ul>
<p>由于 cuda 访存模式按行读取更快, 所以实际的计算结果在遍历 PA block 时线程内部利用 <code>accs</code> 进行累计 (以实现与 V 的一列进行计算的行为):</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">=</span> <span class="n">BLOCK_SIZE</span> <span class="o">/</span> <span class="n">V_VEC_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_ROWS_PER_ITER</span> <span class="o">=</span> <span class="n">WARP_SIZE</span> <span class="o">/</span> <span class="n">NUM_V_VECS_PER_ROW</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_ROWS_PER_THREAD</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">    <span class="n">DIVIDE_ROUND_UP</span><span class="p">(</span><span class="n">HEAD_SIZE</span><span class="p">,</span> <span class="n">NUM_ROWS_PER_ITER</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// NOTE(woosuk): We use FP32 for the accumulator for better accuracy.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">float</span> <span class="n">accs</span><span class="p">[</span><span class="n">NUM_ROWS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">V_vec</span> <span class="n">v_vec</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">V_VEC_SIZE</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Load V to `v_vec` ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">v_vec_ptr</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">token_idx</span> <span class="o">+</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">seq_len</span> <span class="o">?</span> <span class="n">v_vec_ptr</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">:</span> <span class="n">zero_value</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Accumulate the dot product.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">dot</span><span class="p">(</span><span class="n">logits_vec</span><span class="p">,</span> <span class="n">v_vec</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>由于每个线程负责的累计部分不满一整行/列, 所以进行规约:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Perform reduction within each warp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">mask</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">mask</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">acc</span> <span class="o">+=</span> <span class="n">VLLM_SHFL_XOR_SYNC</span><span class="p">(</span><span class="n">acc</span><span class="p">,</span> <span class="n">mask</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">acc</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// NOTE(woosuk): A barrier is required because the shared memory space for
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// logits is reused for the output.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Perform reduction across warps.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">float</span><span class="o">*</span> <span class="n">out_smem</span> <span class="o">=</span> <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">shared_mem</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">NUM_WARPS</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">mid</span> <span class="o">=</span> <span class="n">i</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Upper warps write to shared memory.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">warp_idx</span> <span class="o">&gt;=</span> <span class="n">mid</span> <span class="o">&amp;&amp;</span> <span class="n">warp_idx</span> <span class="o">&lt;</span> <span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="kt">float</span><span class="o">*</span> <span class="n">dst</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">out_smem</span><span class="p">[(</span><span class="n">warp_idx</span> <span class="o">-</span> <span class="n">mid</span><span class="p">)</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">const</span> <span class="kt">int</span> <span class="n">row_idx</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">/</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">NUM_ROWS_PER_ITER</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">HEAD_SIZE</span> <span class="o">&amp;&amp;</span> <span class="n">lane</span> <span class="o">%</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="n">dst</span><span class="p">[</span><span class="n">row_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Lower warps update the output.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">warp_idx</span> <span class="o">&lt;</span> <span class="n">mid</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">src</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">out_smem</span><span class="p">[</span><span class="n">warp_idx</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">const</span> <span class="kt">int</span> <span class="n">row_idx</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">/</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">NUM_ROWS_PER_ITER</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">HEAD_SIZE</span> <span class="o">&amp;&amp;</span> <span class="n">lane</span> <span class="o">%</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">src</span><span class="p">[</span><span class="n">row_idx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span></code></pre></div><p>最后写入到输出中:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl">  <span class="c1">// Write the final output.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="n">warp_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">scalar_t</span><span class="o">*</span> <span class="n">out_ptr</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">+</span> <span class="n">seq_idx</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">max_num_partitions</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">        <span class="n">head_idx</span> <span class="o">*</span> <span class="n">max_num_partitions</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span> <span class="o">+</span> <span class="n">partition_idx</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">const</span> <span class="kt">int</span> <span class="n">row_idx</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">/</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">NUM_ROWS_PER_ITER</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="p">(</span><span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">HEAD_SIZE</span> <span class="o">&amp;&amp;</span> <span class="n">lane</span> <span class="o">%</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">from_float</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">out_ptr</span> <span class="o">+</span> <span class="n">row_idx</span><span class="p">),</span> <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span></code></pre></div>]]></content:encoded></item><item><title>A Simple Pytorch Trainpipeline</title><link>https://jamesnulliu.github.io/blogs/a-simple-pytorch-trainpipeline/</link><pubDate>Sun, 30 Jun 2024 01:52:00 +0800</pubDate><guid>https://jamesnulliu.github.io/blogs/a-simple-pytorch-trainpipeline/</guid><description>How to build a simple Pytorch trainpipeline.</description><content:encoded><![CDATA[<h2 id="1-introduction">1. Introduction</h2>
<p>In general, you will need these things to train a model:</p>
<ul>
<li>A Model</li>
<li>A Dataset</li>
<li>A Dataloader</li>
<li>A Loss Function (Criterion)</li>
<li>An Optimizer</li>
</ul>
<h2 id="2-model">2. Model</h2>
<p>We will build a simple model for demonstration. The model takes a tensor of shape <code>(batch_size, 10)</code> as input and outputs a tensor of shape <code>(batch_size, 2)</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># @file simple_model.py</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># Shape: (4, 10)</span>
</span></span><span class="line"><span class="cl">    <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Shape: (4, 2)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>You can run the script to check how the model works:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">python simple_model.py
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="3-dataset">3. Dataset</h2>
<p>We will build a simple dataset for demonstration. The dataset generates random data and labels.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># @file simple_dataset.py</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">SimpleDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># Shape: (10,); Element type: float32</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>  <span class="c1"># Shape: (1,); Element type: int64</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">dataset</span> <span class="o">=</span> <span class="n">SimpleDataset</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Shape: (10,), (1,)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>You can run the script to check how the dataset works:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">python simple_dataset.py
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="4-dataloader">4. Dataloader</h2>
<p>As long as the dataset is built, creating a dataloader is quite easy.</p>
<p>A dataloader will provide <code>batch_size</code> samples in each iteration. For example:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># @file temp.py</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">simple_dataset</span> <span class="kn">import</span> <span class="n">SimpleDataset</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">dataset</span> <span class="o">=</span> <span class="n">SimpleDataset</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Get a sample, shape: (10,), (1,)</span>
</span></span><span class="line"><span class="cl"><span class="n">sample_x</span><span class="p">,</span> <span class="n">sample_y</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Suppose batch_size is 16, the dataloader will provide 16 samples in each iteration</span>
</span></span><span class="line"><span class="cl"><span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Shape: (16, 10), (16, 1)</span>
</span></span><span class="line"><span class="cl">    <span class="k">break</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>You can run the script to check how the dataloader works:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">python temp.py
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="5-loss-function">5. Loss Function</h2>
<p>Different tasks require different loss functions. For example, a 2-class classification task can use <code>nn.CrossEntropyLoss</code>, while a regression task can use <code>nn.MSELoss</code>.</p>
<p>In our case, we will use <code>nn.CrossEntropyLoss</code>.</p>
<h2 id="6-optimizer">6. Optimizer</h2>
<p>We will use <code>torch.optim.SGD</code> as the optimizer. <code>torch.optim.Adam</code> is also a good choice. This is a hyperparameter that you can tune.</p>
<h2 id="7-trainpipeline">7. Trainpipeline</h2>
<p>Now we can build the trainpipeline. The trainpipeline will train the model on the dataset.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># @file trainpipeline.py</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># This is the model we built</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">simple_model</span> <span class="kn">import</span> <span class="n">SimpleModel</span>
</span></span><span class="line"><span class="cl"><span class="c1"># This is the dataset we built</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">simple_dataset</span> <span class="kn">import</span> <span class="n">SimpleDataset</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">DEVICE</span> <span class="o">=</span> <span class="s2">&#34;cuda&#34;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&#34;cpu&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">16</span>
</span></span><span class="line"><span class="cl"><span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">100</span>
</span></span><span class="line"><span class="cl"><span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.01</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Create a model and move it to DEVICE</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Create train dataset and dataloader</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">SimpleDataset</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">val_dataset</span> <span class="o">=</span> <span class="n">SimpleDataset</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">val_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Create a loss function and an optimizer; The optimizer will update the model&#39;s parameters</span>
</span></span><span class="line"><span class="cl">    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># Set the model to training mode</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># Set the model to evaluation mode</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>  <span class="c1"># Disable gradient calculation</span>
</span></span><span class="line"><span class="cl">            <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="n">total_correct</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="n">total_samples</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">val_dataloader</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">                <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="n">total_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="n">total_samples</span> <span class="o">+=</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">total_loss</span> <span class="o">/</span> <span class="n">total_samples</span><span class="si">}</span><span class="s2">, Accuracy: </span><span class="si">{</span><span class="n">total_correct</span> <span class="o">/</span> <span class="n">total_samples</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">train</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>You can run the script to check how the trainpipeline works:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">python trainpipeline.py
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded></item></channel></rss>