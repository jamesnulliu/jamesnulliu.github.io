<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Vllm on ç§‹æ°´Â·JamesNULLiu</title><link>https://jamesnulliu.github.io/zh/tags/vllm/</link><description>Recent content in Vllm on ç§‹æ°´Â·JamesNULLiu</description><generator>Hugo -- 0.148.2</generator><language>zh</language><copyright>2024-2025 JamesNULLiu</copyright><lastBuildDate>Fri, 12 Sep 2025 22:46:13 +0000</lastBuildDate><atom:link href="https://jamesnulliu.github.io/zh/tags/vllm/index.xml" rel="self" type="application/rss+xml"/><item><title>æµ…è°ˆæŠ•æœºæ¨ç†</title><link>https://jamesnulliu.github.io/zh/blogs/a-brief-talk-on-speculative-decoding/</link><pubDate>Fri, 21 Feb 2025 01:14:06 +0800</pubDate><guid>https://jamesnulliu.github.io/zh/blogs/a-brief-talk-on-speculative-decoding/</guid><description>å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æŠ•æœºæ¨ç†ç®€è¦ä»‹ç».</description><content:encoded><![CDATA[<h2 id="1-æŠ•æœºæ¨ç†-speculative-decoding">1. æŠ•æœºæ¨ç† (Speculative Decoding)</h2>
<h3 id="11-ç®€ä»‹">1.1. ç®€ä»‹</h3>
<p>ä¼ ç»Ÿçš„ LLM ä»¥è‡ªå›å½’æ–¹å¼é€ä¸ªç”Ÿæˆ token. ä¾‹å¦‚, ç»™å®šä¸€ä¸ªæç¤º (prompt), æ¨¡å‹éœ€è¦åˆ†åˆ«è¿›è¡Œä¸‰æ¬¡å‰å‘ä¼ æ’­æ¥ç”Ÿæˆä¸‰ä¸ª token T1, T2, T3. æ¨æµ‹è§£ç  (Speculative Decoding) é€šè¿‡å…è®¸ä¸€æ¬¡æ€§æè®®å¹¶éªŒè¯å¤šä¸ª tokenï¼Œæ”¹å˜äº†è¿™ä¸€ç”Ÿæˆè¿‡ç¨‹.</p>
<p>å…¶æ ¸å¿ƒæµç¨‹å¦‚ä¸‹:</p>
<ol>
<li><strong>è‰ç¨¿æ¨¡å‹æè®®</strong>: é€šè¿‡ä¸€ä¸ªæ›´è½»é‡é«˜æ•ˆçš„æ¨¡å‹é€ä¸ªæè®®å€™é€‰ token</li>
<li><strong>ç›®æ ‡æ¨¡å‹éªŒè¯</strong>: å°†å€™é€‰åºåˆ—æäº¤ç»™å¤§æ¨¡å‹è¿›è¡Œå•æ¬¡å‰å‘ä¼ æ’­éªŒè¯. å¤§æ¨¡å‹ä¼šç¡®è®¤æ­£ç¡® token å¹¶çº æ­£é”™è¯¯æè®®</li>
<li><strong>å•æ¬¡å¤„ç†å¤š token</strong>: ä¸ä¼ ç»Ÿ &ldquo;ä¸€æ¬¡ä¸€ token&rdquo; æ¨¡å¼ä¸åŒ, è¯¥æ–¹æ³•èƒ½å¹¶è¡Œå¤„ç†å¤šä¸ª token, æ˜¾è‘—é™ä½ç”Ÿæˆå»¶è¿Ÿ</li>
</ol>
<p>By using this approach, speculative decoding speeds up token generation, making it an effective method for both small-scale and large-scale language model deployments.</p>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/sd-example.png" 
        alt="" 
        class="image" 
        width="80%"/>
    <div class="image-caption">
        å¦‚å›¾æ‰€ç¤º, è‰ç¨¿æ¨¡å‹æè®®äº†äº”ä¸ªtoken: [&#34;I&#34;, &#34;like&#34;, &#34;cooking&#34;, &#34;and&#34;, &#34;traveling&#34;]. ç›®æ ‡æ¨¡å‹é€šè¿‡å•æ¬¡å‰å‘ä¼ æ’­è¿›è¡Œå¹¶è¡ŒéªŒè¯. æœ¬ä¾‹ä¸­ç¬¬ä¸‰ä¸ª token &#34;cooking&#34; (æ­£ç¡®åº”ä¸º &#34;playing&#34;) æè®®é”™è¯¯, å› æ­¤æœ€ç»ˆæ¥å—å‰ä¸‰ä¸ªæœ‰æ•ˆ token[&#34;I&#34;, &#34;like&#34;, &#34;playing&#34;]
    </div>
</div>
<p>é€šè¿‡è¿™ç§ &ldquo;å…ˆæ¨æµ‹åéªŒè¯&rdquo; çš„æœºåˆ¶, æ¨æµ‹è§£ç å®ç°äº†ç”Ÿæˆé€Ÿåº¦çš„é£è·ƒ. è¯¥æ–¹æ³•å…¼å…·é€šç”¨æ€§å’Œé«˜æ•ˆæ€§, é€‚ç”¨äºä¸åŒè§„æ¨¡çš„æ¨¡å‹éƒ¨ç½²åœºæ™¯.</p>
]]></content:encoded></item><item><title>Dive into Paged Attention</title><link>https://jamesnulliu.github.io/zh/blogs/dive-into-paged-attention/</link><pubDate>Mon, 07 Oct 2024 12:00:00 +0800</pubDate><guid>https://jamesnulliu.github.io/zh/blogs/dive-into-paged-attention/</guid><description>Dive into the paged attention mechanism of vLLM.</description><content:encoded><![CDATA[<h2 id="1-è¯æ˜-attention-çš„--åªä¸--æœ‰å…³">1. è¯æ˜ Attention çš„ $O_i$ åªä¸ $Q_i$ æœ‰å…³</h2>
<p>Attention çš„å…¬å¼å¦‚ä¸‹:</p>
$$
O=Attention(Q,K,V)=softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$<p>å‡è®¾ $Q=\begin{bmatrix}Q_0\\Q_1\end{bmatrix}$, $K=\begin{bmatrix}K_0\\K_1\end{bmatrix}$</p>
<p>é‚£ä¹ˆ:</p>
$$
O=softmax(\frac{\begin{bmatrix}Q_0K_0^T&Q_0K_1^T\\Q_1K_0^T&Q_1K_1^T\end{bmatrix}}{\sqrt{d_k}})V
$$<p>ä»¤:</p>
$$
A=\begin{bmatrix}A_0\\A_1\end{bmatrix}=\begin{bmatrix}Q_0K_0^T&Q_0K_1^T\\Q_1K_0^T&Q_1K_1^T\end{bmatrix},f(x)=\frac{softmax(x)}{\sqrt{d_k}}
$$<p>æ­¤æ—¶, $A_1$ åªå’Œ $Q_1$ æœ‰å…³, å’Œ $Q_0$ æ— å…³, é‚£ä¹ˆ:</p>
$$
\begin{bmatrix}O_0\\O_1\end{bmatrix}=O=\begin{bmatrix}f(A_0)\\f(A_1)\end{bmatrix}V=\begin{bmatrix}f(A_0)V\\f(A_1)V\end{bmatrix}
$$<p>å› æ­¤, $O_i$ åªå’Œ  $A_i$ ç›¸å…³, è€Œæ ¹æ® $A$ çš„è®¾å®š, $A_i$ åªå’Œ $Q_i$ ç›¸å…³, å³:</p>
<p>Attention çŸ©é˜µçš„ç¬¬ $i$ ä¸ªè¾“å‡ºåªå’Œç¬¬ $i$ ä¸ª $Q$ æœ‰å…³, å’Œä¹‹å‰çš„ $Q$ æ— å…³.</p>
<p><strong>æ€»ç»“</strong>:</p>
<ul>
<li>åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶ï¼Œåªéœ€å¯¹æ–° token è®¡ç®—å¯¹åº”çš„ <code>Q_new</code>ï¼Œå¹¶ä¸ä¹‹å‰å·²ç»ç¼“å­˜çš„ <code>K_cache</code> å’Œ <code>V_cache</code> è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚</li>
<li>æ–°çš„ <code>K_new</code> å’Œ <code>V_new</code> ä¼šè¢«åŠ å…¥åˆ°ç¼“å­˜ä¸­ï¼Œç»§ç»­ä¸ºä¸‹ä¸€ä¸ª token ç”Ÿæˆæä¾›åŸºç¡€ã€‚</li>
<li>æ•´ä¸ªè¿‡ç¨‹é¿å…äº†å¯¹æ‰€æœ‰å†å² token çš„é‡å¤è®¡ç®—ï¼Œå¤§å¹…æé«˜äº†æ•ˆç‡ã€‚</li>
</ul>
<h2 id="2-kv-cache-çš„å¢é‡è¿‡ç¨‹">2. KV Cache çš„å¢é‡è¿‡ç¨‹</h2>
<h3 id="21-åˆå§‹è¾“å…¥å®Œæ•´åºåˆ—è®¡ç®—">2.1. åˆå§‹è¾“å…¥ï¼ˆå®Œæ•´åºåˆ—ï¼‰è®¡ç®—ï¼š</h3>
<ul>
<li>å¯¹äºåˆå§‹çš„è¾“å…¥åºåˆ— <code>(seq_len, embed_dim)</code>ï¼Œæˆ‘ä»¬é€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ° <code>Q</code>ã€<code>K</code> å’Œ <code>V</code>ï¼Œå®ƒä»¬çš„å½¢çŠ¶éƒ½æ˜¯ <code>(seq_len, embed_dim)</code>ã€‚</li>
<li>ä½¿ç”¨ <code>Q</code> å’Œ <code>K</code> è¿›è¡Œç‚¹ç§¯è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œç„¶åç»“åˆ <code>V</code> è®¡ç®—å¾—åˆ°è¾“å‡º <code>(seq_len, embed_dim)</code>ï¼Œè¿™æ˜¯ç¬¬ä¸€æ¬¡å¯¹åˆå§‹åºåˆ—çš„å®Œæ•´è®¡ç®—ã€‚</li>
</ul>
<h3 id="22-é¢„æµ‹ä¸‹ä¸€ä¸ª-token-æ—¶çš„å¢é‡è®¡ç®—">2.2. é¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶çš„å¢é‡è®¡ç®—ï¼š</h3>
<p>åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶ï¼Œä¸éœ€è¦å¯¹æ•´ä¸ªåºåˆ—å†è¿›è¡Œå®Œæ•´çš„ <code>Q</code>ã€<code>K</code>ã€<code>V</code> è®¡ç®—ï¼Œè€Œæ˜¯åªéœ€å¯¹æ–°ç”Ÿæˆçš„ token è¿›è¡Œä¸€æ¬¡å¢é‡è®¡ç®—ã€‚è¿™æ—¶çš„æ“ä½œæµç¨‹å¦‚ä¸‹ï¼š</p>
<ol>
<li><strong>è¾“å…¥æ–°çš„ token</strong>ï¼šå°†å·²ç»ç”Ÿæˆçš„ tokenï¼ˆå…¶å½¢çŠ¶ä¸º <code>(embed_dim,)</code>ï¼‰ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ°è¯¥ token å¯¹åº”çš„ <code>Q_new</code>ï¼Œå½¢çŠ¶ä¸º <code>(embed_dim,)</code>ã€‚</li>
<li><strong>ä¸ä¹‹å‰ç¼“å­˜çš„ <code>K</code> å’Œ <code>V</code> è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—</strong>ï¼šä½¿ç”¨ <code>Q_new</code> ä¸ä¹‹å‰å·²ç»è®¡ç®—å¹¶ç¼“å­˜çš„ <code>K_cache</code> å’Œ <code>V_cache</code> è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚è¿™é‡Œçš„ <code>K_cache</code> å’Œ <code>V_cache</code> åˆ†åˆ«æ˜¯ä¹‹å‰æ¯æ¬¡ç”Ÿæˆ token æ—¶å¾—åˆ°çš„ <code>K</code> å’Œ <code>V</code>ï¼Œå®ƒä»¬çš„å½¢çŠ¶æ˜¯ <code>(seq_len, embed_dim)</code>ï¼Œå³ç¼“å­˜äº†ä»æœ€åˆè¾“å…¥åºåˆ—åˆ°å½“å‰å·²ç»ç”Ÿæˆçš„æ‰€æœ‰ token çš„ <code>K</code> å’Œ <code>V</code>ã€‚<code>Q_new</code> å¯ä»¥ç›´æ¥ä¸ <code>K_cache</code> è¿›è¡Œç‚¹ç§¯ï¼Œå¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°ï¼Œç„¶åç»“åˆ <code>V_cache</code> å¾—åˆ°æ–°çš„è¾“å‡ºã€‚</li>
<li><strong>æ›´æ–° <code>KV Cache</code></strong>ï¼šæ–°çš„ <code>K_new</code> å’Œ <code>V_new</code> ä¼šé€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ°ï¼ˆå½¢çŠ¶ä¸º <code>(embed_dim,)</code>ï¼‰ï¼Œå¹¶å°†å®ƒä»¬æ·»åŠ åˆ° <code>K_cache</code> å’Œ <code>V_cache</code> çš„æœ«å°¾ï¼Œä½¿å¾—ç¼“å­˜çš„ <code>K_cache</code> å’Œ <code>V_cache</code> ä¸æ–­å¢å¤§ï¼Œä»¥å¤‡åç»­ä½¿ç”¨ã€‚</li>
<li><strong>è¾“å‡º</strong>ï¼šé€šè¿‡æ³¨æ„åŠ›è®¡ç®—åçš„è¾“å‡ºå½¢çŠ¶ä¸º <code>(embed_dim,)</code>ï¼Œå³æ–°ç”Ÿæˆçš„ tokenã€‚</li>
</ol>
<h2 id="4-vllm-ä¸­çš„-paged-attention">4. vllm ä¸­çš„ Paged Attention</h2>
<h3 id="41-åŠ¨æœº-memory-wastes">4.1. åŠ¨æœº: Memory Wastes</h3>
<p><img alt="memory-wastes.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/memory-wastes.png"></p>
<p>ä¸Šå›¾å±•ç¤ºäº†å¯èƒ½çš„å†…å­˜æµªè´¹æƒ…å†µ, ä¸»è¦æ—¶è¾“å…¥ sequence ä¸çŸ¥é“ eos åœ¨å“ªé‡Œ, å¦‚æœéšæœºç”³è¯·å†…å­˜, å¯èƒ½å¯¼è‡´å¤§é‡å†…å­˜ç¢ç‰‡, å› æ­¤ååé‡ä¸‹é™.</p>
<h3 id="42-è§£å†³æ–¹æ¡ˆ-ç”¨-page-ç®¡ç†å†…å­˜">4.2. è§£å†³æ–¹æ¡ˆ: ç”¨ Page ç®¡ç†å†…å­˜</h3>
<p><img alt="paged-attention-animation.webp" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/paged-attention-animation.webp"></p>
<p>ä¸Šå›¾å±•ç¤ºäº† vLLM ç”¨ Paged ç®¡ç†å†…å­˜å…·ä½“æ€ä¹ˆåšçš„.</p>
<p>ç®€å•æ¥è¯´, vLLM åœ¨å¼€å§‹æ¨ç†å‰ä¸ºæ¯ä¸ª Decoder Layer ç”³è¯·ä¸¤ä¸ªå·¨é•¿çš„ Tensor (<code>k_cache</code> å’Œ <code>v_cache</code>), æŠŠ Tensor åˆ†å‰²æˆè¿ç»­ç­‰é•¿çš„ PA blocks (å›¾ä¸­çš„ä¸€è¡Œä¸ºä¸€ä¸ª PA Block); æ¯ä¸ª PA Block èƒ½å¤Ÿå­˜æ”¾ <code>BLOCK_SIZE</code> ä¸ª token çš„ K æˆ– V cache (æ¯ä¸ª cache çš„å½¢çŠ¶å¯ä»¥ç†è§£ä¸º <code>(num_heads, head_size)</code>).</p>
<p>å› æ­¤, <code>k_cache</code> å’Œ <code>v_cache</code> çš„å½¢çŠ¶å¯ä»¥ç†è§£ä¸º <code>(num_blocks, num_heads, head_size)</code>.</p>
<p>å¯¹äºä¸€ä¸ªè¿ç»­çš„ sequnce, åœ¨ prefill é˜¶æ®µå‰å°±ä¼šåˆ†é…å¥½å®ƒçš„ PA blocks, ä¹‹åæ¨ç†æ—¶:</p>
<ul>
<li>è‹¥æ˜¯è®¡ç®— prompt çš„ Attention, åˆ™å…ˆæŠŠä¼ å…¥çš„ K å’Œ V æŒ‰ç…§ PA blocks å­˜å…¥ <code>k_cache</code> å’Œ <code>v_cache</code> ä¸­; ç„¶ååˆ©ç”¨æ•´æ®µçš„ QKV è®¡ç®— attention.</li>
<li>è‹¥æ˜¯è®¡ç®—æ–° token, åˆ™åˆ©ç”¨ Q å’Œ block table è®¡ç®— decode é˜¶æ®µçš„ attntion; æ­¤æ—¶è®¿å­˜çš„å°±æ˜¯ <code>k_cache</code> å’Œ <code>v_cache</code> ä¸­çš„ PA blocks.</li>
</ul>
<h2 id="5-paged-attention-kernel-è¯¦è§£">5. Paged Attention Kernel è¯¦è§£</h2>
<blockquote>
<p>References:</p>
<ul>
<li><a href="https://docs.vllm.ai/en/latest/dev/kernel/paged_attention.html">vLLM Paged Attention</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/673284781">vLLMçš‡å† ä¸Šçš„æ˜ç ï¼šæ·±å…¥æµ…å‡ºç†è§£PagedAttention CUDAå®ç°</a></li>
</ul></blockquote>
<p>å…ˆçœ‹ä¸‹æ•´ä½“è®¡ç®—æµç¨‹å›¾ (è¿™ä¸ªå›¾åé¢ä¹Ÿä¼šå‡ºç°è¿™é‡Œå…ˆçœ‹ä¸€çœ¼):</p>
<p><img alt="pa-cal.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-cal.png"></p>
<h3 id="51-è¾“å…¥è¾“å‡ºè¾“å‡ºåˆ†æå’Œå‚æ•°è¯´æ˜">5.1. è¾“å…¥è¾“å‡ºè¾“å‡ºåˆ†æå’Œå‚æ•°è¯´æ˜</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Grid: (num_heads, num_seqs, 1).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">template</span><span class="o">&lt;</span>
</span></span><span class="line"><span class="cl"><span class="k">typename</span> <span class="n">scalar_t</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">HEAD_SIZE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">NUM_THREADS</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">PARTITION_SIZE</span> <span class="o">=</span> <span class="mi">0</span><span class="o">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="n">__device__</span> <span class="kt">void</span> <span class="n">paged_attention_kernel</span><span class="p">(</span>
</span></span><span class="line"><span class="cl"><span class="p">...</span> <span class="c1">// Other side args.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="n">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">out</span><span class="p">,</span>       <span class="c1">// [num_seqs, num_heads, max_num_partitions, head_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="n">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">q</span><span class="p">,</span>         <span class="c1">// [num_seqs, num_heads, head_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="n">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">k_cache</span><span class="p">,</span>   <span class="c1">// [num_blocks, num_kv_heads, head_size/x, block_size, x]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="n">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">v_cache</span><span class="p">,</span>   <span class="c1">// [num_blocks, num_kv_heads, head_size, block_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">...</span> <span class="c1">// Other side args.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">)</span>
</span></span></code></pre></div><p>æ¨¡æ¿å‚æ•°è¯´æ˜:</p>
<ul>
<li><code>scalar_t</code> å…ƒç´ ç±»å‹ (å®é™…ä»£ç ä¸­è¿˜æœ‰ <code>cache_t</code> è¡¨ç¤º KV cache çš„å…ƒç´ ç±»å‹).</li>
<li><code>HEAD_SIZE</code> æ¯ä¸ª head ä¸­å…ƒç´ æ•°é‡.</li>
<li><code>BLOCK_SIZE</code> æ¯ä¸ª PA block ä¸­çš„ token æ•°é‡.
<blockquote>
<ol>
<li>KV cache è¢«å­˜å‚¨åœ¨ä¸åŒ PA blocks. æ¯ä¸ª PA block å­˜å‚¨ä¸€ä¸ª head ä¸­ <code>BLOCK_SIZE</code> ä¸ª token.<br>
ä¾‹å¦‚, è‹¥ <code>BLOCK_SIZE=16</code>, <code>HEAD_SIZE=128</code>, åˆ™ä¸€ä¸ª  PA block èƒ½å­˜å‚¨ä¸€ä¸ª head çš„ <code>16 * 128 = 2048</code> ä¸ªå…ƒç´ .</li>
<li>æ¯ä¸ª PA block å¯èƒ½åªåŒ…å«ä¸€éƒ¨åˆ†çš„ context tokens.</li>
<li>ä» page è§’åº¦çœ‹, KV cache æ˜¯è‹¥å¹²ä¸ª page çš„é›†åˆ;</li>
</ol></blockquote>
</li>
<li><code>NUM_THREADS</code> æ¯ä¸ª CUDA thread block ä¸­ thread çš„æ•°é‡.</li>
<li><code>PARTITION_SIZE</code> å‚ä¸ TP çš„ GPU æ•°é‡, é»˜è®¤ 0 è¡¨ç¤ºå•å¡. (ä»¥ä¸‹éƒ½ä»¥å•å¡ä¸ºä¾‹è¯´æ˜)</li>
</ul>
<p>é¢å¤–çš„ä¸€äº›å‚æ•°:</p>
<ul>
<li><code>num_seqs</code>: æœ¬æ¬¡æ¨ç†è¯·æ±‚ sequence æ•°ç›®.
<blockquote>
<p>ç”±äºè¿™ä¸ª kernel åªå¤„ç† decode é˜¶æ®µå• query attention, æ‰€ä»¥å®é™…ä¸Šæ¯ä¸ª sequence åªæœ‰ä¸€ä¸ª query token.</p></blockquote>
</li>
<li><code>num_heads</code>: Q çš„ head æ•°ç›®</li>
<li><code>num_kv_heads</code>: KV çš„ head æ•°ç›®, å¯¹äº MHA å…¶å€¼å’Œ <code>num_heads</code> ç›¸åŒ; å¦‚æœæ˜¯ GQA, MQA åˆ™ <code>num_kv_heads</code> å°äº <code>num_head</code>.</li>
<li><code>head_size</code>: å³ <code>HEAD_SIZE</code></li>
<li><code>k_cache: (num_blocks, num_kv_heads, head_size/x, block_size, x)</code>, å…¶ä¸­ <code>x</code> è¡¨ç¤º <code>THREAD_GROUP_SIZE * VEC_SIZE</code> çš„å¤§å° (åé¢ä¼šç»†è¯´).</li>
</ul>
<p>ä¸‹é¢ç»“åˆ GPU architecture åˆæ­¥åˆ†æä¸€ä¸‹å‚æ•°.</p>
<p><img alt="gpu-archi.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/gpu-archi.png"></p>
<p>ğŸ§ <strong>ä¸ºä»€ä¹ˆè¦åˆ† thread group?</strong></p>
<ul>
<li>å› ä¸ºå½“ä¸€ä¸ª cuda block è¦å–çš„æ•°æ®æ¯”è¾ƒå°‘çš„æ—¶å€™ (è®¡ç®— QK), ä¸€ä¸ª thread group åˆ†åˆ«ä¸€æ¬¡å– Q å’Œ K ä¸­ 16B; å½“ä¸€ä¸ª cuda block è¦å–çš„æ•°æ®æ¯”è¾ƒå¤šçš„æ—¶å€™ (è®¡ç®— LV), ä¸€ä¸ª thread å– 16B.</li>
</ul>
<h3 id="52shared-memory-q_vecs-çš„å†™å…¥">5.2.Shared Memory: <code>q_vecs</code> çš„å†™å…¥</h3>
<p>ä» kernel ä¸­çš„ç¬¬ä¸€ä¸ªç”³è¯·çš„ shared memory å¼€å§‹è¯´.</p>
<blockquote>
<p>å…³äº shared memeory:</p>
<ol>
<li>åœ¨ kernel ä¸­ç”³è¯·çš„ shared memory è¢«å½“å‰ cuda block ä¸­çš„æ‰€æœ‰ thread å…±äº«.</li>
<li>shared memory çš„ä½œç”¨æ˜¯ä¸ºäº†å‡å°‘ global memory çš„è®¿é—®æ¬¡æ•°ï¼Œæé«˜è®¿å­˜æ•ˆç‡.</li>
</ol></blockquote>
<p>ä»¥ä¸‹ä»£ç ç”³è¯·äº†ä¸€å— shared memroy è¢«æ•´ä¸ª CUDA Block ä¸­æ‰€æœ‰ kernel å…±äº«:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__shared__</span> <span class="n">Q_vec</span> <span class="n">q_vecs</span><span class="p">[</span><span class="n">THREAD_GROUP_SIZE</span><span class="p">][</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span></code></pre></div><p>é¦–å…ˆ, <code>q_vecs</code> è¦†ç›–äº† Q ä¸­ <code>head_size</code> ä¸ªå…ƒç´  - è¿™ä¹Ÿæ˜¯ä¸€ä¸ª cuda block éœ€è¦å¤„ç†çš„æ•°æ®é‡.</p>
<p>æ¥ç€å†è¯´ä¸¤ä¸ªç»´åº¦çš„å‚æ•°çš„æ„æ€:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_ELEMS_PER_THREAD</span> <span class="o">=</span> <span class="n">HEAD_SIZE</span> <span class="o">/</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_VECS_PER_THREAD</span> <span class="o">=</span> <span class="n">NUM_ELEMS_PER_THREAD</span> <span class="o">/</span> <span class="n">VEC_SIZE</span><span class="p">;</span>
</span></span></code></pre></div><ul>
<li><code>THREAD_GROUP_SIZE</code>: æ¯ä¸ª thread group ä¸­çš„ thread æ•°é‡. æ³¨æ„, ä¸€ä¸ª cuda block ä¸­æœ‰ <code>NUM_THREADS</code> ä¸ª thread, <code>NUM_THREAD_GROUPS</code> ä¸ª thread group. <code>THREAD_GROUP_SIZE = MAX(WARP_SIZE/BLOCK_SIZE, 1)</code>.</li>
<li><code>NUM_VECS_PER_THREAD</code>: <code>HEAD_SIZE</code> èƒ½è¢«åˆ†æˆå¤šå°‘ä¸ª 16B. (è¿™ä¸ªå˜é‡è¿™ä¹ˆå‘½åçš„ç†ç”±æ˜¯åé¢è¯»å– K çš„æ—¶å€™æ¯ä¸ª thread ä¼šå¾€è‡ªå·±çš„å¯„å­˜å™¨å†…è¯» <code>NUM_VECS_PER_THREAD</code> ä¸ª k_vec.)</li>
</ul>
<blockquote>
<p>è¯æ˜: <code>q_vecs</code> è¦†ç›– Q çš„ä¸€ä¸ª head, å¹¶ä¸” <code>NUM_VECS_PER_THREAD</code> è¡¨ç¤º Q çš„ä¸€ä¸ª head è¢«åˆ†æˆå¤šå°‘ä¸ª 16B.<br>
=&gt; <code>THREAD_GROUP_SIZE</code> * <code>VEC_SIZE</code> = 16B / <code>sizeof(scalar_t)</code>;<br>
=&gt; <code>NUM_VECS_PER_THREAD</code> * 16B / <code>sizeof(scalar_t)</code> = <code>HEAD_SIZE</code>;</p></blockquote>
<p>ç„¶åçœ‹ load Q çš„ä»£ç , å»ºè®®ç»“åˆä¸‹é¢çš„å›¾ä¸€èµ·çœ‹:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl">  <span class="c1">// Load Q to shmem
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_group_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREAD_GROUPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">vec_idx</span> <span class="o">=</span> <span class="n">thread_group_offset</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">q_vecs</span><span class="p">[</span><span class="n">thread_group_offset</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">        <span class="o">*</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="n">Q_vec</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">q_ptr</span> <span class="o">+</span> <span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><ul>
<li><code>thread_group_idx</code> è¡¨ç¤ºå½“å‰ thread å±äºå½“å‰ cuda block ä¸­ç¬¬å‡ ä¸ª thread group.</li>
<li><code>thread_group_offset</code> è¡¨ç¤ºå½“å‰ thread åœ¨å½“å‰ thread group ä¸­æ˜¯ç¬¬å‡ ä¸ª thread.</li>
</ul>
<p><img alt="pa-load-q.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-load-q.png"></p>
<p>ä¸Šå›¾å±•ç¤ºäº†å¾ªç¯å…·ä½“æ˜¯æ€ä¹ˆè·‘çš„.</p>
<ul>
<li>ä¸€ä¸ªç´«è‰²ç®­å¤´è¡¨ç¤ºä¸€ä¸ª thread group.</li>
<li><code>NUM_VECS_PER_THREAD</code> è¡¨ç¤º <code>HEAD_SIZE</code> èƒ½è¢«åˆ†æˆå¤šå°‘ä¸ª 16B.</li>
<li>å®é™…è¯»å– Q çš„å†…å­˜æ—¶, æ‰€æœ‰ thread group ä» Q çš„èµ·å§‹ä½ç½®ç´§å¯†æ’åˆ—, æ ¹æ®å›¾ä¸Šçœ‹çš„è¯ä¸€å…±æœ‰ <code>NUM_THREAD_GROUPS</code> ä¸ªç´«è‰²ç®­å¤´.</li>
<li>æ‰€æœ‰ thread group è¯»å–ä¸€æ¬¡ Q å¹¶å­˜å…¥ <code>q_vecs</code> å¯¹åº”å¾ªç¯ä¸­çš„ä¸€æ¬¡è¿­ä»£; å› æ­¤ä¸‹æ¬¡è¿­ä»£ thread group éœ€è¦å‘ååç§» <code>NUM_THREAD_GROUPS</code> ä¸ªä½ç½® (ä¾‹å¦‚ <code>i</code> ä» 1 å˜ä¸º 7).</li>
<li>æ­¤å¤–, è¯»ä¸€æ¬¡ 16B å¯¹åº”ä¸€ä¸ª thread æ¥è¯´è‡ªç„¶ä¹Ÿæ˜¯å–ä¸€ä¸ª VEC.</li>
<li>å¯¹åº”åˆ° kernel ç¼–å†™, è¿˜éœ€è¦è®¡ç®—å½“å‰ thread å…·ä½“è¯»å–å“ªä¸ª vec; å› æ­¤å¾—åˆ° <code>vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE</code>.</li>
</ul>
<blockquote>
<p>ğŸ¤” è¿™é‡Œä¼šä¸ä¼šæœ‰ bank conflict?</p></blockquote>
<p>æ€»ä¹‹ç°åœ¨æˆ‘ä»¬æŠŠ <code>(1, head_size)</code> å¤§å°çš„å…ƒç´ è¯»åˆ°äº† cuda block å…±äº«çš„ shared memory <code>q_vecs</code> ä¸­.</p>
<h3 id="53-è¯»å–-k-cache-å¹¶è®¡ç®—-qk">5.3. è¯»å– K Cache å¹¶è®¡ç®— QK</h3>
<p>ç°åœ¨ä» cuda block çš„è§’åº¦çœ‹, å½“å‰ block å·²ç»è·å¾—äº†è‡ªå·±è¦ç®—çš„ Q ä¸­çš„ä¸€ä¸ª head (å½¢çŠ¶ä¸º <code>(1, head_size)</code>), æ¥ä¸‹æ¥å°±æ˜¯è®¡ç®— Q å’Œ K çš„ç‚¹ç§¯.</p>
<p>ç‚¹ç§¯è¿‡ç¨‹æ˜¯æŠŠå½“å‰ block æ‹¥æœ‰çš„ Q head å’Œæ•´ä¸ª K Cache (è¿­ä»£åœ°) è¿›è¡Œç‚¹ç§¯è¿ç®—. å‚è€ƒä¸‹å›¾:</p>
<p><img alt="pa-cal-kq-01.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-cal-kq-01.png"></p>
<p>QK ä¹˜ç§¯å®é™…ä¸Šè¢«æš‚å­˜åœ¨ <code>logits</code> (ä¹Ÿæ˜¯ä¸€å— shared memory) ä¸­, ä¹‹åä¼šè¢«ç”¨æ¥è®¡ç®— softmax.</p>
<p>ğŸ˜‡ çœ‹ä¸‹å¾ªç¯çš„å…·ä½“ä»£ç å§:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Loop 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Physical block calculation ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// Loop 2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Offset calculation ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">K_vec</span> <span class="n">k_vecs</span><span class="p">[</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Loop 3
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Load K to `k_vecs` ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="kt">float</span> <span class="n">qk</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">Qk_dot</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span> <span class="n">THREAD_GROUP_SIZE</span><span class="o">&gt;::</span><span class="n">dot</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                        <span class="n">q_vecs</span><span class="p">[</span><span class="n">thread_group_offset</span><span class="p">],</span> <span class="n">k_vecs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Add the ALiBi bias if slopes are given.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">qk</span> <span class="o">+=</span> <span class="p">(</span><span class="n">alibi_slope</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">?</span> <span class="n">alibi_slope</span> <span class="o">*</span> <span class="p">(</span><span class="n">token_idx</span> <span class="o">-</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">thread_group_offset</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Store the partial reductions to shared memory.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// Mask
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// Update the max value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>å…ˆè¯´ç¬¬ä¸€ä¸ªå¾ªç¯, å…¶ä¸­æ¯”è¾ƒé‡è¦çš„å‡ ä¸ªå‚æ•°å®šä¹‰å¦‚ä¸‹:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// [start_block_idx, end_block_idx) is the range of blocks to process.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">int</span> <span class="n">start_block_idx</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">    <span class="n">USE_PARTITIONING</span> <span class="o">?</span> <span class="n">partition_idx</span> <span class="o">*</span> <span class="nl">num_blocks_per_partition</span> <span class="p">:</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="c1">// If not using partitioning, `end_block_idx` should be equal to `num_seq_blocks`.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">int</span> <span class="n">end_block_idx</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">    <span class="n">MIN</span><span class="p">(</span><span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">num_blocks_per_partition</span><span class="p">,</span> <span class="n">num_seq_blocks</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="c1">// Number of blocks to process.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">int</span> <span class="n">num_blocks</span> <span class="o">=</span> <span class="n">end_block_idx</span> <span class="o">-</span> <span class="n">start_block_idx</span><span class="p">;</span>
</span></span></code></pre></div><p>ç”¨æ–‡å­—æè¿°å°±æ˜¯:</p>
<ul>
<li><code>blk_idx</code> è¡¨ç¤ºå½“å‰ thread æ‰€åœ¨ warp éœ€è¦å¤„ç†çš„ PA block çš„åœ¨ <code>block_table</code> ä¸­ç´¢å¼• (é€»è¾‘ä¸Šçš„ç´¢å¼•).</li>
<li><code>start_block_idx</code> å’Œ <code>end_block_idx</code> è¡¨ç¤ºå½“å‰ cuda block éœ€è¦å¤„ç†çš„ block èŒƒå›´.</li>
<li><code>num_blocks</code> è¡¨ç¤ºå½“å‰ cuda block éœ€è¦å¤„ç†çš„ block æ•°é‡.</li>
<li><code>NUM_WARPS</code> è¡¨ç¤ºå½“å‰ cuda block ä¸­ warp çš„æ•°é‡. ä¸€ä¸ª warp åŒ…å« 32 ä¸ª thread.</li>
<li><code>warp_idx</code> è¡¨ç¤ºå½“å‰ warp åœ¨å½“å‰ cuda block ä¸­çš„ç´¢å¼•.</li>
</ul>
<p>è¯´äººè¯å°±æ˜¯æ¯ä¸ª warp å¤„ç†ä¸€ä¸ª PA block, ä¸€å¼€å§‹ cuda block ä¸­çš„æ‰€æœ‰ warp ç´§å¯†åœ°æŒ‡å‘æœ€å‰é¢çš„ <code>NUM_WARPS</code> ä¸ª PA block, æ¯æ¬¡å¾ªç¯æ‰€æœ‰ warp å‘ååç§» <code>NUM_WARPS</code> ä¸ª PA block çš„é•¿åº¦. å‚è€ƒä¸‹å›¾:</p>
<p><img alt="pa-cal-kq-02.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-cal-kq-02.png"></p>
<blockquote>
<p>ğŸ”” è¿™é‡Œå†å›é¡¾ä¸€ä¸‹, ä¸€ä¸ª PA block é‡Œå­˜æ”¾äº† <code>BLOCK_SIZE</code> ä¸ª token çš„ K æˆ– V cache.</p></blockquote>
<p>æ‰€ä»¥è¯´è¿™ä¸ªå¾ªç¯å’Œä¸Šé¢è¯»å– Q çš„å¾ªç¯ä¸€ä¸ªå°¿æ€§ğŸ¤®, ä¸è¿‡æ˜¯ä»¥ warp çš„ç²’åº¦å¤„ç†æ•°æ®;</p>
<p>è¿›å…¥äº†ç¬¬ä¸€ä¸ªå¾ªç¯å†…éƒ¨, ç¬¬ä¸€æ­¥å½“ç„¶æ˜¯è®¡ç®—å½“å‰ thread å¯¹åº”çš„ warp åº”è¯¥è®¡ç®—å“ªä¸ª PA block (ç‰©ç†ä¸Šçš„ç´¢å¼•), å› æ­¤å¾—åˆ°äº† <code>physical_block_number</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">const</span> <span class="kt">int64_t</span> <span class="n">physical_block_number</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">    <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">block_table</span><span class="p">[</span><span class="n">block_idx</span><span class="p">]);</span>
</span></span></code></pre></div><hr>
<p>ç„¶åè§£é‡Šç¬¬äºŒä¸ªå¾ªç¯, ç¬¬äºŒä¸ªå¾ªç¯çš„æ•´ä½“ç›®æ ‡å°±æ˜¯è®©å½“å‰ warp è®¡ç®—å¥½è‡ªå·±è´Ÿè´£çš„ PA block ä¸­ <code>BLOCK_SIZE</code> ä¸ª token çš„ QK ä¹˜ç§¯.</p>
<p>å…ˆçœ‹ä¸€ä¸‹ <code>i</code> çš„ä¸Šç•Œ:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">    <span class="n">DIVIDE_ROUND_UP</span><span class="p">(</span><span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">WARP_SIZE</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="c1">// Loop 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Loop 2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></div><p>ä» kernel è§’åº¦çœ‹, æ¯ä¸ª thread éœ€è¦è¾…åŠ©å½“å‰ warp è®¡ç®—è‡ªå·±è´Ÿè´£çš„ä¸€æ•´ä¸ª PA block (åŒ…å« <code>BLOCK_SIZE</code> ä¸ª token), è€Œæˆ‘ä»¬æŠŠè¿™ä¸ªè¿‡ç¨‹æ‹†åˆ†ä¸º Loop 2 ä¸­çš„ <code>NUM_TOKEN_PER_THREAD_GROUP</code> (ä¹Ÿå°±æ˜¯ <code>ceil(BLOCK_SIZE / 32)</code>) æ¬¡å¾ªç¯;</p>
<p>è¯´äººè¯å°±æ˜¯<strong>ä¸€ä¸ª thread group å¯¹åº”ä¸€ä¸ª token ä¸­çš„ä¸€ä¸ª head</strong>, å¦‚æœ BLOCK SIZE å¤ªå¤§äº†åé¢æ¯ä¸ª thread å‘ååç§» <code>i * WARP_SIZE</code> ä¸ª token ç»§ç»­ç‹ ç‹ ç®—ğŸ¤£.</p>
<p>ä¹Ÿå› æ­¤ç¬¬äºŒä¸ªå¾ªç¯å†…éƒ¨ä¸€ä¸Šæ¥å…ˆè®¡ç®—äº†å‡ ä¸ªåç§»é‡, å¹¶ä¸”ç”³è¯·äº† thread å†…éƒ¨ç§æœ‰çš„ <code>k_vecs</code> æ•°ç»„:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">const</span> <span class="kt">int</span> <span class="n">physical_block_offset</span> <span class="o">=</span> <span class="p">(</span><span class="n">thread_group_idx</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">WARP_SIZE</span><span class="p">)</span> <span class="o">%</span> <span class="n">BLOCK_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">const</span> <span class="kt">int</span> <span class="n">token_idx</span> <span class="o">=</span> <span class="n">block_idx</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">physical_block_offset</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">K_vec</span> <span class="n">k_vecs</span><span class="p">[</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span></code></pre></div><ul>
<li><code>thread_group_idx</code> è¡¨ç¤ºå½“å‰ thread group åœ¨æ•´ä¸ª cuda block ä¸­çš„ç´¢å¼•.</li>
<li>â˜¢ï¸ ä¸€ä¸ª thread group åœ¨ä¸€æ¬¡å¾ªç¯ä¸­è´Ÿè´£ fetch ä¸€ä¸ª PA block ä¸­ K cache çš„ä¸€ä¸ª token ä¸­<strong>è‡ªå·±è´Ÿè´£çš„ head</strong>.</li>
<li>â˜¢ï¸ ä¸€ä¸ª thread group è´Ÿè´£è®¡ç®—ä¸€ä¸ª qk å€¼; è¿™ä¸ªå€¼æ˜¾ç„¶æ˜¯ç”±ä¸€ä¸ª Q head å’Œä¸€ä¸ª K head ç‚¹ç§¯å¾—åˆ°çš„.</li>
<li><code>physical_block_offset</code> è¡¨ç¤ºå½“å‰è¦ç®—çš„ token åœ¨å½“å‰ PA block ä¸­çš„åç§»é‡ (æ³¨æ„å’Œå‰é¢çš„ <code>physical_block_number</code> åŒºåˆ†).</li>
<li>åŠ  <code>i * WARP_SIZE</code> çš„åŸå› æ˜¯å¦‚æœ <code>BLOCK_SIZE</code> å¤§äº 32, é‚£ä¹ˆä¸€ä¸ª warp è¦å¤šæ¬¡å¾ªç¯æ‰èƒ½å¤„ç†å®Œä¸€ä¸ª PA block ä¸­çš„æ‰€æœ‰ token, å¯¹åº” <code>thread_group_idx</code> éœ€è¦åšåç§».</li>
<li><code>token_idx</code> è¡¨ç¤ºå½“å‰è¦ç®—çš„ token åœ¨æ•´ä¸ª seq çš„ KV cache ä¸­çš„ç´¢å¼•.</li>
<li><code>k_vecs</code> ä¸­èƒ½å­˜æ”¾ <code>NUM_VECS_PER_THREAD</code> ä¸ª VEC, è€Œä¸€æ•´ä¸ª thread group ä¸­æ‰€æœ‰çš„ thread çš„ <code>k_vecs</code> åˆèµ·æ¥æ‰èƒ½ç»„æˆä¸€ä¸ª K çš„ head (æ¨å¯¼å‚è€ƒä¸Šé¢ Q çš„ ğŸ˜‡). è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåé¢ç®— QK çš„æ—¶å€™è¦ reduce.</li>
</ul>
<p>ğŸ¤” <strong>çœ‹åˆ°è¿™é‡Œè¯»è€…å¯èƒ½æœ‰ä¸€ä¸ªé—®é¢˜: ä¸€ä¸ª token çš„ K cache åº”è¯¥å¯¹åº”å¤šä¸ª head, ä¸ºä»€ä¹ˆä¸Šé¢è¯´ä¸€ä¸ª thread group åªè´Ÿè´£ä¸€ä¸ª head?</strong><br>
ç­”: å› ä¸ºå®é™…è®¡ç®—çš„æ—¶å€™, ä¸€ä¸ª cuda block åªè´Ÿè´£è®¡ç®—ä¸€ä¸ª head, å¯¹åº”åˆ° K Cache ä¹ƒè‡³åé¢ V Cache çš„ä½ç½®ä¹Ÿæ˜¯ä¸€æ ·çš„.</p>
<blockquote>
<p>è¿™é‡Œé¢å¤–è¯´ä¸€ä¸‹, è¯» K çš„ head çš„ä¸€ä¸ªç›®æ ‡åº”è¯¥æ˜¯åœ¨å°½é‡å°‘çš„ register ä¸­è£…ä¸‹ä¸€ä¸ª head çš„æ‰€æœ‰å…ƒç´ , è¿™æ ·åç»­å’Œ shared memory ä¸­çš„ Q åšç‚¹ä¹˜å¹¶è§„çº¦çš„é€Ÿåº¦æ›´å¿«. å‡è®¾ä¸€ä¸ª head æœ‰ 128 ä¸ª float16, åˆ™å ç”¨ 256B, è€Œ A100 ä¸­ä¸€ä¸ª thread æœ€å¤šèƒ½æœ‰ 255 ä¸ª 32-bit register (ä¹Ÿå°±æ˜¯ 1020B), æ­¤æ—¶å¯ä»¥è®¤ä¸ºä¸€ä¸ª thread èƒ½è£…ä¸‹ä¸€ä¸ª head çš„æ‰€æœ‰å…ƒç´ .<br>
ä½†æ˜¯ç”±äºç›®å‰ PA kernel åœ¨ <code>BLOCK_SIZE</code> ä¸º 16 çš„æƒ…å†µä¸‹ <code>THREAD_GROUP_SIZE</code> ç­‰äº 2, å› æ­¤ä¸€ä¸ª thread åªä¼šè£…ä¸€ä¸ª head çš„ä¸€åŠå…ƒç´ , è¿™æ ·å¯èƒ½ä¼šå¯¼è‡´ register çš„ä½¿ç”¨ç‡ä¸é«˜.</p></blockquote>
<hr>
<p>æ¥ç€è¿›å…¥ç¬¬ä¸‰ä¸ªå¾ªç¯, ç›®çš„æ˜¯è®© thread group ä» K cache ä¸­è¯»ä¸€ä¸ª head, å¹¶å­˜å…¥ <code>k_vecs</code> ä¸­:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// x == THREAD_GROUP_SIZE * VEC_SIZE
</span></span></span><span class="line"><span class="cl"><span class="c1">// Each thread group fetches x elements from the key at a time.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">16</span> <span class="o">/</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">cache_t</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="c1">//...
</span></span></span><span class="line"><span class="cl"><span class="c1">// Loop 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Loop 2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">K_vec</span> <span class="n">k_vecs</span><span class="p">[</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Loop 3
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">const</span> <span class="n">cache_t</span><span class="o">*</span> <span class="n">k_ptr</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">                <span class="n">k_cache</span> <span class="o">+</span> <span class="n">physical_block_number</span> <span class="o">*</span> <span class="n">kv_block_stride</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">                <span class="n">kv_head_idx</span> <span class="o">*</span> <span class="n">kv_head_stride</span> <span class="o">+</span> <span class="n">physical_block_offset</span> <span class="o">*</span> <span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="k">const</span> <span class="kt">int</span> <span class="n">vec_idx</span> <span class="o">=</span> <span class="n">thread_group_offset</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="k">const</span> <span class="kt">int</span> <span class="n">offset1</span> <span class="o">=</span> <span class="p">(</span><span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">)</span> <span class="o">/</span> <span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="k">const</span> <span class="kt">int</span> <span class="n">offset2</span> <span class="o">=</span> <span class="p">(</span><span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">)</span> <span class="o">%</span> <span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// if Fp8KVCacheDataType::kAuto
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">k_vecs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="o">*</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="n">K_vec</span><span class="o">*&gt;</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">              <span class="n">k_ptr</span> <span class="o">+</span> <span class="n">offset1</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">offset2</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></div><p>è€è§„çŸ©, å…ˆçœ‹ <code>j</code>, æœ¬è´¨å°±æ˜¯ä» 0 è¿­ä»£åˆ° <code>NUM_VECS_PER_THREAD</code>, æ¯æ¬¡è¿­ä»£å½“å‰ thread è¯»å–ä¸€ä¸ª VEC å­˜å…¥ <code>k_vecs</code> ä¸­.</p>
<blockquote>
<p>ğŸ”” å›é¡¾:</p>
<ol>
<li><code>NUM_VECS_PER_THREAD</code> è¡¨ç¤ºä¸€ä¸ª head è¢«åˆ†æˆå¤šå°‘ä¸ª 16B.</li>
<li><code>k_cache</code> çš„ shape ä¸º <code>(num_blocks, num_kv_heads, head_size/x, block_size, x)</code>.</li>
</ol></blockquote>
<p>å…¶ä¸­çš„ <code>x</code> è¡¨ç¤ºä¸€ä¸ª thread group éœ€è¦è¯»å–çš„å…ƒç´ æ•°é‡ (<code>VEC_SIZE</code> * <code>THREAD_GROUP_SIZE</code>); å› æ­¤ä½œè€…å°† K Cache çš„ layout çš„æœ€åä¸€ç»´è®¾ç½®ä¸º <code>x</code> å…¶å®ä¹Ÿæ˜¯æ–¹ä¾¿åç»­ thread group å¯¹ K cache çš„è¯»å–.</p>
<p>ä¸‹å›¾å…·ä½“å±•ç¤ºäº†å¯»å€çš„è¿‡ç¨‹:</p>
<p><img alt="pa-cal-kq-03.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-cal-kq-03.png"></p>
<p>å…¶ä¸­:</p>
<ul>
<li>åœ¨ MHSA ä¸­, <code>num_kv_heads</code> ç­‰äº <code>num_heads</code>; è€Œåœ¨ GQA, MQA ä¸­, <code>num_kv_heads</code> å°äº <code>num_heads</code>.</li>
<li>(1) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread å±äºçš„ warp è¦å¤„ç†å“ªä¸ª PA block.</li>
<li>(2) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread è¦è®¡ç®—çš„ head åœ¨ K cache ä¸­çš„ä½ç½®. è¿™ä¸ª head çš„ç´¢å¼•å’Œ Q ä¸­ head çš„ç´¢å¼•åœ¨ MHSA ä¸­ç›¸åŒ.</li>
<li>(3) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread group è¦è®¡ç®—çš„ token åœ¨å½“å‰ PA block ä¸­çš„ä½ç½®.</li>
<li>(5) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread åœ¨éœ€è¦è¯»å–çš„ head (è“è‰²é•¿æ–¹ä½“) ä¸­ x çš„åç§», é€šè¿‡ <code>j</code> è¿›è¡Œè¿­ä»£è¯»å–. <strong>æ¯æ¬¡å¾ªç¯ thread group ä¸­çš„æ‰€æœ‰ thread å–ä¸€ä¸ª x.</strong></li>
<li>(6) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread åœ¨ thread gruop ä¸­è¯»å–çš„ x ä¸­ VEC çš„åç§»; thread ä¸€æ¬¡è¯»å–ä¸€ä¸ª VEC.</li>
</ul>
<p>ğŸ¤” <strong>ä¸ºä»€ä¹ˆ (5) åœ¨å®é™…å¯»å€æ—¶éœ€è¦ <code>* BLOCK_SIZE * x</code> ?</strong><br>
ç­”: è¿™æ˜¯æ ¹æ® <code>k_cache</code> çš„ layout å¾—åˆ°çš„ stride. åŒç† (3) <code>* x</code> ä¹Ÿæ˜¯ stride.</p>
<p>ç¬¬ 3 ä¸ªå¾ªç¯ç»“æŸæ—¶å½“å‰ warp è´Ÿè´£çš„æ¯ä¸ª token ä¸­éœ€è¦çš„ K cache head å·²ç»å…¨è¢«åŠ è½½å…¥ thread æœ¬åœ°çš„ <code>k_vecs</code> ä¸­äº†.</p>
<p>ç”±äºä¸€ä¸ª thread group çš„ <code>k_vecs</code> æ‰èƒ½çœŸæ­£ç»„æˆä¸€ä¸ª head, åœ¨é€€å›ç¬¬äºŒä¸ªå¾ªç¯è¿›è¡Œ QK dot çš„æ—¶å€™, éœ€è¦åšä¸ª reduction, å…·ä½“çš„èŒƒå›´å°±æ˜¯ <code>THREAD_GROUP_SIZE</code> ä¸ª thread:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Loop 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Loop 2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">K_vec</span> <span class="n">k_vecs</span><span class="p">[</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Loop 3
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="kt">float</span> <span class="n">qk</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">Qk_dot</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span> <span class="n">THREAD_GROUP_SIZE</span><span class="o">&gt;::</span><span class="n">dot</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                             <span class="n">q_vecs</span><span class="p">[</span><span class="n">thread_group_offset</span><span class="p">],</span> <span class="n">k_vecs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></div><p>è®¡ç®—å®Œ <code>qk</code> å, ç”±å½“å‰ thread group ä¸­ç¬¬ä¸€ä¸ª (offset ä¸º 0) çš„ thread å¯¹è‡ªå·±åˆšæ‰ç®—å‡ºæ¥çš„ <code>qk</code> è¿›è¡Œ mask, é¡ºä¾¿çœ‹çœ‹å¦‚æœæ²¡æœ‰ mask æ‰, æŠŠ <code>qk_max</code> èµ‹å€¼ä¸º <code>qk</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">thread_group_offset</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Store the partial reductions to shared memory.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// NOTE(woosuk): It is required to zero out the masked logits.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">bool</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">token_idx</span> <span class="o">&gt;=</span> <span class="n">seq_len</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">[</span><span class="n">token_idx</span> <span class="o">-</span> <span class="n">start_token_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">?</span> <span class="mf">0.f</span> <span class="o">:</span> <span class="n">qk</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Update the max value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">qk_max</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">?</span> <span class="nl">qk_max</span> <span class="p">:</span> <span class="n">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">qk</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>ğŸ§ <strong>ä¸ºä»€ä¹ˆè¦åš mask?</strong></p>
<ul>
<li>å› ä¸ºä¸€ä¸ª seq çš„æœ€åä¸€ä¸ª PA block å¯èƒ½è¦†ç›–ä¸æ»¡ <code>BLOCK_SIZE</code> ä¸ª token. è¿™é‡Œçš„ mask å°±æ˜¯æŠŠé‚£éƒ¨åˆ† qk ç½®é›¶.</li>
</ul>
<h3 id="54-softmax">5.4. Softmax</h3>
<p>æˆ‘å‹’ä¸ª QK å•Š, æ€»ç®—ç®—å®Œäº†, é”å…‹ five éƒ½è¦è¢«æŠ½æ¸…ä»“äº†. é¡µæ„ä¸çœŸ, é‰´å®šä¸ºå¼€ç®— softmax.</p>
<p>ä¸»è¦æ­¥éª¤å°±æ˜¯å¹¿æ’­ç„¶åç®—, ç®— softmax éœ€è¦çŸ¥é“æ¯ä¸ª head å¯¹åº”çš„ qk çš„æœ€å¤§å€¼. ç”±äºä¸€ä¸ª cuda block è´Ÿè´£çš„å°±æ˜¯ä¸€ä¸ª head, å¯¹äºè¿™ä¸ª head ä¸Šé¢çš„è®¡ç®—æ­¥éª¤ä¸€å…±ç®—äº† <code>cache_len</code>ä¸ª token çš„ qk, å› æ­¤éœ€è¦åšä¸€ä¸ª cuda block èŒƒå›´çš„è§„çº¦, æ‰¾åˆ°å…¶ä¸­æœ€å¤§çš„ qk å€¼.</p>
<p>å…ˆåœ¨ warp å±‚é¢è§„çº¦.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__shared__</span> <span class="kt">float</span> <span class="n">red_smem</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">NUM_WARPS</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="c1">// Perform reduction across the threads in the same warp to get the
</span></span></span><span class="line"><span class="cl"><span class="c1">// max qk value for each &#34;warp&#34; (not across the thread block yet).
</span></span></span><span class="line"><span class="cl"><span class="c1">// The 0-th thread of each thread group already has its max qk value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">WARP_SIZE</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">mask</span> <span class="o">&gt;=</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span> <span class="n">mask</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">qk_max</span> <span class="o">=</span> <span class="n">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">VLLM_SHFL_XOR_SYNC</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">mask</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">lane</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">red_smem</span><span class="p">[</span><span class="n">warp_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">qk_max</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">__syncthreads</span><span class="p">();</span>
</span></span></code></pre></div><ul>
<li><code>red_smem</code> æ˜¯ä¹‹å‰ç”³è¯·çš„ shared memory.</li>
<li><code>VLLM_SHFL_XOR_SYNC</code> æ˜¯ä¸€ä¸ª warp å†…çš„ shuffle æ“ä½œ, å…·ä½“æ¥è¯´, åœ¨æ¯æ¬¡å¾ªç¯æ—¶, æ¯ä¸ª thread å’Œè‡ªå·±ç›¸è· <code>mask</code> ä½ç½®çš„çº¿ç¨‹äº¤æ¢æ•°æ® (äº¤æ¢æ¥çš„æ•°æ®é€šè¿‡ <code>fmaxf</code> æ¯”è¾ƒ), å¹¶ä¸” <code>mask</code> ä¼šé€æ¸å‡åŠ, ç›´åˆ° <code>THREAD_GROUP_SIZE</code> ä¸ºæ­¢.</li>
<li><code>lane</code> è¡¨ç¤ºå½“å‰ warp ä¸­çš„çº¿ç¨‹ç´¢å¼•.</li>
</ul>
<p>æ¥ç€å†å¯¹æ¯ä¸ª warp çš„æœ€å¤§å€¼è¿›è¡Œè§„çº¦, ç”±äºæ¯ä¸ª warp çš„æœ€å¤§å€¼éƒ½è¢«å­˜å…¥äº† <code>red_smem</code> ä¸­, æ‰€ä»¥åªéœ€è¦å†æ¬¡è¿›è¡Œ shuffle æ“ä½œå³å¯.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// TODO(woosuk): Refactor this part.
</span></span></span><span class="line"><span class="cl"><span class="c1">// Get the max qk value for the sequence.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">qk_max</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">&lt;</span> <span class="n">NUM_WARPS</span> <span class="o">?</span> <span class="n">red_smem</span><span class="p">[</span><span class="n">lane</span><span class="p">]</span> <span class="o">:</span> <span class="o">-</span><span class="n">FLT_MAX</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">NUM_WARPS</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">mask</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">mask</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">qk_max</span> <span class="o">=</span> <span class="n">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">VLLM_SHFL_XOR_SYNC</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">mask</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>æ­¤æ—¶, ç¬¬ 1 ä¸ªçº¿ç¨‹çš„ <code>qk_max</code> å°±æ˜¯å½“å‰ cuda block ä¸­æ‰€æœ‰ warp ä¸­æœ€å¤§çš„ qk å€¼. å°†å…¶å¹¿æ’­ç»™æ‰€æœ‰çº¿ç¨‹:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Broadcast the max qk value to all threads.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">qk_max</span> <span class="o">=</span> <span class="n">VLLM_SHFL_SYNC</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
</span></span></code></pre></div><p>åœ¨è·å¾—äº† <code>qk_max</code> å, å°±å¯ä»¥è®¡ç®— softmax äº†:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Get the sum of the exp values.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">float</span> <span class="n">exp_sum</span> <span class="o">=</span> <span class="mf">0.f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_tokens</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREADS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">__expf</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">qk_max</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">exp_sum</span> <span class="o">+=</span> <span class="n">val</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">exp_sum</span> <span class="o">=</span> <span class="n">block_sum</span><span class="o">&lt;</span><span class="n">NUM_WARPS</span><span class="o">&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="n">red_smem</span><span class="p">[</span><span class="n">NUM_WARPS</span><span class="p">],</span> <span class="n">exp_sum</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Compute softmax.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">float</span> <span class="n">inv_sum</span> <span class="o">=</span> <span class="n">__fdividef</span><span class="p">(</span><span class="mf">1.f</span><span class="p">,</span> <span class="n">exp_sum</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="n">f</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_tokens</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREADS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*=</span> <span class="n">inv_sum</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">__syncthreads</span><span class="p">();</span>
</span></span></code></pre></div><h3 id="55-lv-logits--value">5.5. LV (Logits * Value)</h3>
<p><img alt="pa-cal.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-cal.png"></p>
<p>ä¸Šå›¾å±•ç¤ºäº† LV çš„è®¡ç®—è¿‡ç¨‹, ä¸»è¦åŒºåˆ«æ˜¯ç”±äºè¦è®¡ç®— Logits çš„ shape å¯ä»¥è¡¨ç¤ºä¸º <code>(num_heads, num_seqs, cache_len)</code>, è€Œ V çš„ shape å¯ä»¥è¡¨ç¤ºä¸º <code>(num_heads, cache_len, head_size)</code>, å› æ­¤ LV çš„çŸ©é˜µä¹˜æ³•ä¸­, æ¯è®¡ç®—ä¸€ä¸ªå…ƒç´ éœ€è¦è¯»å– logits çš„ä¸€è¡Œå’Œ V çš„ä¸€åˆ—è¿›è¡Œè®¡ç®—.</p>
<p>æ­¤æ—¶, ä¸€ä¸ª cuda block çš„èŒè´£ä» &ldquo;è‡ª Q ä¸­è¯»å–ä¸€ä¸ª head&rdquo; è½¬å˜ä¸º &ldquo;è®¡ç®— output ä¸­çš„ä¸€ä¸ª head&rdquo;.</p>
<p>ğŸ§ <strong>ä¸ºä»€ä¹ˆåœ¨è®¡ç®— LV æ—¶, å»æ‰äº† thread group çš„æ¦‚å¿µ, æ¯ä¸ª thread éƒ½è¢«è®¾å®šä¸ºæ¯æ¬¡è¯»å– 16B?</strong></p>
<ul>
<li>å› ä¸ºç°åœ¨æ¯è®¡ç®—ä¸€ä¸ªå…ƒç´ , éœ€è¦çš„è®¿å­˜é‡æ›´å¤§, å› æ­¤ç»™æ¯ä¸ª thread åˆ†é…äº†æ›´å¤šçš„æ•°æ®è¯»å–é‡. ä¹Ÿå°±æ˜¯è¯´, <code>V_VEC_SIZE</code> æ¯” <code>VEC_SIZE</code> æ›´å¤§.</li>
</ul>
<p>ç”±äº cuda è®¿å­˜æ¨¡å¼æŒ‰è¡Œè¯»å–æ›´å¿«, æ‰€ä»¥å®é™…çš„è®¡ç®—ç»“æœåœ¨éå† PA block æ—¶çº¿ç¨‹å†…éƒ¨åˆ©ç”¨ <code>accs</code> è¿›è¡Œç´¯è®¡ (ä»¥å®ç°ä¸ V çš„ä¸€åˆ—è¿›è¡Œè®¡ç®—çš„è¡Œä¸º):</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">=</span> <span class="n">BLOCK_SIZE</span> <span class="o">/</span> <span class="n">V_VEC_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_ROWS_PER_ITER</span> <span class="o">=</span> <span class="n">WARP_SIZE</span> <span class="o">/</span> <span class="n">NUM_V_VECS_PER_ROW</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_ROWS_PER_THREAD</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">    <span class="n">DIVIDE_ROUND_UP</span><span class="p">(</span><span class="n">HEAD_SIZE</span><span class="p">,</span> <span class="n">NUM_ROWS_PER_ITER</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// NOTE(woosuk): We use FP32 for the accumulator for better accuracy.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">float</span> <span class="n">accs</span><span class="p">[</span><span class="n">NUM_ROWS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">V_vec</span> <span class="n">v_vec</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">V_VEC_SIZE</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Load V to `v_vec` ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">v_vec_ptr</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">token_idx</span> <span class="o">+</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">seq_len</span> <span class="o">?</span> <span class="n">v_vec_ptr</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">:</span> <span class="n">zero_value</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Accumulate the dot product.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">dot</span><span class="p">(</span><span class="n">logits_vec</span><span class="p">,</span> <span class="n">v_vec</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>ç”±äºæ¯ä¸ªçº¿ç¨‹è´Ÿè´£çš„ç´¯è®¡éƒ¨åˆ†ä¸æ»¡ä¸€æ•´è¡Œ/åˆ—, æ‰€ä»¥è¿›è¡Œè§„çº¦:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Perform reduction within each warp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">mask</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">mask</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">acc</span> <span class="o">+=</span> <span class="n">VLLM_SHFL_XOR_SYNC</span><span class="p">(</span><span class="n">acc</span><span class="p">,</span> <span class="n">mask</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">acc</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// NOTE(woosuk): A barrier is required because the shared memory space for
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// logits is reused for the output.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Perform reduction across warps.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">float</span><span class="o">*</span> <span class="n">out_smem</span> <span class="o">=</span> <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">shared_mem</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">NUM_WARPS</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">mid</span> <span class="o">=</span> <span class="n">i</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Upper warps write to shared memory.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">warp_idx</span> <span class="o">&gt;=</span> <span class="n">mid</span> <span class="o">&amp;&amp;</span> <span class="n">warp_idx</span> <span class="o">&lt;</span> <span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="kt">float</span><span class="o">*</span> <span class="n">dst</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">out_smem</span><span class="p">[(</span><span class="n">warp_idx</span> <span class="o">-</span> <span class="n">mid</span><span class="p">)</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">const</span> <span class="kt">int</span> <span class="n">row_idx</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">/</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">NUM_ROWS_PER_ITER</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">HEAD_SIZE</span> <span class="o">&amp;&amp;</span> <span class="n">lane</span> <span class="o">%</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="n">dst</span><span class="p">[</span><span class="n">row_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Lower warps update the output.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">warp_idx</span> <span class="o">&lt;</span> <span class="n">mid</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">src</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">out_smem</span><span class="p">[</span><span class="n">warp_idx</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">const</span> <span class="kt">int</span> <span class="n">row_idx</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">/</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">NUM_ROWS_PER_ITER</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">HEAD_SIZE</span> <span class="o">&amp;&amp;</span> <span class="n">lane</span> <span class="o">%</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">src</span><span class="p">[</span><span class="n">row_idx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span></code></pre></div><p>æœ€åå†™å…¥åˆ°è¾“å‡ºä¸­:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl">  <span class="c1">// Write the final output.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="n">warp_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">scalar_t</span><span class="o">*</span> <span class="n">out_ptr</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">+</span> <span class="n">seq_idx</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">max_num_partitions</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">        <span class="n">head_idx</span> <span class="o">*</span> <span class="n">max_num_partitions</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span> <span class="o">+</span> <span class="n">partition_idx</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">const</span> <span class="kt">int</span> <span class="n">row_idx</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">/</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">NUM_ROWS_PER_ITER</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="p">(</span><span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">HEAD_SIZE</span> <span class="o">&amp;&amp;</span> <span class="n">lane</span> <span class="o">%</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">from_float</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">out_ptr</span> <span class="o">+</span> <span class="n">row_idx</span><span class="p">),</span> <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span></code></pre></div>]]></content:encoded></item></channel></rss>