<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Dive into Paged Attention | ç§‹æ°´Â·JamesNULLiu</title><meta name=keywords content="vllm,continuous-batching,paged-attention"><meta name=description content="Dive into the paged attention mechanism of vLLM."><meta name=author content="jamesnulliu"><link rel=canonical href=https://jamesnulliu.github.io/zh/blogs/dive-into-paged-attention/><link crossorigin=anonymous href=/assets/css/stylesheet.62cb9c488bb33c0e9a9d3c29b7f4259cbb0db25aaa19ba672188203d3d5bcaf9.css integrity="sha256-YsucSIuzPA6anTwpt/QlnLsNslqqGbpnIYggPT1byvk=" rel="preload stylesheet" as=style><link rel=icon href=https://jamesnulliu.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://jamesnulliu.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://jamesnulliu.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://jamesnulliu.github.io/apple-touch-icon.png><link rel=mask-icon href=https://jamesnulliu.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://jamesnulliu.github.io/blogs/dive-into-paged-attention/><link rel=alternate hreflang=zh href=https://jamesnulliu.github.io/zh/blogs/dive-into-paged-attention/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]]}}</script><meta property="og:url" content="https://jamesnulliu.github.io/zh/blogs/dive-into-paged-attention/"><meta property="og:site_name" content="ç§‹æ°´Â·JamesNULLiu"><meta property="og:title" content="Dive into Paged Attention"><meta property="og:description" content="Dive into the paged attention mechanism of vLLM."><meta property="og:locale" content="zh"><meta property="og:type" content="article"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2024-10-07T12:00:00+08:00"><meta property="article:modified_time" content="2025-09-12T22:39:56+00:00"><meta property="article:tag" content="Python"><meta property="article:tag" content="Vllm"><meta property="article:tag" content="Attention"><meta name=twitter:card content="summary"><meta name=twitter:title content="Dive into Paged Attention"><meta name=twitter:description content="Dive into the paged attention mechanism of vLLM."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://jamesnulliu.github.io/zh/blogs/"},{"@type":"ListItem","position":2,"name":"Dive into Paged Attention","item":"https://jamesnulliu.github.io/zh/blogs/dive-into-paged-attention/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Dive into Paged Attention","name":"Dive into Paged Attention","description":"Dive into the paged attention mechanism of vLLM.","keywords":["vllm","continuous-batching","paged-attention"],"articleBody":"1. è¯æ˜ Attention çš„ $O_i$ åªä¸ $Q_i$ æœ‰å…³ Attention çš„å…¬å¼å¦‚ä¸‹:\n$$ O=Attention(Q,K,V)=softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$å‡è®¾ $Q=\\begin{bmatrix}Q_0\\\\Q_1\\end{bmatrix}$, $K=\\begin{bmatrix}K_0\\\\K_1\\end{bmatrix}$\né‚£ä¹ˆ:\n$$ O=softmax(\\frac{\\begin{bmatrix}Q_0K_0^T\u0026Q_0K_1^T\\\\Q_1K_0^T\u0026Q_1K_1^T\\end{bmatrix}}{\\sqrt{d_k}})V $$ä»¤:\n$$ A=\\begin{bmatrix}A_0\\\\A_1\\end{bmatrix}=\\begin{bmatrix}Q_0K_0^T\u0026Q_0K_1^T\\\\Q_1K_0^T\u0026Q_1K_1^T\\end{bmatrix},f(x)=\\frac{softmax(x)}{\\sqrt{d_k}} $$æ­¤æ—¶, $A_1$ åªå’Œ $Q_1$ æœ‰å…³, å’Œ $Q_0$ æ— å…³, é‚£ä¹ˆ:\n$$ \\begin{bmatrix}O_0\\\\O_1\\end{bmatrix}=O=\\begin{bmatrix}f(A_0)\\\\f(A_1)\\end{bmatrix}V=\\begin{bmatrix}f(A_0)V\\\\f(A_1)V\\end{bmatrix} $$å› æ­¤, $O_i$ åªå’Œ $A_i$ ç›¸å…³, è€Œæ ¹æ® $A$ çš„è®¾å®š, $A_i$ åªå’Œ $Q_i$ ç›¸å…³, å³:\nAttention çŸ©é˜µçš„ç¬¬ $i$ ä¸ªè¾“å‡ºåªå’Œç¬¬ $i$ ä¸ª $Q$ æœ‰å…³, å’Œä¹‹å‰çš„ $Q$ æ— å…³.\næ€»ç»“:\nåœ¨é¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶ï¼Œåªéœ€å¯¹æ–° token è®¡ç®—å¯¹åº”çš„ Q_newï¼Œå¹¶ä¸ä¹‹å‰å·²ç»ç¼“å­˜çš„ K_cache å’Œ V_cache è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚ æ–°çš„ K_new å’Œ V_new ä¼šè¢«åŠ å…¥åˆ°ç¼“å­˜ä¸­ï¼Œç»§ç»­ä¸ºä¸‹ä¸€ä¸ª token ç”Ÿæˆæä¾›åŸºç¡€ã€‚ æ•´ä¸ªè¿‡ç¨‹é¿å…äº†å¯¹æ‰€æœ‰å†å² token çš„é‡å¤è®¡ç®—ï¼Œå¤§å¹…æé«˜äº†æ•ˆç‡ã€‚ 2. KV Cache çš„å¢é‡è¿‡ç¨‹ 2.1. åˆå§‹è¾“å…¥ï¼ˆå®Œæ•´åºåˆ—ï¼‰è®¡ç®—ï¼š å¯¹äºåˆå§‹çš„è¾“å…¥åºåˆ— (seq_len, embed_dim)ï¼Œæˆ‘ä»¬é€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ° Qã€K å’Œ Vï¼Œå®ƒä»¬çš„å½¢çŠ¶éƒ½æ˜¯ (seq_len, embed_dim)ã€‚ ä½¿ç”¨ Q å’Œ K è¿›è¡Œç‚¹ç§¯è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œç„¶åç»“åˆ V è®¡ç®—å¾—åˆ°è¾“å‡º (seq_len, embed_dim)ï¼Œè¿™æ˜¯ç¬¬ä¸€æ¬¡å¯¹åˆå§‹åºåˆ—çš„å®Œæ•´è®¡ç®—ã€‚ 2.2. é¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶çš„å¢é‡è®¡ç®—ï¼š åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶ï¼Œä¸éœ€è¦å¯¹æ•´ä¸ªåºåˆ—å†è¿›è¡Œå®Œæ•´çš„ Qã€Kã€V è®¡ç®—ï¼Œè€Œæ˜¯åªéœ€å¯¹æ–°ç”Ÿæˆçš„ token è¿›è¡Œä¸€æ¬¡å¢é‡è®¡ç®—ã€‚è¿™æ—¶çš„æ“ä½œæµç¨‹å¦‚ä¸‹ï¼š\nè¾“å…¥æ–°çš„ tokenï¼šå°†å·²ç»ç”Ÿæˆçš„ tokenï¼ˆå…¶å½¢çŠ¶ä¸º (embed_dim,)ï¼‰ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ°è¯¥ token å¯¹åº”çš„ Q_newï¼Œå½¢çŠ¶ä¸º (embed_dim,)ã€‚ ä¸ä¹‹å‰ç¼“å­˜çš„ K å’Œ V è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼šä½¿ç”¨ Q_new ä¸ä¹‹å‰å·²ç»è®¡ç®—å¹¶ç¼“å­˜çš„ K_cache å’Œ V_cache è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚è¿™é‡Œçš„ K_cache å’Œ V_cache åˆ†åˆ«æ˜¯ä¹‹å‰æ¯æ¬¡ç”Ÿæˆ token æ—¶å¾—åˆ°çš„ K å’Œ Vï¼Œå®ƒä»¬çš„å½¢çŠ¶æ˜¯ (seq_len, embed_dim)ï¼Œå³ç¼“å­˜äº†ä»æœ€åˆè¾“å…¥åºåˆ—åˆ°å½“å‰å·²ç»ç”Ÿæˆçš„æ‰€æœ‰ token çš„ K å’Œ Vã€‚Q_new å¯ä»¥ç›´æ¥ä¸ K_cache è¿›è¡Œç‚¹ç§¯ï¼Œå¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°ï¼Œç„¶åç»“åˆ V_cache å¾—åˆ°æ–°çš„è¾“å‡ºã€‚ æ›´æ–° KV Cacheï¼šæ–°çš„ K_new å’Œ V_new ä¼šé€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ°ï¼ˆå½¢çŠ¶ä¸º (embed_dim,)ï¼‰ï¼Œå¹¶å°†å®ƒä»¬æ·»åŠ åˆ° K_cache å’Œ V_cache çš„æœ«å°¾ï¼Œä½¿å¾—ç¼“å­˜çš„ K_cache å’Œ V_cache ä¸æ–­å¢å¤§ï¼Œä»¥å¤‡åç»­ä½¿ç”¨ã€‚ è¾“å‡ºï¼šé€šè¿‡æ³¨æ„åŠ›è®¡ç®—åçš„è¾“å‡ºå½¢çŠ¶ä¸º (embed_dim,)ï¼Œå³æ–°ç”Ÿæˆçš„ tokenã€‚ 4. vllm ä¸­çš„ Paged Attention 4.1. åŠ¨æœº: Memory Wastes ä¸Šå›¾å±•ç¤ºäº†å¯èƒ½çš„å†…å­˜æµªè´¹æƒ…å†µ, ä¸»è¦æ—¶è¾“å…¥ sequence ä¸çŸ¥é“ eos åœ¨å“ªé‡Œ, å¦‚æœéšæœºç”³è¯·å†…å­˜, å¯èƒ½å¯¼è‡´å¤§é‡å†…å­˜ç¢ç‰‡, å› æ­¤ååé‡ä¸‹é™.\n4.2. è§£å†³æ–¹æ¡ˆ: ç”¨ Page ç®¡ç†å†…å­˜ ä¸Šå›¾å±•ç¤ºäº† vLLM ç”¨ Paged ç®¡ç†å†…å­˜å…·ä½“æ€ä¹ˆåšçš„.\nç®€å•æ¥è¯´, vLLM åœ¨å¼€å§‹æ¨ç†å‰ä¸ºæ¯ä¸ª Decoder Layer ç”³è¯·ä¸¤ä¸ªå·¨é•¿çš„ Tensor (k_cache å’Œ v_cache), æŠŠ Tensor åˆ†å‰²æˆè¿ç»­ç­‰é•¿çš„ PA blocks (å›¾ä¸­çš„ä¸€è¡Œä¸ºä¸€ä¸ª PA Block); æ¯ä¸ª PA Block èƒ½å¤Ÿå­˜æ”¾ BLOCK_SIZE ä¸ª token çš„ K æˆ– V cache (æ¯ä¸ª cache çš„å½¢çŠ¶å¯ä»¥ç†è§£ä¸º (num_heads, head_size)).\nå› æ­¤, k_cache å’Œ v_cache çš„å½¢çŠ¶å¯ä»¥ç†è§£ä¸º (num_blocks, num_heads, head_size).\nå¯¹äºä¸€ä¸ªè¿ç»­çš„ sequnce, åœ¨ prefill é˜¶æ®µå‰å°±ä¼šåˆ†é…å¥½å®ƒçš„ PA blocks, ä¹‹åæ¨ç†æ—¶:\nè‹¥æ˜¯è®¡ç®— prompt çš„ Attention, åˆ™å…ˆæŠŠä¼ å…¥çš„ K å’Œ V æŒ‰ç…§ PA blocks å­˜å…¥ k_cache å’Œ v_cache ä¸­; ç„¶ååˆ©ç”¨æ•´æ®µçš„ QKV è®¡ç®— attention. è‹¥æ˜¯è®¡ç®—æ–° token, åˆ™åˆ©ç”¨ Q å’Œ block table è®¡ç®— decode é˜¶æ®µçš„ attntion; æ­¤æ—¶è®¿å­˜çš„å°±æ˜¯ k_cache å’Œ v_cache ä¸­çš„ PA blocks. 5. Paged Attention Kernel è¯¦è§£ References:\nvLLM Paged Attention vLLMçš‡å† ä¸Šçš„æ˜ç ï¼šæ·±å…¥æµ…å‡ºç†è§£PagedAttention CUDAå®ç° å…ˆçœ‹ä¸‹æ•´ä½“è®¡ç®—æµç¨‹å›¾ (è¿™ä¸ªå›¾åé¢ä¹Ÿä¼šå‡ºç°è¿™é‡Œå…ˆçœ‹ä¸€çœ¼):\n5.1. è¾“å…¥è¾“å‡ºè¾“å‡ºåˆ†æå’Œå‚æ•°è¯´æ˜ // Grid: (num_heads, num_seqs, 1). template\u003c typename scalar_t, int HEAD_SIZE, int BLOCK_SIZE, int NUM_THREADS, int PARTITION_SIZE = 0\u003e __device__ void paged_attention_kernel( ... // Other side args. const scalar_t* __restrict__ out, // [num_seqs, num_heads, max_num_partitions, head_size] const scalar_t* __restrict__ q, // [num_seqs, num_heads, head_size] const scalar_t* __restrict__ k_cache, // [num_blocks, num_kv_heads, head_size/x, block_size, x] const scalar_t* __restrict__ v_cache, // [num_blocks, num_kv_heads, head_size, block_size] ... // Other side args. ) æ¨¡æ¿å‚æ•°è¯´æ˜:\nscalar_t å…ƒç´ ç±»å‹ (å®é™…ä»£ç ä¸­è¿˜æœ‰ cache_t è¡¨ç¤º KV cache çš„å…ƒç´ ç±»å‹). HEAD_SIZE æ¯ä¸ª head ä¸­å…ƒç´ æ•°é‡. BLOCK_SIZE æ¯ä¸ª PA block ä¸­çš„ token æ•°é‡. KV cache è¢«å­˜å‚¨åœ¨ä¸åŒ PA blocks. æ¯ä¸ª PA block å­˜å‚¨ä¸€ä¸ª head ä¸­ BLOCK_SIZE ä¸ª token.\nä¾‹å¦‚, è‹¥ BLOCK_SIZE=16, HEAD_SIZE=128, åˆ™ä¸€ä¸ª PA block èƒ½å­˜å‚¨ä¸€ä¸ª head çš„ 16 * 128 = 2048 ä¸ªå…ƒç´ . æ¯ä¸ª PA block å¯èƒ½åªåŒ…å«ä¸€éƒ¨åˆ†çš„ context tokens. ä» page è§’åº¦çœ‹, KV cache æ˜¯è‹¥å¹²ä¸ª page çš„é›†åˆ; NUM_THREADS æ¯ä¸ª CUDA thread block ä¸­ thread çš„æ•°é‡. PARTITION_SIZE å‚ä¸ TP çš„ GPU æ•°é‡, é»˜è®¤ 0 è¡¨ç¤ºå•å¡. (ä»¥ä¸‹éƒ½ä»¥å•å¡ä¸ºä¾‹è¯´æ˜) é¢å¤–çš„ä¸€äº›å‚æ•°:\nnum_seqs: æœ¬æ¬¡æ¨ç†è¯·æ±‚ sequence æ•°ç›®. ç”±äºè¿™ä¸ª kernel åªå¤„ç† decode é˜¶æ®µå• query attention, æ‰€ä»¥å®é™…ä¸Šæ¯ä¸ª sequence åªæœ‰ä¸€ä¸ª query token.\nnum_heads: Q çš„ head æ•°ç›® num_kv_heads: KV çš„ head æ•°ç›®, å¯¹äº MHA å…¶å€¼å’Œ num_heads ç›¸åŒ; å¦‚æœæ˜¯ GQA, MQA åˆ™ num_kv_heads å°äº num_head. head_size: å³ HEAD_SIZE k_cache: (num_blocks, num_kv_heads, head_size/x, block_size, x), å…¶ä¸­ x è¡¨ç¤º THREAD_GROUP_SIZE * VEC_SIZE çš„å¤§å° (åé¢ä¼šç»†è¯´). ä¸‹é¢ç»“åˆ GPU architecture åˆæ­¥åˆ†æä¸€ä¸‹å‚æ•°.\nğŸ§ ä¸ºä»€ä¹ˆè¦åˆ† thread group?\nå› ä¸ºå½“ä¸€ä¸ª cuda block è¦å–çš„æ•°æ®æ¯”è¾ƒå°‘çš„æ—¶å€™ (è®¡ç®— QK), ä¸€ä¸ª thread group åˆ†åˆ«ä¸€æ¬¡å– Q å’Œ K ä¸­ 16B; å½“ä¸€ä¸ª cuda block è¦å–çš„æ•°æ®æ¯”è¾ƒå¤šçš„æ—¶å€™ (è®¡ç®— LV), ä¸€ä¸ª thread å– 16B. 5.2.Shared Memory: q_vecs çš„å†™å…¥ ä» kernel ä¸­çš„ç¬¬ä¸€ä¸ªç”³è¯·çš„ shared memory å¼€å§‹è¯´.\nå…³äº shared memeory:\nåœ¨ kernel ä¸­ç”³è¯·çš„ shared memory è¢«å½“å‰ cuda block ä¸­çš„æ‰€æœ‰ thread å…±äº«. shared memory çš„ä½œç”¨æ˜¯ä¸ºäº†å‡å°‘ global memory çš„è®¿é—®æ¬¡æ•°ï¼Œæé«˜è®¿å­˜æ•ˆç‡. ä»¥ä¸‹ä»£ç ç”³è¯·äº†ä¸€å— shared memroy è¢«æ•´ä¸ª CUDA Block ä¸­æ‰€æœ‰ kernel å…±äº«:\n__shared__ Q_vec q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD]; é¦–å…ˆ, q_vecs è¦†ç›–äº† Q ä¸­ head_size ä¸ªå…ƒç´  - è¿™ä¹Ÿæ˜¯ä¸€ä¸ª cuda block éœ€è¦å¤„ç†çš„æ•°æ®é‡.\næ¥ç€å†è¯´ä¸¤ä¸ªç»´åº¦çš„å‚æ•°çš„æ„æ€:\nconstexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE; constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE; THREAD_GROUP_SIZE: æ¯ä¸ª thread group ä¸­çš„ thread æ•°é‡. æ³¨æ„, ä¸€ä¸ª cuda block ä¸­æœ‰ NUM_THREADS ä¸ª thread, NUM_THREAD_GROUPS ä¸ª thread group. THREAD_GROUP_SIZE = MAX(WARP_SIZE/BLOCK_SIZE, 1). NUM_VECS_PER_THREAD: HEAD_SIZE èƒ½è¢«åˆ†æˆå¤šå°‘ä¸ª 16B. (è¿™ä¸ªå˜é‡è¿™ä¹ˆå‘½åçš„ç†ç”±æ˜¯åé¢è¯»å– K çš„æ—¶å€™æ¯ä¸ª thread ä¼šå¾€è‡ªå·±çš„å¯„å­˜å™¨å†…è¯» NUM_VECS_PER_THREAD ä¸ª k_vec.) è¯æ˜: q_vecs è¦†ç›– Q çš„ä¸€ä¸ª head, å¹¶ä¸” NUM_VECS_PER_THREAD è¡¨ç¤º Q çš„ä¸€ä¸ª head è¢«åˆ†æˆå¤šå°‘ä¸ª 16B.\n=\u003e THREAD_GROUP_SIZE * VEC_SIZE = 16B / sizeof(scalar_t);\n=\u003e NUM_VECS_PER_THREAD * 16B / sizeof(scalar_t) = HEAD_SIZE;\nç„¶åçœ‹ load Q çš„ä»£ç , å»ºè®®ç»“åˆä¸‹é¢çš„å›¾ä¸€èµ·çœ‹:\n// Load Q to shmem #pragma unroll for (int i = thread_group_idx; i \u003c NUM_VECS_PER_THREAD; i += NUM_THREAD_GROUPS) { const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE; q_vecs[thread_group_offset][i] = *reinterpret_cast\u003cconst Q_vec*\u003e(q_ptr + vec_idx * VEC_SIZE); } thread_group_idx è¡¨ç¤ºå½“å‰ thread å±äºå½“å‰ cuda block ä¸­ç¬¬å‡ ä¸ª thread group. thread_group_offset è¡¨ç¤ºå½“å‰ thread åœ¨å½“å‰ thread group ä¸­æ˜¯ç¬¬å‡ ä¸ª thread. ä¸Šå›¾å±•ç¤ºäº†å¾ªç¯å…·ä½“æ˜¯æ€ä¹ˆè·‘çš„.\nä¸€ä¸ªç´«è‰²ç®­å¤´è¡¨ç¤ºä¸€ä¸ª thread group. NUM_VECS_PER_THREAD è¡¨ç¤º HEAD_SIZE èƒ½è¢«åˆ†æˆå¤šå°‘ä¸ª 16B. å®é™…è¯»å– Q çš„å†…å­˜æ—¶, æ‰€æœ‰ thread group ä» Q çš„èµ·å§‹ä½ç½®ç´§å¯†æ’åˆ—, æ ¹æ®å›¾ä¸Šçœ‹çš„è¯ä¸€å…±æœ‰ NUM_THREAD_GROUPS ä¸ªç´«è‰²ç®­å¤´. æ‰€æœ‰ thread group è¯»å–ä¸€æ¬¡ Q å¹¶å­˜å…¥ q_vecs å¯¹åº”å¾ªç¯ä¸­çš„ä¸€æ¬¡è¿­ä»£; å› æ­¤ä¸‹æ¬¡è¿­ä»£ thread group éœ€è¦å‘ååç§» NUM_THREAD_GROUPS ä¸ªä½ç½® (ä¾‹å¦‚ i ä» 1 å˜ä¸º 7). æ­¤å¤–, è¯»ä¸€æ¬¡ 16B å¯¹åº”ä¸€ä¸ª thread æ¥è¯´è‡ªç„¶ä¹Ÿæ˜¯å–ä¸€ä¸ª VEC. å¯¹åº”åˆ° kernel ç¼–å†™, è¿˜éœ€è¦è®¡ç®—å½“å‰ thread å…·ä½“è¯»å–å“ªä¸ª vec; å› æ­¤å¾—åˆ° vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE. ğŸ¤” è¿™é‡Œä¼šä¸ä¼šæœ‰ bank conflict?\næ€»ä¹‹ç°åœ¨æˆ‘ä»¬æŠŠ (1, head_size) å¤§å°çš„å…ƒç´ è¯»åˆ°äº† cuda block å…±äº«çš„ shared memory q_vecs ä¸­.\n5.3. è¯»å– K Cache å¹¶è®¡ç®— QK ç°åœ¨ä» cuda block çš„è§’åº¦çœ‹, å½“å‰ block å·²ç»è·å¾—äº†è‡ªå·±è¦ç®—çš„ Q ä¸­çš„ä¸€ä¸ª head (å½¢çŠ¶ä¸º (1, head_size)), æ¥ä¸‹æ¥å°±æ˜¯è®¡ç®— Q å’Œ K çš„ç‚¹ç§¯.\nç‚¹ç§¯è¿‡ç¨‹æ˜¯æŠŠå½“å‰ block æ‹¥æœ‰çš„ Q head å’Œæ•´ä¸ª K Cache (è¿­ä»£åœ°) è¿›è¡Œç‚¹ç§¯è¿ç®—. å‚è€ƒä¸‹å›¾:\nQK ä¹˜ç§¯å®é™…ä¸Šè¢«æš‚å­˜åœ¨ logits (ä¹Ÿæ˜¯ä¸€å— shared memory) ä¸­, ä¹‹åä¼šè¢«ç”¨æ¥è®¡ç®— softmax.\nğŸ˜‡ çœ‹ä¸‹å¾ªç¯çš„å…·ä½“ä»£ç å§:\n// Loop 1 for (int block_idx = start_block_idx + warp_idx; block_idx \u003c end_block_idx; block_idx += NUM_WARPS) { // Physical block calculation ... // Loop 2 for (int i = 0; i \u003c NUM_TOKENS_PER_THREAD_GROUP; i++) { // Offset calculation ... K_vec k_vecs[NUM_VECS_PER_THREAD]; // Loop 3 #pragma unroll for (int j = 0; j \u003c NUM_VECS_PER_THREAD; j++) { // Load K to `k_vecs` ... } float qk = scale * Qk_dot\u003cscalar_t, THREAD_GROUP_SIZE\u003e::dot( q_vecs[thread_group_offset], k_vecs); // Add the ALiBi bias if slopes are given. qk += (alibi_slope != 0) ? alibi_slope * (token_idx - seq_len + 1) : 0; if (thread_group_offset == 0) { // Store the partial reductions to shared memory. // Mask // Update the max value. } } } å…ˆè¯´ç¬¬ä¸€ä¸ªå¾ªç¯, å…¶ä¸­æ¯”è¾ƒé‡è¦çš„å‡ ä¸ªå‚æ•°å®šä¹‰å¦‚ä¸‹:\n// [start_block_idx, end_block_idx) is the range of blocks to process. const int start_block_idx = USE_PARTITIONING ? partition_idx * num_blocks_per_partition : 0; // If not using partitioning, `end_block_idx` should be equal to `num_seq_blocks`. const int end_block_idx = MIN(start_block_idx + num_blocks_per_partition, num_seq_blocks); // Number of blocks to process. const int num_blocks = end_block_idx - start_block_idx; ç”¨æ–‡å­—æè¿°å°±æ˜¯:\nblk_idx è¡¨ç¤ºå½“å‰ thread æ‰€åœ¨ warp éœ€è¦å¤„ç†çš„ PA block çš„åœ¨ block_table ä¸­ç´¢å¼• (é€»è¾‘ä¸Šçš„ç´¢å¼•). start_block_idx å’Œ end_block_idx è¡¨ç¤ºå½“å‰ cuda block éœ€è¦å¤„ç†çš„ block èŒƒå›´. num_blocks è¡¨ç¤ºå½“å‰ cuda block éœ€è¦å¤„ç†çš„ block æ•°é‡. NUM_WARPS è¡¨ç¤ºå½“å‰ cuda block ä¸­ warp çš„æ•°é‡. ä¸€ä¸ª warp åŒ…å« 32 ä¸ª thread. warp_idx è¡¨ç¤ºå½“å‰ warp åœ¨å½“å‰ cuda block ä¸­çš„ç´¢å¼•. è¯´äººè¯å°±æ˜¯æ¯ä¸ª warp å¤„ç†ä¸€ä¸ª PA block, ä¸€å¼€å§‹ cuda block ä¸­çš„æ‰€æœ‰ warp ç´§å¯†åœ°æŒ‡å‘æœ€å‰é¢çš„ NUM_WARPS ä¸ª PA block, æ¯æ¬¡å¾ªç¯æ‰€æœ‰ warp å‘ååç§» NUM_WARPS ä¸ª PA block çš„é•¿åº¦. å‚è€ƒä¸‹å›¾:\nğŸ”” è¿™é‡Œå†å›é¡¾ä¸€ä¸‹, ä¸€ä¸ª PA block é‡Œå­˜æ”¾äº† BLOCK_SIZE ä¸ª token çš„ K æˆ– V cache.\næ‰€ä»¥è¯´è¿™ä¸ªå¾ªç¯å’Œä¸Šé¢è¯»å– Q çš„å¾ªç¯ä¸€ä¸ªå°¿æ€§ğŸ¤®, ä¸è¿‡æ˜¯ä»¥ warp çš„ç²’åº¦å¤„ç†æ•°æ®;\nè¿›å…¥äº†ç¬¬ä¸€ä¸ªå¾ªç¯å†…éƒ¨, ç¬¬ä¸€æ­¥å½“ç„¶æ˜¯è®¡ç®—å½“å‰ thread å¯¹åº”çš„ warp åº”è¯¥è®¡ç®—å“ªä¸ª PA block (ç‰©ç†ä¸Šçš„ç´¢å¼•), å› æ­¤å¾—åˆ°äº† physical_block_number:\nconst int64_t physical_block_number = static_cast\u003cint64_t\u003e(block_table[block_idx]); ç„¶åè§£é‡Šç¬¬äºŒä¸ªå¾ªç¯, ç¬¬äºŒä¸ªå¾ªç¯çš„æ•´ä½“ç›®æ ‡å°±æ˜¯è®©å½“å‰ warp è®¡ç®—å¥½è‡ªå·±è´Ÿè´£çš„ PA block ä¸­ BLOCK_SIZE ä¸ª token çš„ QK ä¹˜ç§¯.\nå…ˆçœ‹ä¸€ä¸‹ i çš„ä¸Šç•Œ:\nconstexpr int NUM_TOKENS_PER_THREAD_GROUP = DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE); // ... // Loop 1 for (int block_idx = start_block_idx + warp_idx; block_idx \u003c end_block_idx; block_idx += NUM_WARPS) { // Loop 2 for (int i = 0; i \u003c NUM_TOKENS_PER_THREAD_GROUP; i++) { // ... } // ... } ä» kernel è§’åº¦çœ‹, æ¯ä¸ª thread éœ€è¦è¾…åŠ©å½“å‰ warp è®¡ç®—è‡ªå·±è´Ÿè´£çš„ä¸€æ•´ä¸ª PA block (åŒ…å« BLOCK_SIZE ä¸ª token), è€Œæˆ‘ä»¬æŠŠè¿™ä¸ªè¿‡ç¨‹æ‹†åˆ†ä¸º Loop 2 ä¸­çš„ NUM_TOKEN_PER_THREAD_GROUP (ä¹Ÿå°±æ˜¯ ceil(BLOCK_SIZE / 32)) æ¬¡å¾ªç¯;\nè¯´äººè¯å°±æ˜¯ä¸€ä¸ª thread group å¯¹åº”ä¸€ä¸ª token ä¸­çš„ä¸€ä¸ª head, å¦‚æœ BLOCK SIZE å¤ªå¤§äº†åé¢æ¯ä¸ª thread å‘ååç§» i * WARP_SIZE ä¸ª token ç»§ç»­ç‹ ç‹ ç®—ğŸ¤£.\nä¹Ÿå› æ­¤ç¬¬äºŒä¸ªå¾ªç¯å†…éƒ¨ä¸€ä¸Šæ¥å…ˆè®¡ç®—äº†å‡ ä¸ªåç§»é‡, å¹¶ä¸”ç”³è¯·äº† thread å†…éƒ¨ç§æœ‰çš„ k_vecs æ•°ç»„:\nconst int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE; const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset; K_vec k_vecs[NUM_VECS_PER_THREAD]; thread_group_idx è¡¨ç¤ºå½“å‰ thread group åœ¨æ•´ä¸ª cuda block ä¸­çš„ç´¢å¼•. â˜¢ï¸ ä¸€ä¸ª thread group åœ¨ä¸€æ¬¡å¾ªç¯ä¸­è´Ÿè´£ fetch ä¸€ä¸ª PA block ä¸­ K cache çš„ä¸€ä¸ª token ä¸­è‡ªå·±è´Ÿè´£çš„ head. â˜¢ï¸ ä¸€ä¸ª thread group è´Ÿè´£è®¡ç®—ä¸€ä¸ª qk å€¼; è¿™ä¸ªå€¼æ˜¾ç„¶æ˜¯ç”±ä¸€ä¸ª Q head å’Œä¸€ä¸ª K head ç‚¹ç§¯å¾—åˆ°çš„. physical_block_offset è¡¨ç¤ºå½“å‰è¦ç®—çš„ token åœ¨å½“å‰ PA block ä¸­çš„åç§»é‡ (æ³¨æ„å’Œå‰é¢çš„ physical_block_number åŒºåˆ†). åŠ  i * WARP_SIZE çš„åŸå› æ˜¯å¦‚æœ BLOCK_SIZE å¤§äº 32, é‚£ä¹ˆä¸€ä¸ª warp è¦å¤šæ¬¡å¾ªç¯æ‰èƒ½å¤„ç†å®Œä¸€ä¸ª PA block ä¸­çš„æ‰€æœ‰ token, å¯¹åº” thread_group_idx éœ€è¦åšåç§». token_idx è¡¨ç¤ºå½“å‰è¦ç®—çš„ token åœ¨æ•´ä¸ª seq çš„ KV cache ä¸­çš„ç´¢å¼•. k_vecs ä¸­èƒ½å­˜æ”¾ NUM_VECS_PER_THREAD ä¸ª VEC, è€Œä¸€æ•´ä¸ª thread group ä¸­æ‰€æœ‰çš„ thread çš„ k_vecs åˆèµ·æ¥æ‰èƒ½ç»„æˆä¸€ä¸ª K çš„ head (æ¨å¯¼å‚è€ƒä¸Šé¢ Q çš„ ğŸ˜‡). è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåé¢ç®— QK çš„æ—¶å€™è¦ reduce. ğŸ¤” çœ‹åˆ°è¿™é‡Œè¯»è€…å¯èƒ½æœ‰ä¸€ä¸ªé—®é¢˜: ä¸€ä¸ª token çš„ K cache åº”è¯¥å¯¹åº”å¤šä¸ª head, ä¸ºä»€ä¹ˆä¸Šé¢è¯´ä¸€ä¸ª thread group åªè´Ÿè´£ä¸€ä¸ª head?\nç­”: å› ä¸ºå®é™…è®¡ç®—çš„æ—¶å€™, ä¸€ä¸ª cuda block åªè´Ÿè´£è®¡ç®—ä¸€ä¸ª head, å¯¹åº”åˆ° K Cache ä¹ƒè‡³åé¢ V Cache çš„ä½ç½®ä¹Ÿæ˜¯ä¸€æ ·çš„.\nè¿™é‡Œé¢å¤–è¯´ä¸€ä¸‹, è¯» K çš„ head çš„ä¸€ä¸ªç›®æ ‡åº”è¯¥æ˜¯åœ¨å°½é‡å°‘çš„ register ä¸­è£…ä¸‹ä¸€ä¸ª head çš„æ‰€æœ‰å…ƒç´ , è¿™æ ·åç»­å’Œ shared memory ä¸­çš„ Q åšç‚¹ä¹˜å¹¶è§„çº¦çš„é€Ÿåº¦æ›´å¿«. å‡è®¾ä¸€ä¸ª head æœ‰ 128 ä¸ª float16, åˆ™å ç”¨ 256B, è€Œ A100 ä¸­ä¸€ä¸ª thread æœ€å¤šèƒ½æœ‰ 255 ä¸ª 32-bit register (ä¹Ÿå°±æ˜¯ 1020B), æ­¤æ—¶å¯ä»¥è®¤ä¸ºä¸€ä¸ª thread èƒ½è£…ä¸‹ä¸€ä¸ª head çš„æ‰€æœ‰å…ƒç´ .\nä½†æ˜¯ç”±äºç›®å‰ PA kernel åœ¨ BLOCK_SIZE ä¸º 16 çš„æƒ…å†µä¸‹ THREAD_GROUP_SIZE ç­‰äº 2, å› æ­¤ä¸€ä¸ª thread åªä¼šè£…ä¸€ä¸ª head çš„ä¸€åŠå…ƒç´ , è¿™æ ·å¯èƒ½ä¼šå¯¼è‡´ register çš„ä½¿ç”¨ç‡ä¸é«˜.\næ¥ç€è¿›å…¥ç¬¬ä¸‰ä¸ªå¾ªç¯, ç›®çš„æ˜¯è®© thread group ä» K cache ä¸­è¯»ä¸€ä¸ª head, å¹¶å­˜å…¥ k_vecs ä¸­:\n// x == THREAD_GROUP_SIZE * VEC_SIZE // Each thread group fetches x elements from the key at a time. constexpr int x = 16 / sizeof(cache_t); //... // Loop 1 for (int block_idx = start_block_idx + warp_idx; block_idx \u003c end_block_idx; block_idx += NUM_WARPS) { // Loop 2 for (int i = 0; i \u003c NUM_TOKENS_PER_THREAD_GROUP; i++) { K_vec k_vecs[NUM_VECS_PER_THREAD]; // Loop 3 for (int j = 0; j \u003c NUM_VECS_PER_THREAD; j++) { const cache_t* k_ptr = k_cache + physical_block_number * kv_block_stride + kv_head_idx * kv_head_stride + physical_block_offset * x; const int vec_idx = thread_group_offset + j * THREAD_GROUP_SIZE; const int offset1 = (vec_idx * VEC_SIZE) / x; const int offset2 = (vec_idx * VEC_SIZE) % x; // if Fp8KVCacheDataType::kAuto k_vecs[j] = *reinterpret_cast\u003cconst K_vec*\u003e( k_ptr + offset1 * BLOCK_SIZE * x + offset2); } // ... } // ... } è€è§„çŸ©, å…ˆçœ‹ j, æœ¬è´¨å°±æ˜¯ä» 0 è¿­ä»£åˆ° NUM_VECS_PER_THREAD, æ¯æ¬¡è¿­ä»£å½“å‰ thread è¯»å–ä¸€ä¸ª VEC å­˜å…¥ k_vecs ä¸­.\nğŸ”” å›é¡¾:\nNUM_VECS_PER_THREAD è¡¨ç¤ºä¸€ä¸ª head è¢«åˆ†æˆå¤šå°‘ä¸ª 16B. k_cache çš„ shape ä¸º (num_blocks, num_kv_heads, head_size/x, block_size, x). å…¶ä¸­çš„ x è¡¨ç¤ºä¸€ä¸ª thread group éœ€è¦è¯»å–çš„å…ƒç´ æ•°é‡ (VEC_SIZE * THREAD_GROUP_SIZE); å› æ­¤ä½œè€…å°† K Cache çš„ layout çš„æœ€åä¸€ç»´è®¾ç½®ä¸º x å…¶å®ä¹Ÿæ˜¯æ–¹ä¾¿åç»­ thread group å¯¹ K cache çš„è¯»å–.\nä¸‹å›¾å…·ä½“å±•ç¤ºäº†å¯»å€çš„è¿‡ç¨‹:\nå…¶ä¸­:\nåœ¨ MHSA ä¸­, num_kv_heads ç­‰äº num_heads; è€Œåœ¨ GQA, MQA ä¸­, num_kv_heads å°äº num_heads. (1) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread å±äºçš„ warp è¦å¤„ç†å“ªä¸ª PA block. (2) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread è¦è®¡ç®—çš„ head åœ¨ K cache ä¸­çš„ä½ç½®. è¿™ä¸ª head çš„ç´¢å¼•å’Œ Q ä¸­ head çš„ç´¢å¼•åœ¨ MHSA ä¸­ç›¸åŒ. (3) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread group è¦è®¡ç®—çš„ token åœ¨å½“å‰ PA block ä¸­çš„ä½ç½®. (5) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread åœ¨éœ€è¦è¯»å–çš„ head (è“è‰²é•¿æ–¹ä½“) ä¸­ x çš„åç§», é€šè¿‡ j è¿›è¡Œè¿­ä»£è¯»å–. æ¯æ¬¡å¾ªç¯ thread group ä¸­çš„æ‰€æœ‰ thread å–ä¸€ä¸ª x. (6) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread åœ¨ thread gruop ä¸­è¯»å–çš„ x ä¸­ VEC çš„åç§»; thread ä¸€æ¬¡è¯»å–ä¸€ä¸ª VEC. ğŸ¤” ä¸ºä»€ä¹ˆ (5) åœ¨å®é™…å¯»å€æ—¶éœ€è¦ * BLOCK_SIZE * x ?\nç­”: è¿™æ˜¯æ ¹æ® k_cache çš„ layout å¾—åˆ°çš„ stride. åŒç† (3) * x ä¹Ÿæ˜¯ stride.\nç¬¬ 3 ä¸ªå¾ªç¯ç»“æŸæ—¶å½“å‰ warp è´Ÿè´£çš„æ¯ä¸ª token ä¸­éœ€è¦çš„ K cache head å·²ç»å…¨è¢«åŠ è½½å…¥ thread æœ¬åœ°çš„ k_vecs ä¸­äº†.\nç”±äºä¸€ä¸ª thread group çš„ k_vecs æ‰èƒ½çœŸæ­£ç»„æˆä¸€ä¸ª head, åœ¨é€€å›ç¬¬äºŒä¸ªå¾ªç¯è¿›è¡Œ QK dot çš„æ—¶å€™, éœ€è¦åšä¸ª reduction, å…·ä½“çš„èŒƒå›´å°±æ˜¯ THREAD_GROUP_SIZE ä¸ª thread:\n// Loop 1 for (int block_idx = start_block_idx + warp_idx; block_idx \u003c end_block_idx; block_idx += NUM_WARPS) { // Loop 2 for (int i = 0; i \u003c NUM_TOKENS_PER_THREAD_GROUP; i++) { K_vec k_vecs[NUM_VECS_PER_THREAD]; // Loop 3 for (int j = 0; j \u003c NUM_VECS_PER_THREAD; j++) { // ... } float qk = scale * Qk_dot\u003cscalar_t, THREAD_GROUP_SIZE\u003e::dot( q_vecs[thread_group_offset], k_vecs); } // ... } è®¡ç®—å®Œ qk å, ç”±å½“å‰ thread group ä¸­ç¬¬ä¸€ä¸ª (offset ä¸º 0) çš„ thread å¯¹è‡ªå·±åˆšæ‰ç®—å‡ºæ¥çš„ qk è¿›è¡Œ mask, é¡ºä¾¿çœ‹çœ‹å¦‚æœæ²¡æœ‰ mask æ‰, æŠŠ qk_max èµ‹å€¼ä¸º qk:\nif (thread_group_offset == 0) { // Store the partial reductions to shared memory. // NOTE(woosuk): It is required to zero out the masked logits. const bool mask = token_idx \u003e= seq_len; logits[token_idx - start_token_idx] = mask ? 0.f : qk; // Update the max value. qk_max = mask ? qk_max : fmaxf(qk_max, qk); } ğŸ§ ä¸ºä»€ä¹ˆè¦åš mask?\nå› ä¸ºä¸€ä¸ª seq çš„æœ€åä¸€ä¸ª PA block å¯èƒ½è¦†ç›–ä¸æ»¡ BLOCK_SIZE ä¸ª token. è¿™é‡Œçš„ mask å°±æ˜¯æŠŠé‚£éƒ¨åˆ† qk ç½®é›¶. 5.4. Softmax æˆ‘å‹’ä¸ª QK å•Š, æ€»ç®—ç®—å®Œäº†, é”å…‹ five éƒ½è¦è¢«æŠ½æ¸…ä»“äº†. é¡µæ„ä¸çœŸ, é‰´å®šä¸ºå¼€ç®— softmax.\nä¸»è¦æ­¥éª¤å°±æ˜¯å¹¿æ’­ç„¶åç®—, ç®— softmax éœ€è¦çŸ¥é“æ¯ä¸ª head å¯¹åº”çš„ qk çš„æœ€å¤§å€¼. ç”±äºä¸€ä¸ª cuda block è´Ÿè´£çš„å°±æ˜¯ä¸€ä¸ª head, å¯¹äºè¿™ä¸ª head ä¸Šé¢çš„è®¡ç®—æ­¥éª¤ä¸€å…±ç®—äº† cache_lenä¸ª token çš„ qk, å› æ­¤éœ€è¦åšä¸€ä¸ª cuda block èŒƒå›´çš„è§„çº¦, æ‰¾åˆ°å…¶ä¸­æœ€å¤§çš„ qk å€¼.\nå…ˆåœ¨ warp å±‚é¢è§„çº¦.\n__shared__ float red_smem[2 * NUM_WARPS]; // ... // Perform reduction across the threads in the same warp to get the // max qk value for each \"warp\" (not across the thread block yet). // The 0-th thread of each thread group already has its max qk value. #pragma unroll for (int mask = WARP_SIZE / 2; mask \u003e= THREAD_GROUP_SIZE; mask /= 2) { qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask)); } if (lane == 0) { red_smem[warp_idx] = qk_max; } __syncthreads(); red_smem æ˜¯ä¹‹å‰ç”³è¯·çš„ shared memory. VLLM_SHFL_XOR_SYNC æ˜¯ä¸€ä¸ª warp å†…çš„ shuffle æ“ä½œ, å…·ä½“æ¥è¯´, åœ¨æ¯æ¬¡å¾ªç¯æ—¶, æ¯ä¸ª thread å’Œè‡ªå·±ç›¸è· mask ä½ç½®çš„çº¿ç¨‹äº¤æ¢æ•°æ® (äº¤æ¢æ¥çš„æ•°æ®é€šè¿‡ fmaxf æ¯”è¾ƒ), å¹¶ä¸” mask ä¼šé€æ¸å‡åŠ, ç›´åˆ° THREAD_GROUP_SIZE ä¸ºæ­¢. lane è¡¨ç¤ºå½“å‰ warp ä¸­çš„çº¿ç¨‹ç´¢å¼•. æ¥ç€å†å¯¹æ¯ä¸ª warp çš„æœ€å¤§å€¼è¿›è¡Œè§„çº¦, ç”±äºæ¯ä¸ª warp çš„æœ€å¤§å€¼éƒ½è¢«å­˜å…¥äº† red_smem ä¸­, æ‰€ä»¥åªéœ€è¦å†æ¬¡è¿›è¡Œ shuffle æ“ä½œå³å¯.\n// TODO(woosuk): Refactor this part. // Get the max qk value for the sequence. qk_max = lane \u003c NUM_WARPS ? red_smem[lane] : -FLT_MAX; #pragma unroll for (int mask = NUM_WARPS / 2; mask \u003e= 1; mask /= 2) { qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask)); } æ­¤æ—¶, ç¬¬ 1 ä¸ªçº¿ç¨‹çš„ qk_max å°±æ˜¯å½“å‰ cuda block ä¸­æ‰€æœ‰ warp ä¸­æœ€å¤§çš„ qk å€¼. å°†å…¶å¹¿æ’­ç»™æ‰€æœ‰çº¿ç¨‹:\n// Broadcast the max qk value to all threads. qk_max = VLLM_SHFL_SYNC(qk_max, 0); åœ¨è·å¾—äº† qk_max å, å°±å¯ä»¥è®¡ç®— softmax äº†:\n// Get the sum of the exp values. float exp_sum = 0.f; for (int i = thread_idx; i \u003c num_tokens; i += NUM_THREADS) { float val = __expf(logits[i] - qk_max); logits[i] = val; exp_sum += val; } exp_sum = block_sum\u003cNUM_WARPS\u003e(\u0026red_smem[NUM_WARPS], exp_sum); // Compute softmax. const float inv_sum = __fdividef(1.f, exp_sum + 1e-6f); for (int i = thread_idx; i \u003c num_tokens; i += NUM_THREADS) { logits[i] *= inv_sum; } __syncthreads(); 5.5. LV (Logits * Value) ä¸Šå›¾å±•ç¤ºäº† LV çš„è®¡ç®—è¿‡ç¨‹, ä¸»è¦åŒºåˆ«æ˜¯ç”±äºè¦è®¡ç®— Logits çš„ shape å¯ä»¥è¡¨ç¤ºä¸º (num_heads, num_seqs, cache_len), è€Œ V çš„ shape å¯ä»¥è¡¨ç¤ºä¸º (num_heads, cache_len, head_size), å› æ­¤ LV çš„çŸ©é˜µä¹˜æ³•ä¸­, æ¯è®¡ç®—ä¸€ä¸ªå…ƒç´ éœ€è¦è¯»å– logits çš„ä¸€è¡Œå’Œ V çš„ä¸€åˆ—è¿›è¡Œè®¡ç®—.\næ­¤æ—¶, ä¸€ä¸ª cuda block çš„èŒè´£ä» â€œè‡ª Q ä¸­è¯»å–ä¸€ä¸ª headâ€ è½¬å˜ä¸º â€œè®¡ç®— output ä¸­çš„ä¸€ä¸ª headâ€.\nğŸ§ ä¸ºä»€ä¹ˆåœ¨è®¡ç®— LV æ—¶, å»æ‰äº† thread group çš„æ¦‚å¿µ, æ¯ä¸ª thread éƒ½è¢«è®¾å®šä¸ºæ¯æ¬¡è¯»å– 16B?\nå› ä¸ºç°åœ¨æ¯è®¡ç®—ä¸€ä¸ªå…ƒç´ , éœ€è¦çš„è®¿å­˜é‡æ›´å¤§, å› æ­¤ç»™æ¯ä¸ª thread åˆ†é…äº†æ›´å¤šçš„æ•°æ®è¯»å–é‡. ä¹Ÿå°±æ˜¯è¯´, V_VEC_SIZE æ¯” VEC_SIZE æ›´å¤§. ç”±äº cuda è®¿å­˜æ¨¡å¼æŒ‰è¡Œè¯»å–æ›´å¿«, æ‰€ä»¥å®é™…çš„è®¡ç®—ç»“æœåœ¨éå† PA block æ—¶çº¿ç¨‹å†…éƒ¨åˆ©ç”¨ accs è¿›è¡Œç´¯è®¡ (ä»¥å®ç°ä¸ V çš„ä¸€åˆ—è¿›è¡Œè®¡ç®—çš„è¡Œä¸º):\nconstexpr int NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE; constexpr int NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW; constexpr int NUM_ROWS_PER_THREAD = DIVIDE_ROUND_UP(HEAD_SIZE, NUM_ROWS_PER_ITER); // NOTE(woosuk): We use FP32 for the accumulator for better accuracy. float accs[NUM_ROWS_PER_THREAD]; for (int block_idx = start_block_idx + warp_idx; block_idx \u003c end_block_idx; block_idx += NUM_WARPS) { V_vec v_vec; // ... for (int i = 0; i \u003c NUM_ROWS_PER_THREAD; i++) { // ... for (int j = 0; j \u003c V_VEC_SIZE; j++) { // Load V to `v_vec` ... v_vec_ptr[j] = token_idx + j \u003c seq_len ? v_vec_ptr[j] : zero_value; } // Accumulate the dot product. accs[i] += dot(logits_vec, v_vec); } } ç”±äºæ¯ä¸ªçº¿ç¨‹è´Ÿè´£çš„ç´¯è®¡éƒ¨åˆ†ä¸æ»¡ä¸€æ•´è¡Œ/åˆ—, æ‰€ä»¥è¿›è¡Œè§„çº¦:\n// Perform reduction within each warp. #pragma unroll for (int i = 0; i \u003c NUM_ROWS_PER_THREAD; i++) { float acc = accs[i]; #pragma unroll for (int mask = NUM_V_VECS_PER_ROW / 2; mask \u003e= 1; mask /= 2) { acc += VLLM_SHFL_XOR_SYNC(acc, mask); } accs[i] = acc; } // NOTE(woosuk): A barrier is required because the shared memory space for // logits is reused for the output. __syncthreads(); // Perform reduction across warps. float* out_smem = reinterpret_cast\u003cfloat*\u003e(shared_mem); #pragma unroll for (int i = NUM_WARPS; i \u003e 1; i /= 2) { int mid = i / 2; // Upper warps write to shared memory. if (warp_idx \u003e= mid \u0026\u0026 warp_idx \u003c i) { float* dst = \u0026out_smem[(warp_idx - mid) * HEAD_SIZE]; #pragma unroll for (int i = 0; i \u003c NUM_ROWS_PER_THREAD; i++) { const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER; if (row_idx \u003c HEAD_SIZE \u0026\u0026 lane % NUM_V_VECS_PER_ROW == 0) { dst[row_idx] = accs[i]; } } } __syncthreads(); // Lower warps update the output. if (warp_idx \u003c mid) { const float* src = \u0026out_smem[warp_idx * HEAD_SIZE]; #pragma unroll for (int i = 0; i \u003c NUM_ROWS_PER_THREAD; i++) { const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER; if (row_idx \u003c HEAD_SIZE \u0026\u0026 lane % NUM_V_VECS_PER_ROW == 0) { accs[i] += src[row_idx]; } } } __syncthreads(); } æœ€åå†™å…¥åˆ°è¾“å‡ºä¸­:\n// Write the final output. if (warp_idx == 0) { scalar_t* out_ptr = out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE + head_idx * max_num_partitions * HEAD_SIZE + partition_idx * HEAD_SIZE; #pragma unroll for (int i = 0; i \u003c NUM_ROWS_PER_THREAD; i++) { const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER; if (row_idx \u003c HEAD_SIZE \u0026\u0026 lane % NUM_V_VECS_PER_ROW == 0) { from_float(*(out_ptr + row_idx), accs[i]); } } } ","wordCount":"5628","inLanguage":"zh","datePublished":"2024-10-07T12:00:00+08:00","dateModified":"2025-09-12T22:39:56Z","author":[{"@type":"Person","name":"jamesnulliu"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://jamesnulliu.github.io/zh/blogs/dive-into-paged-attention/"},"publisher":{"@type":"Organization","name":"ç§‹æ°´Â·JamesNULLiu","logo":{"@type":"ImageObject","url":"https://jamesnulliu.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://jamesnulliu.github.io/zh/ accesskey=h title="ç§‹æ°´Â·JamesNULLiu (Alt + H)">ç§‹æ°´Â·JamesNULLiu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://jamesnulliu.github.io/ title=English aria-label=English>English</a></li></ul></div></div><ul id=menu><li><a href=https://jamesnulliu.github.io/zh/ title=Home><span>Home</span></a></li><li><a href=https://jamesnulliu.github.io/zh/about_me/ title="About Me"><span>About Me</span></a></li><li><a href=https://jamesnulliu.github.io/zh/blogs/ title=Blogs><span>Blogs</span></a></li><li><a href=https://jamesnulliu.github.io/zh/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://jamesnulliu.github.io/zh/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://jamesnulliu.github.io/zh/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://jamesnulliu.github.io/zh/search/ title=Search><span>Search</span></a></li><li><a href=https://jamesnulliu.github.io/zh/friends/ title=Friends><span>Friends</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://jamesnulliu.github.io/zh/>ä¸»é¡µ</a>&nbsp;Â»&nbsp;<a href=https://jamesnulliu.github.io/zh/blogs/>Blogs</a></div><h1 class="post-title entry-hint-parent">Dive into Paged Attention</h1><div class=post-description>Dive into the paged attention mechanism of vLLM.</div><div class=post-meta><span title='2024-10-07 12:00:00 +0800 +0800'>10æœˆ-07-2024</span>&nbsp;Â·&nbsp;12 åˆ†é’Ÿ&nbsp;Â·&nbsp;5628 å­—&nbsp;Â·&nbsp;jamesnulliu&nbsp;|&nbsp;è¯­è¨€:<ul class=i18n_list><li><a href=https://jamesnulliu.github.io/blogs/dive-into-paged-attention/>English</a></li></ul></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>ç›®å½•</span></summary><div class=inner><ul><li><a href=#1-%e8%af%81%e6%98%8e-attention-%e7%9a%84--%e5%8f%aa%e4%b8%8e--%e6%9c%89%e5%85%b3 aria-label="1. è¯æ˜ Attention çš„ $O_i$ åªä¸ $Q_i$ æœ‰å…³">1. è¯æ˜ Attention çš„ $O_i$ åªä¸ $Q_i$ æœ‰å…³</a></li><li><a href=#2-kv-cache-%e7%9a%84%e5%a2%9e%e9%87%8f%e8%bf%87%e7%a8%8b aria-label="2. KV Cache çš„å¢é‡è¿‡ç¨‹">2. KV Cache çš„å¢é‡è¿‡ç¨‹</a><ul><li><a href=#21-%e5%88%9d%e5%a7%8b%e8%be%93%e5%85%a5%e5%ae%8c%e6%95%b4%e5%ba%8f%e5%88%97%e8%ae%a1%e7%ae%97 aria-label="2.1. åˆå§‹è¾“å…¥ï¼ˆå®Œæ•´åºåˆ—ï¼‰è®¡ç®—ï¼š">2.1. åˆå§‹è¾“å…¥ï¼ˆå®Œæ•´åºåˆ—ï¼‰è®¡ç®—ï¼š</a></li><li><a href=#22-%e9%a2%84%e6%b5%8b%e4%b8%8b%e4%b8%80%e4%b8%aa-token-%e6%97%b6%e7%9a%84%e5%a2%9e%e9%87%8f%e8%ae%a1%e7%ae%97 aria-label="2.2. é¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶çš„å¢é‡è®¡ç®—ï¼š">2.2. é¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶çš„å¢é‡è®¡ç®—ï¼š</a></li></ul></li><li><a href=#4-vllm-%e4%b8%ad%e7%9a%84-paged-attention aria-label="4. vllm ä¸­çš„ Paged Attention">4. vllm ä¸­çš„ Paged Attention</a><ul><li><a href=#41-%e5%8a%a8%e6%9c%ba-memory-wastes aria-label="4.1. åŠ¨æœº: Memory Wastes">4.1. åŠ¨æœº: Memory Wastes</a></li><li><a href=#42-%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88-%e7%94%a8-page-%e7%ae%a1%e7%90%86%e5%86%85%e5%ad%98 aria-label="4.2. è§£å†³æ–¹æ¡ˆ: ç”¨ Page ç®¡ç†å†…å­˜">4.2. è§£å†³æ–¹æ¡ˆ: ç”¨ Page ç®¡ç†å†…å­˜</a></li></ul></li><li><a href=#5-paged-attention-kernel-%e8%af%a6%e8%a7%a3 aria-label="5. Paged Attention Kernel è¯¦è§£">5. Paged Attention Kernel è¯¦è§£</a><ul><li><a href=#51-%e8%be%93%e5%85%a5%e8%be%93%e5%87%ba%e8%be%93%e5%87%ba%e5%88%86%e6%9e%90%e5%92%8c%e5%8f%82%e6%95%b0%e8%af%b4%e6%98%8e aria-label="5.1. è¾“å…¥è¾“å‡ºè¾“å‡ºåˆ†æå’Œå‚æ•°è¯´æ˜">5.1. è¾“å…¥è¾“å‡ºè¾“å‡ºåˆ†æå’Œå‚æ•°è¯´æ˜</a></li><li><a href=#52shared-memory-q_vecs-%e7%9a%84%e5%86%99%e5%85%a5 aria-label="5.2.Shared Memory: q_vecs çš„å†™å…¥">5.2.Shared Memory: <code>q_vecs</code> çš„å†™å…¥</a></li><li><a href=#53-%e8%af%bb%e5%8f%96-k-cache-%e5%b9%b6%e8%ae%a1%e7%ae%97-qk aria-label="5.3. è¯»å– K Cache å¹¶è®¡ç®— QK">5.3. è¯»å– K Cache å¹¶è®¡ç®— QK</a></li><li><a href=#54-softmax aria-label="5.4. Softmax">5.4. Softmax</a></li><li><a href=#55-lv-logits--value aria-label="5.5. LV (Logits * Value)">5.5. LV (Logits * Value)</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{let e=null;const t=window.innerHeight+window.pageYOffset>=document.body.offsetHeight-100;if(t)e=elements[elements.length-1];else{let t=null,n=1/0;elements.forEach(e=>{const s=getOffsetTop(e)-window.pageYOffset;if(s<=window.innerHeight*.3){const o=Math.abs(s);o<n&&(n=o,t=e)}}),e=t||elements[0]}if(e&&e!==activeElement){if(activeElement){const t=encodeURI(activeElement.getAttribute("id")).toLowerCase(),e=document.querySelector(`.inner ul li a[href="#${t}"]`);e&&e.classList.remove("active")}activeElement=e;const n=encodeURI(activeElement.getAttribute("id")).toLowerCase(),t=document.querySelector(`.inner ul li a[href="#${n}"]`);t&&(t.classList.add("active"),document.getElementById("toc-container").classList.contains("wide")&&scrollTocToActiveItem(t))}},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}function scrollTocToActiveItem(e){const t=document.querySelector(".toc .inner");if(!t||!e)return;const n=t.getBoundingClientRect(),s=e.getBoundingClientRect(),o=n.height/2,i=s.top-n.top+t.scrollTop,a=i-o;t.scrollTo({top:Math.max(0,a),behavior:"smooth"})}</script><div class=post-content><h2 id=1-è¯æ˜-attention-çš„--åªä¸--æœ‰å…³>1. è¯æ˜ Attention çš„ $O_i$ åªä¸ $Q_i$ æœ‰å…³<a hidden class=anchor aria-hidden=true href=#1-è¯æ˜-attention-çš„--åªä¸--æœ‰å…³>#</a></h2><p>Attention çš„å…¬å¼å¦‚ä¸‹:</p>$$
O=Attention(Q,K,V)=softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$<p>å‡è®¾ $Q=\begin{bmatrix}Q_0\\Q_1\end{bmatrix}$, $K=\begin{bmatrix}K_0\\K_1\end{bmatrix}$</p><p>é‚£ä¹ˆ:</p>$$
O=softmax(\frac{\begin{bmatrix}Q_0K_0^T&Q_0K_1^T\\Q_1K_0^T&Q_1K_1^T\end{bmatrix}}{\sqrt{d_k}})V
$$<p>ä»¤:</p>$$
A=\begin{bmatrix}A_0\\A_1\end{bmatrix}=\begin{bmatrix}Q_0K_0^T&Q_0K_1^T\\Q_1K_0^T&Q_1K_1^T\end{bmatrix},f(x)=\frac{softmax(x)}{\sqrt{d_k}}
$$<p>æ­¤æ—¶, $A_1$ åªå’Œ $Q_1$ æœ‰å…³, å’Œ $Q_0$ æ— å…³, é‚£ä¹ˆ:</p>$$
\begin{bmatrix}O_0\\O_1\end{bmatrix}=O=\begin{bmatrix}f(A_0)\\f(A_1)\end{bmatrix}V=\begin{bmatrix}f(A_0)V\\f(A_1)V\end{bmatrix}
$$<p>å› æ­¤, $O_i$ åªå’Œ $A_i$ ç›¸å…³, è€Œæ ¹æ® $A$ çš„è®¾å®š, $A_i$ åªå’Œ $Q_i$ ç›¸å…³, å³:</p><p>Attention çŸ©é˜µçš„ç¬¬ $i$ ä¸ªè¾“å‡ºåªå’Œç¬¬ $i$ ä¸ª $Q$ æœ‰å…³, å’Œä¹‹å‰çš„ $Q$ æ— å…³.</p><p><strong>æ€»ç»“</strong>:</p><ul><li>åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶ï¼Œåªéœ€å¯¹æ–° token è®¡ç®—å¯¹åº”çš„ <code>Q_new</code>ï¼Œå¹¶ä¸ä¹‹å‰å·²ç»ç¼“å­˜çš„ <code>K_cache</code> å’Œ <code>V_cache</code> è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚</li><li>æ–°çš„ <code>K_new</code> å’Œ <code>V_new</code> ä¼šè¢«åŠ å…¥åˆ°ç¼“å­˜ä¸­ï¼Œç»§ç»­ä¸ºä¸‹ä¸€ä¸ª token ç”Ÿæˆæä¾›åŸºç¡€ã€‚</li><li>æ•´ä¸ªè¿‡ç¨‹é¿å…äº†å¯¹æ‰€æœ‰å†å² token çš„é‡å¤è®¡ç®—ï¼Œå¤§å¹…æé«˜äº†æ•ˆç‡ã€‚</li></ul><h2 id=2-kv-cache-çš„å¢é‡è¿‡ç¨‹>2. KV Cache çš„å¢é‡è¿‡ç¨‹<a hidden class=anchor aria-hidden=true href=#2-kv-cache-çš„å¢é‡è¿‡ç¨‹>#</a></h2><h3 id=21-åˆå§‹è¾“å…¥å®Œæ•´åºåˆ—è®¡ç®—>2.1. åˆå§‹è¾“å…¥ï¼ˆå®Œæ•´åºåˆ—ï¼‰è®¡ç®—ï¼š<a hidden class=anchor aria-hidden=true href=#21-åˆå§‹è¾“å…¥å®Œæ•´åºåˆ—è®¡ç®—>#</a></h3><ul><li>å¯¹äºåˆå§‹çš„è¾“å…¥åºåˆ— <code>(seq_len, embed_dim)</code>ï¼Œæˆ‘ä»¬é€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ° <code>Q</code>ã€<code>K</code> å’Œ <code>V</code>ï¼Œå®ƒä»¬çš„å½¢çŠ¶éƒ½æ˜¯ <code>(seq_len, embed_dim)</code>ã€‚</li><li>ä½¿ç”¨ <code>Q</code> å’Œ <code>K</code> è¿›è¡Œç‚¹ç§¯è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œç„¶åç»“åˆ <code>V</code> è®¡ç®—å¾—åˆ°è¾“å‡º <code>(seq_len, embed_dim)</code>ï¼Œè¿™æ˜¯ç¬¬ä¸€æ¬¡å¯¹åˆå§‹åºåˆ—çš„å®Œæ•´è®¡ç®—ã€‚</li></ul><h3 id=22-é¢„æµ‹ä¸‹ä¸€ä¸ª-token-æ—¶çš„å¢é‡è®¡ç®—>2.2. é¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶çš„å¢é‡è®¡ç®—ï¼š<a hidden class=anchor aria-hidden=true href=#22-é¢„æµ‹ä¸‹ä¸€ä¸ª-token-æ—¶çš„å¢é‡è®¡ç®—>#</a></h3><p>åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶ï¼Œä¸éœ€è¦å¯¹æ•´ä¸ªåºåˆ—å†è¿›è¡Œå®Œæ•´çš„ <code>Q</code>ã€<code>K</code>ã€<code>V</code> è®¡ç®—ï¼Œè€Œæ˜¯åªéœ€å¯¹æ–°ç”Ÿæˆçš„ token è¿›è¡Œä¸€æ¬¡å¢é‡è®¡ç®—ã€‚è¿™æ—¶çš„æ“ä½œæµç¨‹å¦‚ä¸‹ï¼š</p><ol><li><strong>è¾“å…¥æ–°çš„ token</strong>ï¼šå°†å·²ç»ç”Ÿæˆçš„ tokenï¼ˆå…¶å½¢çŠ¶ä¸º <code>(embed_dim,)</code>ï¼‰ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ°è¯¥ token å¯¹åº”çš„ <code>Q_new</code>ï¼Œå½¢çŠ¶ä¸º <code>(embed_dim,)</code>ã€‚</li><li><strong>ä¸ä¹‹å‰ç¼“å­˜çš„ <code>K</code> å’Œ <code>V</code> è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—</strong>ï¼šä½¿ç”¨ <code>Q_new</code> ä¸ä¹‹å‰å·²ç»è®¡ç®—å¹¶ç¼“å­˜çš„ <code>K_cache</code> å’Œ <code>V_cache</code> è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚è¿™é‡Œçš„ <code>K_cache</code> å’Œ <code>V_cache</code> åˆ†åˆ«æ˜¯ä¹‹å‰æ¯æ¬¡ç”Ÿæˆ token æ—¶å¾—åˆ°çš„ <code>K</code> å’Œ <code>V</code>ï¼Œå®ƒä»¬çš„å½¢çŠ¶æ˜¯ <code>(seq_len, embed_dim)</code>ï¼Œå³ç¼“å­˜äº†ä»æœ€åˆè¾“å…¥åºåˆ—åˆ°å½“å‰å·²ç»ç”Ÿæˆçš„æ‰€æœ‰ token çš„ <code>K</code> å’Œ <code>V</code>ã€‚<code>Q_new</code> å¯ä»¥ç›´æ¥ä¸ <code>K_cache</code> è¿›è¡Œç‚¹ç§¯ï¼Œå¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°ï¼Œç„¶åç»“åˆ <code>V_cache</code> å¾—åˆ°æ–°çš„è¾“å‡ºã€‚</li><li><strong>æ›´æ–° <code>KV Cache</code></strong>ï¼šæ–°çš„ <code>K_new</code> å’Œ <code>V_new</code> ä¼šé€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ°ï¼ˆå½¢çŠ¶ä¸º <code>(embed_dim,)</code>ï¼‰ï¼Œå¹¶å°†å®ƒä»¬æ·»åŠ åˆ° <code>K_cache</code> å’Œ <code>V_cache</code> çš„æœ«å°¾ï¼Œä½¿å¾—ç¼“å­˜çš„ <code>K_cache</code> å’Œ <code>V_cache</code> ä¸æ–­å¢å¤§ï¼Œä»¥å¤‡åç»­ä½¿ç”¨ã€‚</li><li><strong>è¾“å‡º</strong>ï¼šé€šè¿‡æ³¨æ„åŠ›è®¡ç®—åçš„è¾“å‡ºå½¢çŠ¶ä¸º <code>(embed_dim,)</code>ï¼Œå³æ–°ç”Ÿæˆçš„ tokenã€‚</li></ol><h2 id=4-vllm-ä¸­çš„-paged-attention>4. vllm ä¸­çš„ Paged Attention<a hidden class=anchor aria-hidden=true href=#4-vllm-ä¸­çš„-paged-attention>#</a></h2><h3 id=41-åŠ¨æœº-memory-wastes>4.1. åŠ¨æœº: Memory Wastes<a hidden class=anchor aria-hidden=true href=#41-åŠ¨æœº-memory-wastes>#</a></h3><p><img alt=memory-wastes.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/memory-wastes.png></p><p>ä¸Šå›¾å±•ç¤ºäº†å¯èƒ½çš„å†…å­˜æµªè´¹æƒ…å†µ, ä¸»è¦æ—¶è¾“å…¥ sequence ä¸çŸ¥é“ eos åœ¨å“ªé‡Œ, å¦‚æœéšæœºç”³è¯·å†…å­˜, å¯èƒ½å¯¼è‡´å¤§é‡å†…å­˜ç¢ç‰‡, å› æ­¤ååé‡ä¸‹é™.</p><h3 id=42-è§£å†³æ–¹æ¡ˆ-ç”¨-page-ç®¡ç†å†…å­˜>4.2. è§£å†³æ–¹æ¡ˆ: ç”¨ Page ç®¡ç†å†…å­˜<a hidden class=anchor aria-hidden=true href=#42-è§£å†³æ–¹æ¡ˆ-ç”¨-page-ç®¡ç†å†…å­˜>#</a></h3><p><img alt=paged-attention-animation.webp loading=lazy src=/imgs/blogs/dive-into-paged-attention/paged-attention-animation.webp></p><p>ä¸Šå›¾å±•ç¤ºäº† vLLM ç”¨ Paged ç®¡ç†å†…å­˜å…·ä½“æ€ä¹ˆåšçš„.</p><p>ç®€å•æ¥è¯´, vLLM åœ¨å¼€å§‹æ¨ç†å‰ä¸ºæ¯ä¸ª Decoder Layer ç”³è¯·ä¸¤ä¸ªå·¨é•¿çš„ Tensor (<code>k_cache</code> å’Œ <code>v_cache</code>), æŠŠ Tensor åˆ†å‰²æˆè¿ç»­ç­‰é•¿çš„ PA blocks (å›¾ä¸­çš„ä¸€è¡Œä¸ºä¸€ä¸ª PA Block); æ¯ä¸ª PA Block èƒ½å¤Ÿå­˜æ”¾ <code>BLOCK_SIZE</code> ä¸ª token çš„ K æˆ– V cache (æ¯ä¸ª cache çš„å½¢çŠ¶å¯ä»¥ç†è§£ä¸º <code>(num_heads, head_size)</code>).</p><p>å› æ­¤, <code>k_cache</code> å’Œ <code>v_cache</code> çš„å½¢çŠ¶å¯ä»¥ç†è§£ä¸º <code>(num_blocks, num_heads, head_size)</code>.</p><p>å¯¹äºä¸€ä¸ªè¿ç»­çš„ sequnce, åœ¨ prefill é˜¶æ®µå‰å°±ä¼šåˆ†é…å¥½å®ƒçš„ PA blocks, ä¹‹åæ¨ç†æ—¶:</p><ul><li>è‹¥æ˜¯è®¡ç®— prompt çš„ Attention, åˆ™å…ˆæŠŠä¼ å…¥çš„ K å’Œ V æŒ‰ç…§ PA blocks å­˜å…¥ <code>k_cache</code> å’Œ <code>v_cache</code> ä¸­; ç„¶ååˆ©ç”¨æ•´æ®µçš„ QKV è®¡ç®— attention.</li><li>è‹¥æ˜¯è®¡ç®—æ–° token, åˆ™åˆ©ç”¨ Q å’Œ block table è®¡ç®— decode é˜¶æ®µçš„ attntion; æ­¤æ—¶è®¿å­˜çš„å°±æ˜¯ <code>k_cache</code> å’Œ <code>v_cache</code> ä¸­çš„ PA blocks.</li></ul><h2 id=5-paged-attention-kernel-è¯¦è§£>5. Paged Attention Kernel è¯¦è§£<a hidden class=anchor aria-hidden=true href=#5-paged-attention-kernel-è¯¦è§£>#</a></h2><blockquote><p>References:</p><ul><li><a href=https://docs.vllm.ai/en/latest/dev/kernel/paged_attention.html>vLLM Paged Attention</a></li><li><a href=https://zhuanlan.zhihu.com/p/673284781>vLLMçš‡å† ä¸Šçš„æ˜ç ï¼šæ·±å…¥æµ…å‡ºç†è§£PagedAttention CUDAå®ç°</a></li></ul></blockquote><p>å…ˆçœ‹ä¸‹æ•´ä½“è®¡ç®—æµç¨‹å›¾ (è¿™ä¸ªå›¾åé¢ä¹Ÿä¼šå‡ºç°è¿™é‡Œå…ˆçœ‹ä¸€çœ¼):</p><p><img alt=pa-cal.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/pa-cal.png></p><h3 id=51-è¾“å…¥è¾“å‡ºè¾“å‡ºåˆ†æå’Œå‚æ•°è¯´æ˜>5.1. è¾“å…¥è¾“å‡ºè¾“å‡ºåˆ†æå’Œå‚æ•°è¯´æ˜<a hidden class=anchor aria-hidden=true href=#51-è¾“å…¥è¾“å‡ºè¾“å‡ºåˆ†æå’Œå‚æ•°è¯´æ˜>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Grid: (num_heads, num_seqs, 1).
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>template</span><span class=o>&lt;</span>
</span></span><span class=line><span class=cl><span class=k>typename</span> <span class=n>scalar_t</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>HEAD_SIZE</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>BLOCK_SIZE</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>NUM_THREADS</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>PARTITION_SIZE</span> <span class=o>=</span> <span class=mi>0</span><span class=o>&gt;</span>
</span></span><span class=line><span class=cl><span class=n>__device__</span> <span class=kt>void</span> <span class=n>paged_attention_kernel</span><span class=p>(</span>
</span></span><span class=line><span class=cl><span class=p>...</span> <span class=c1>// Other side args.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=n>scalar_t</span><span class=o>*</span> <span class=n>__restrict__</span> <span class=n>out</span><span class=p>,</span>       <span class=c1>// [num_seqs, num_heads, max_num_partitions, head_size]
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=n>scalar_t</span><span class=o>*</span> <span class=n>__restrict__</span> <span class=n>q</span><span class=p>,</span>         <span class=c1>// [num_seqs, num_heads, head_size]
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=n>scalar_t</span><span class=o>*</span> <span class=n>__restrict__</span> <span class=n>k_cache</span><span class=p>,</span>   <span class=c1>// [num_blocks, num_kv_heads, head_size/x, block_size, x]
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=n>scalar_t</span><span class=o>*</span> <span class=n>__restrict__</span> <span class=n>v_cache</span><span class=p>,</span>   <span class=c1>// [num_blocks, num_kv_heads, head_size, block_size]
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>...</span> <span class=c1>// Other side args.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>)</span>
</span></span></code></pre></div><p>æ¨¡æ¿å‚æ•°è¯´æ˜:</p><ul><li><code>scalar_t</code> å…ƒç´ ç±»å‹ (å®é™…ä»£ç ä¸­è¿˜æœ‰ <code>cache_t</code> è¡¨ç¤º KV cache çš„å…ƒç´ ç±»å‹).</li><li><code>HEAD_SIZE</code> æ¯ä¸ª head ä¸­å…ƒç´ æ•°é‡.</li><li><code>BLOCK_SIZE</code> æ¯ä¸ª PA block ä¸­çš„ token æ•°é‡.<blockquote><ol><li>KV cache è¢«å­˜å‚¨åœ¨ä¸åŒ PA blocks. æ¯ä¸ª PA block å­˜å‚¨ä¸€ä¸ª head ä¸­ <code>BLOCK_SIZE</code> ä¸ª token.<br>ä¾‹å¦‚, è‹¥ <code>BLOCK_SIZE=16</code>, <code>HEAD_SIZE=128</code>, åˆ™ä¸€ä¸ª PA block èƒ½å­˜å‚¨ä¸€ä¸ª head çš„ <code>16 * 128 = 2048</code> ä¸ªå…ƒç´ .</li><li>æ¯ä¸ª PA block å¯èƒ½åªåŒ…å«ä¸€éƒ¨åˆ†çš„ context tokens.</li><li>ä» page è§’åº¦çœ‹, KV cache æ˜¯è‹¥å¹²ä¸ª page çš„é›†åˆ;</li></ol></blockquote></li><li><code>NUM_THREADS</code> æ¯ä¸ª CUDA thread block ä¸­ thread çš„æ•°é‡.</li><li><code>PARTITION_SIZE</code> å‚ä¸ TP çš„ GPU æ•°é‡, é»˜è®¤ 0 è¡¨ç¤ºå•å¡. (ä»¥ä¸‹éƒ½ä»¥å•å¡ä¸ºä¾‹è¯´æ˜)</li></ul><p>é¢å¤–çš„ä¸€äº›å‚æ•°:</p><ul><li><code>num_seqs</code>: æœ¬æ¬¡æ¨ç†è¯·æ±‚ sequence æ•°ç›®.<blockquote><p>ç”±äºè¿™ä¸ª kernel åªå¤„ç† decode é˜¶æ®µå• query attention, æ‰€ä»¥å®é™…ä¸Šæ¯ä¸ª sequence åªæœ‰ä¸€ä¸ª query token.</p></blockquote></li><li><code>num_heads</code>: Q çš„ head æ•°ç›®</li><li><code>num_kv_heads</code>: KV çš„ head æ•°ç›®, å¯¹äº MHA å…¶å€¼å’Œ <code>num_heads</code> ç›¸åŒ; å¦‚æœæ˜¯ GQA, MQA åˆ™ <code>num_kv_heads</code> å°äº <code>num_head</code>.</li><li><code>head_size</code>: å³ <code>HEAD_SIZE</code></li><li><code>k_cache: (num_blocks, num_kv_heads, head_size/x, block_size, x)</code>, å…¶ä¸­ <code>x</code> è¡¨ç¤º <code>THREAD_GROUP_SIZE * VEC_SIZE</code> çš„å¤§å° (åé¢ä¼šç»†è¯´).</li></ul><p>ä¸‹é¢ç»“åˆ GPU architecture åˆæ­¥åˆ†æä¸€ä¸‹å‚æ•°.</p><p><img alt=gpu-archi.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/gpu-archi.png></p><p>ğŸ§ <strong>ä¸ºä»€ä¹ˆè¦åˆ† thread group?</strong></p><ul><li>å› ä¸ºå½“ä¸€ä¸ª cuda block è¦å–çš„æ•°æ®æ¯”è¾ƒå°‘çš„æ—¶å€™ (è®¡ç®— QK), ä¸€ä¸ª thread group åˆ†åˆ«ä¸€æ¬¡å– Q å’Œ K ä¸­ 16B; å½“ä¸€ä¸ª cuda block è¦å–çš„æ•°æ®æ¯”è¾ƒå¤šçš„æ—¶å€™ (è®¡ç®— LV), ä¸€ä¸ª thread å– 16B.</li></ul><h3 id=52shared-memory-q_vecs-çš„å†™å…¥>5.2.Shared Memory: <code>q_vecs</code> çš„å†™å…¥<a hidden class=anchor aria-hidden=true href=#52shared-memory-q_vecs-çš„å†™å…¥>#</a></h3><p>ä» kernel ä¸­çš„ç¬¬ä¸€ä¸ªç”³è¯·çš„ shared memory å¼€å§‹è¯´.</p><blockquote><p>å…³äº shared memeory:</p><ol><li>åœ¨ kernel ä¸­ç”³è¯·çš„ shared memory è¢«å½“å‰ cuda block ä¸­çš„æ‰€æœ‰ thread å…±äº«.</li><li>shared memory çš„ä½œç”¨æ˜¯ä¸ºäº†å‡å°‘ global memory çš„è®¿é—®æ¬¡æ•°ï¼Œæé«˜è®¿å­˜æ•ˆç‡.</li></ol></blockquote><p>ä»¥ä¸‹ä»£ç ç”³è¯·äº†ä¸€å— shared memroy è¢«æ•´ä¸ª CUDA Block ä¸­æ‰€æœ‰ kernel å…±äº«:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>__shared__</span> <span class=n>Q_vec</span> <span class=n>q_vecs</span><span class=p>[</span><span class=n>THREAD_GROUP_SIZE</span><span class=p>][</span><span class=n>NUM_VECS_PER_THREAD</span><span class=p>];</span>
</span></span></code></pre></div><p>é¦–å…ˆ, <code>q_vecs</code> è¦†ç›–äº† Q ä¸­ <code>head_size</code> ä¸ªå…ƒç´  - è¿™ä¹Ÿæ˜¯ä¸€ä¸ª cuda block éœ€è¦å¤„ç†çš„æ•°æ®é‡.</p><p>æ¥ç€å†è¯´ä¸¤ä¸ªç»´åº¦çš„å‚æ•°çš„æ„æ€:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>constexpr</span> <span class=kt>int</span> <span class=n>NUM_ELEMS_PER_THREAD</span> <span class=o>=</span> <span class=n>HEAD_SIZE</span> <span class=o>/</span> <span class=n>THREAD_GROUP_SIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>constexpr</span> <span class=kt>int</span> <span class=n>NUM_VECS_PER_THREAD</span> <span class=o>=</span> <span class=n>NUM_ELEMS_PER_THREAD</span> <span class=o>/</span> <span class=n>VEC_SIZE</span><span class=p>;</span>
</span></span></code></pre></div><ul><li><code>THREAD_GROUP_SIZE</code>: æ¯ä¸ª thread group ä¸­çš„ thread æ•°é‡. æ³¨æ„, ä¸€ä¸ª cuda block ä¸­æœ‰ <code>NUM_THREADS</code> ä¸ª thread, <code>NUM_THREAD_GROUPS</code> ä¸ª thread group. <code>THREAD_GROUP_SIZE = MAX(WARP_SIZE/BLOCK_SIZE, 1)</code>.</li><li><code>NUM_VECS_PER_THREAD</code>: <code>HEAD_SIZE</code> èƒ½è¢«åˆ†æˆå¤šå°‘ä¸ª 16B. (è¿™ä¸ªå˜é‡è¿™ä¹ˆå‘½åçš„ç†ç”±æ˜¯åé¢è¯»å– K çš„æ—¶å€™æ¯ä¸ª thread ä¼šå¾€è‡ªå·±çš„å¯„å­˜å™¨å†…è¯» <code>NUM_VECS_PER_THREAD</code> ä¸ª k_vec.)</li></ul><blockquote><p>è¯æ˜: <code>q_vecs</code> è¦†ç›– Q çš„ä¸€ä¸ª head, å¹¶ä¸” <code>NUM_VECS_PER_THREAD</code> è¡¨ç¤º Q çš„ä¸€ä¸ª head è¢«åˆ†æˆå¤šå°‘ä¸ª 16B.<br>=> <code>THREAD_GROUP_SIZE</code> * <code>VEC_SIZE</code> = 16B / <code>sizeof(scalar_t)</code>;<br>=> <code>NUM_VECS_PER_THREAD</code> * 16B / <code>sizeof(scalar_t)</code> = <code>HEAD_SIZE</code>;</p></blockquote><p>ç„¶åçœ‹ load Q çš„ä»£ç , å»ºè®®ç»“åˆä¸‹é¢çš„å›¾ä¸€èµ·çœ‹:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl>  <span class=c1>// Load Q to shmem
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=n>thread_group_idx</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_VECS_PER_THREAD</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>i</span> <span class=o>+=</span> <span class=n>NUM_THREAD_GROUPS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>int</span> <span class=n>vec_idx</span> <span class=o>=</span> <span class=n>thread_group_offset</span> <span class=o>+</span> <span class=n>i</span> <span class=o>*</span> <span class=n>THREAD_GROUP_SIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>q_vecs</span><span class=p>[</span><span class=n>thread_group_offset</span><span class=p>][</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>        <span class=o>*</span><span class=k>reinterpret_cast</span><span class=o>&lt;</span><span class=k>const</span> <span class=n>Q_vec</span><span class=o>*&gt;</span><span class=p>(</span><span class=n>q_ptr</span> <span class=o>+</span> <span class=n>vec_idx</span> <span class=o>*</span> <span class=n>VEC_SIZE</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><ul><li><code>thread_group_idx</code> è¡¨ç¤ºå½“å‰ thread å±äºå½“å‰ cuda block ä¸­ç¬¬å‡ ä¸ª thread group.</li><li><code>thread_group_offset</code> è¡¨ç¤ºå½“å‰ thread åœ¨å½“å‰ thread group ä¸­æ˜¯ç¬¬å‡ ä¸ª thread.</li></ul><p><img alt=pa-load-q.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/pa-load-q.png></p><p>ä¸Šå›¾å±•ç¤ºäº†å¾ªç¯å…·ä½“æ˜¯æ€ä¹ˆè·‘çš„.</p><ul><li>ä¸€ä¸ªç´«è‰²ç®­å¤´è¡¨ç¤ºä¸€ä¸ª thread group.</li><li><code>NUM_VECS_PER_THREAD</code> è¡¨ç¤º <code>HEAD_SIZE</code> èƒ½è¢«åˆ†æˆå¤šå°‘ä¸ª 16B.</li><li>å®é™…è¯»å– Q çš„å†…å­˜æ—¶, æ‰€æœ‰ thread group ä» Q çš„èµ·å§‹ä½ç½®ç´§å¯†æ’åˆ—, æ ¹æ®å›¾ä¸Šçœ‹çš„è¯ä¸€å…±æœ‰ <code>NUM_THREAD_GROUPS</code> ä¸ªç´«è‰²ç®­å¤´.</li><li>æ‰€æœ‰ thread group è¯»å–ä¸€æ¬¡ Q å¹¶å­˜å…¥ <code>q_vecs</code> å¯¹åº”å¾ªç¯ä¸­çš„ä¸€æ¬¡è¿­ä»£; å› æ­¤ä¸‹æ¬¡è¿­ä»£ thread group éœ€è¦å‘ååç§» <code>NUM_THREAD_GROUPS</code> ä¸ªä½ç½® (ä¾‹å¦‚ <code>i</code> ä» 1 å˜ä¸º 7).</li><li>æ­¤å¤–, è¯»ä¸€æ¬¡ 16B å¯¹åº”ä¸€ä¸ª thread æ¥è¯´è‡ªç„¶ä¹Ÿæ˜¯å–ä¸€ä¸ª VEC.</li><li>å¯¹åº”åˆ° kernel ç¼–å†™, è¿˜éœ€è¦è®¡ç®—å½“å‰ thread å…·ä½“è¯»å–å“ªä¸ª vec; å› æ­¤å¾—åˆ° <code>vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE</code>.</li></ul><blockquote><p>ğŸ¤” è¿™é‡Œä¼šä¸ä¼šæœ‰ bank conflict?</p></blockquote><p>æ€»ä¹‹ç°åœ¨æˆ‘ä»¬æŠŠ <code>(1, head_size)</code> å¤§å°çš„å…ƒç´ è¯»åˆ°äº† cuda block å…±äº«çš„ shared memory <code>q_vecs</code> ä¸­.</p><h3 id=53-è¯»å–-k-cache-å¹¶è®¡ç®—-qk>5.3. è¯»å– K Cache å¹¶è®¡ç®— QK<a hidden class=anchor aria-hidden=true href=#53-è¯»å–-k-cache-å¹¶è®¡ç®—-qk>#</a></h3><p>ç°åœ¨ä» cuda block çš„è§’åº¦çœ‹, å½“å‰ block å·²ç»è·å¾—äº†è‡ªå·±è¦ç®—çš„ Q ä¸­çš„ä¸€ä¸ª head (å½¢çŠ¶ä¸º <code>(1, head_size)</code>), æ¥ä¸‹æ¥å°±æ˜¯è®¡ç®— Q å’Œ K çš„ç‚¹ç§¯.</p><p>ç‚¹ç§¯è¿‡ç¨‹æ˜¯æŠŠå½“å‰ block æ‹¥æœ‰çš„ Q head å’Œæ•´ä¸ª K Cache (è¿­ä»£åœ°) è¿›è¡Œç‚¹ç§¯è¿ç®—. å‚è€ƒä¸‹å›¾:</p><p><img alt=pa-cal-kq-01.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/pa-cal-kq-01.png></p><p>QK ä¹˜ç§¯å®é™…ä¸Šè¢«æš‚å­˜åœ¨ <code>logits</code> (ä¹Ÿæ˜¯ä¸€å— shared memory) ä¸­, ä¹‹åä¼šè¢«ç”¨æ¥è®¡ç®— softmax.</p><p>ğŸ˜‡ çœ‹ä¸‹å¾ªç¯çš„å…·ä½“ä»£ç å§:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Loop 1
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>block_idx</span> <span class=o>=</span> <span class=n>start_block_idx</span> <span class=o>+</span> <span class=n>warp_idx</span><span class=p>;</span> <span class=n>block_idx</span> <span class=o>&lt;</span> <span class=n>end_block_idx</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>block_idx</span> <span class=o>+=</span> <span class=n>NUM_WARPS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Physical block calculation ...
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// Loop 2
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_TOKENS_PER_THREAD_GROUP</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Offset calculation ...
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>K_vec</span> <span class=n>k_vecs</span><span class=p>[</span><span class=n>NUM_VECS_PER_THREAD</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Loop 3
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>NUM_VECS_PER_THREAD</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=c1>// Load K to `k_vecs` ...
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=kt>float</span> <span class=n>qk</span> <span class=o>=</span> <span class=n>scale</span> <span class=o>*</span> <span class=n>Qk_dot</span><span class=o>&lt;</span><span class=n>scalar_t</span><span class=p>,</span> <span class=n>THREAD_GROUP_SIZE</span><span class=o>&gt;::</span><span class=n>dot</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                        <span class=n>q_vecs</span><span class=p>[</span><span class=n>thread_group_offset</span><span class=p>],</span> <span class=n>k_vecs</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Add the ALiBi bias if slopes are given.
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>qk</span> <span class=o>+=</span> <span class=p>(</span><span class=n>alibi_slope</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>)</span> <span class=o>?</span> <span class=n>alibi_slope</span> <span class=o>*</span> <span class=p>(</span><span class=n>token_idx</span> <span class=o>-</span> <span class=n>seq_len</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>:</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=n>thread_group_offset</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=c1>// Store the partial reductions to shared memory.
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=c1>// Mask
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=c1>// Update the max value.
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>å…ˆè¯´ç¬¬ä¸€ä¸ªå¾ªç¯, å…¶ä¸­æ¯”è¾ƒé‡è¦çš„å‡ ä¸ªå‚æ•°å®šä¹‰å¦‚ä¸‹:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// [start_block_idx, end_block_idx) is the range of blocks to process.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=kt>int</span> <span class=n>start_block_idx</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>    <span class=n>USE_PARTITIONING</span> <span class=o>?</span> <span class=n>partition_idx</span> <span class=o>*</span> <span class=nl>num_blocks_per_partition</span> <span class=p>:</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=c1>// If not using partitioning, `end_block_idx` should be equal to `num_seq_blocks`.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=kt>int</span> <span class=n>end_block_idx</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>    <span class=n>MIN</span><span class=p>(</span><span class=n>start_block_idx</span> <span class=o>+</span> <span class=n>num_blocks_per_partition</span><span class=p>,</span> <span class=n>num_seq_blocks</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=c1>// Number of blocks to process.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=kt>int</span> <span class=n>num_blocks</span> <span class=o>=</span> <span class=n>end_block_idx</span> <span class=o>-</span> <span class=n>start_block_idx</span><span class=p>;</span>
</span></span></code></pre></div><p>ç”¨æ–‡å­—æè¿°å°±æ˜¯:</p><ul><li><code>blk_idx</code> è¡¨ç¤ºå½“å‰ thread æ‰€åœ¨ warp éœ€è¦å¤„ç†çš„ PA block çš„åœ¨ <code>block_table</code> ä¸­ç´¢å¼• (é€»è¾‘ä¸Šçš„ç´¢å¼•).</li><li><code>start_block_idx</code> å’Œ <code>end_block_idx</code> è¡¨ç¤ºå½“å‰ cuda block éœ€è¦å¤„ç†çš„ block èŒƒå›´.</li><li><code>num_blocks</code> è¡¨ç¤ºå½“å‰ cuda block éœ€è¦å¤„ç†çš„ block æ•°é‡.</li><li><code>NUM_WARPS</code> è¡¨ç¤ºå½“å‰ cuda block ä¸­ warp çš„æ•°é‡. ä¸€ä¸ª warp åŒ…å« 32 ä¸ª thread.</li><li><code>warp_idx</code> è¡¨ç¤ºå½“å‰ warp åœ¨å½“å‰ cuda block ä¸­çš„ç´¢å¼•.</li></ul><p>è¯´äººè¯å°±æ˜¯æ¯ä¸ª warp å¤„ç†ä¸€ä¸ª PA block, ä¸€å¼€å§‹ cuda block ä¸­çš„æ‰€æœ‰ warp ç´§å¯†åœ°æŒ‡å‘æœ€å‰é¢çš„ <code>NUM_WARPS</code> ä¸ª PA block, æ¯æ¬¡å¾ªç¯æ‰€æœ‰ warp å‘ååç§» <code>NUM_WARPS</code> ä¸ª PA block çš„é•¿åº¦. å‚è€ƒä¸‹å›¾:</p><p><img alt=pa-cal-kq-02.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/pa-cal-kq-02.png></p><blockquote><p>ğŸ”” è¿™é‡Œå†å›é¡¾ä¸€ä¸‹, ä¸€ä¸ª PA block é‡Œå­˜æ”¾äº† <code>BLOCK_SIZE</code> ä¸ª token çš„ K æˆ– V cache.</p></blockquote><p>æ‰€ä»¥è¯´è¿™ä¸ªå¾ªç¯å’Œä¸Šé¢è¯»å– Q çš„å¾ªç¯ä¸€ä¸ªå°¿æ€§ğŸ¤®, ä¸è¿‡æ˜¯ä»¥ warp çš„ç²’åº¦å¤„ç†æ•°æ®;</p><p>è¿›å…¥äº†ç¬¬ä¸€ä¸ªå¾ªç¯å†…éƒ¨, ç¬¬ä¸€æ­¥å½“ç„¶æ˜¯è®¡ç®—å½“å‰ thread å¯¹åº”çš„ warp åº”è¯¥è®¡ç®—å“ªä¸ª PA block (ç‰©ç†ä¸Šçš„ç´¢å¼•), å› æ­¤å¾—åˆ°äº† <code>physical_block_number</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>const</span> <span class=kt>int64_t</span> <span class=n>physical_block_number</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>    <span class=k>static_cast</span><span class=o>&lt;</span><span class=kt>int64_t</span><span class=o>&gt;</span><span class=p>(</span><span class=n>block_table</span><span class=p>[</span><span class=n>block_idx</span><span class=p>]);</span>
</span></span></code></pre></div><hr><p>ç„¶åè§£é‡Šç¬¬äºŒä¸ªå¾ªç¯, ç¬¬äºŒä¸ªå¾ªç¯çš„æ•´ä½“ç›®æ ‡å°±æ˜¯è®©å½“å‰ warp è®¡ç®—å¥½è‡ªå·±è´Ÿè´£çš„ PA block ä¸­ <code>BLOCK_SIZE</code> ä¸ª token çš„ QK ä¹˜ç§¯.</p><p>å…ˆçœ‹ä¸€ä¸‹ <code>i</code> çš„ä¸Šç•Œ:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>constexpr</span> <span class=kt>int</span> <span class=n>NUM_TOKENS_PER_THREAD_GROUP</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>    <span class=n>DIVIDE_ROUND_UP</span><span class=p>(</span><span class=n>BLOCK_SIZE</span><span class=p>,</span> <span class=n>WARP_SIZE</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=c1>// Loop 1
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>block_idx</span> <span class=o>=</span> <span class=n>start_block_idx</span> <span class=o>+</span> <span class=n>warp_idx</span><span class=p>;</span> <span class=n>block_idx</span> <span class=o>&lt;</span> <span class=n>end_block_idx</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>block_idx</span> <span class=o>+=</span> <span class=n>NUM_WARPS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Loop 2
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_TOKENS_PER_THREAD_GROUP</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></div><p>ä» kernel è§’åº¦çœ‹, æ¯ä¸ª thread éœ€è¦è¾…åŠ©å½“å‰ warp è®¡ç®—è‡ªå·±è´Ÿè´£çš„ä¸€æ•´ä¸ª PA block (åŒ…å« <code>BLOCK_SIZE</code> ä¸ª token), è€Œæˆ‘ä»¬æŠŠè¿™ä¸ªè¿‡ç¨‹æ‹†åˆ†ä¸º Loop 2 ä¸­çš„ <code>NUM_TOKEN_PER_THREAD_GROUP</code> (ä¹Ÿå°±æ˜¯ <code>ceil(BLOCK_SIZE / 32)</code>) æ¬¡å¾ªç¯;</p><p>è¯´äººè¯å°±æ˜¯<strong>ä¸€ä¸ª thread group å¯¹åº”ä¸€ä¸ª token ä¸­çš„ä¸€ä¸ª head</strong>, å¦‚æœ BLOCK SIZE å¤ªå¤§äº†åé¢æ¯ä¸ª thread å‘ååç§» <code>i * WARP_SIZE</code> ä¸ª token ç»§ç»­ç‹ ç‹ ç®—ğŸ¤£.</p><p>ä¹Ÿå› æ­¤ç¬¬äºŒä¸ªå¾ªç¯å†…éƒ¨ä¸€ä¸Šæ¥å…ˆè®¡ç®—äº†å‡ ä¸ªåç§»é‡, å¹¶ä¸”ç”³è¯·äº† thread å†…éƒ¨ç§æœ‰çš„ <code>k_vecs</code> æ•°ç»„:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>const</span> <span class=kt>int</span> <span class=n>physical_block_offset</span> <span class=o>=</span> <span class=p>(</span><span class=n>thread_group_idx</span> <span class=o>+</span> <span class=n>i</span> <span class=o>*</span> <span class=n>WARP_SIZE</span><span class=p>)</span> <span class=o>%</span> <span class=n>BLOCK_SIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>const</span> <span class=kt>int</span> <span class=n>token_idx</span> <span class=o>=</span> <span class=n>block_idx</span> <span class=o>*</span> <span class=n>BLOCK_SIZE</span> <span class=o>+</span> <span class=n>physical_block_offset</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>K_vec</span> <span class=n>k_vecs</span><span class=p>[</span><span class=n>NUM_VECS_PER_THREAD</span><span class=p>];</span>
</span></span></code></pre></div><ul><li><code>thread_group_idx</code> è¡¨ç¤ºå½“å‰ thread group åœ¨æ•´ä¸ª cuda block ä¸­çš„ç´¢å¼•.</li><li>â˜¢ï¸ ä¸€ä¸ª thread group åœ¨ä¸€æ¬¡å¾ªç¯ä¸­è´Ÿè´£ fetch ä¸€ä¸ª PA block ä¸­ K cache çš„ä¸€ä¸ª token ä¸­<strong>è‡ªå·±è´Ÿè´£çš„ head</strong>.</li><li>â˜¢ï¸ ä¸€ä¸ª thread group è´Ÿè´£è®¡ç®—ä¸€ä¸ª qk å€¼; è¿™ä¸ªå€¼æ˜¾ç„¶æ˜¯ç”±ä¸€ä¸ª Q head å’Œä¸€ä¸ª K head ç‚¹ç§¯å¾—åˆ°çš„.</li><li><code>physical_block_offset</code> è¡¨ç¤ºå½“å‰è¦ç®—çš„ token åœ¨å½“å‰ PA block ä¸­çš„åç§»é‡ (æ³¨æ„å’Œå‰é¢çš„ <code>physical_block_number</code> åŒºåˆ†).</li><li>åŠ  <code>i * WARP_SIZE</code> çš„åŸå› æ˜¯å¦‚æœ <code>BLOCK_SIZE</code> å¤§äº 32, é‚£ä¹ˆä¸€ä¸ª warp è¦å¤šæ¬¡å¾ªç¯æ‰èƒ½å¤„ç†å®Œä¸€ä¸ª PA block ä¸­çš„æ‰€æœ‰ token, å¯¹åº” <code>thread_group_idx</code> éœ€è¦åšåç§».</li><li><code>token_idx</code> è¡¨ç¤ºå½“å‰è¦ç®—çš„ token åœ¨æ•´ä¸ª seq çš„ KV cache ä¸­çš„ç´¢å¼•.</li><li><code>k_vecs</code> ä¸­èƒ½å­˜æ”¾ <code>NUM_VECS_PER_THREAD</code> ä¸ª VEC, è€Œä¸€æ•´ä¸ª thread group ä¸­æ‰€æœ‰çš„ thread çš„ <code>k_vecs</code> åˆèµ·æ¥æ‰èƒ½ç»„æˆä¸€ä¸ª K çš„ head (æ¨å¯¼å‚è€ƒä¸Šé¢ Q çš„ ğŸ˜‡). è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåé¢ç®— QK çš„æ—¶å€™è¦ reduce.</li></ul><p>ğŸ¤” <strong>çœ‹åˆ°è¿™é‡Œè¯»è€…å¯èƒ½æœ‰ä¸€ä¸ªé—®é¢˜: ä¸€ä¸ª token çš„ K cache åº”è¯¥å¯¹åº”å¤šä¸ª head, ä¸ºä»€ä¹ˆä¸Šé¢è¯´ä¸€ä¸ª thread group åªè´Ÿè´£ä¸€ä¸ª head?</strong><br>ç­”: å› ä¸ºå®é™…è®¡ç®—çš„æ—¶å€™, ä¸€ä¸ª cuda block åªè´Ÿè´£è®¡ç®—ä¸€ä¸ª head, å¯¹åº”åˆ° K Cache ä¹ƒè‡³åé¢ V Cache çš„ä½ç½®ä¹Ÿæ˜¯ä¸€æ ·çš„.</p><blockquote><p>è¿™é‡Œé¢å¤–è¯´ä¸€ä¸‹, è¯» K çš„ head çš„ä¸€ä¸ªç›®æ ‡åº”è¯¥æ˜¯åœ¨å°½é‡å°‘çš„ register ä¸­è£…ä¸‹ä¸€ä¸ª head çš„æ‰€æœ‰å…ƒç´ , è¿™æ ·åç»­å’Œ shared memory ä¸­çš„ Q åšç‚¹ä¹˜å¹¶è§„çº¦çš„é€Ÿåº¦æ›´å¿«. å‡è®¾ä¸€ä¸ª head æœ‰ 128 ä¸ª float16, åˆ™å ç”¨ 256B, è€Œ A100 ä¸­ä¸€ä¸ª thread æœ€å¤šèƒ½æœ‰ 255 ä¸ª 32-bit register (ä¹Ÿå°±æ˜¯ 1020B), æ­¤æ—¶å¯ä»¥è®¤ä¸ºä¸€ä¸ª thread èƒ½è£…ä¸‹ä¸€ä¸ª head çš„æ‰€æœ‰å…ƒç´ .<br>ä½†æ˜¯ç”±äºç›®å‰ PA kernel åœ¨ <code>BLOCK_SIZE</code> ä¸º 16 çš„æƒ…å†µä¸‹ <code>THREAD_GROUP_SIZE</code> ç­‰äº 2, å› æ­¤ä¸€ä¸ª thread åªä¼šè£…ä¸€ä¸ª head çš„ä¸€åŠå…ƒç´ , è¿™æ ·å¯èƒ½ä¼šå¯¼è‡´ register çš„ä½¿ç”¨ç‡ä¸é«˜.</p></blockquote><hr><p>æ¥ç€è¿›å…¥ç¬¬ä¸‰ä¸ªå¾ªç¯, ç›®çš„æ˜¯è®© thread group ä» K cache ä¸­è¯»ä¸€ä¸ª head, å¹¶å­˜å…¥ <code>k_vecs</code> ä¸­:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// x == THREAD_GROUP_SIZE * VEC_SIZE
</span></span></span><span class=line><span class=cl><span class=c1>// Each thread group fetches x elements from the key at a time.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>constexpr</span> <span class=kt>int</span> <span class=n>x</span> <span class=o>=</span> <span class=mi>16</span> <span class=o>/</span> <span class=k>sizeof</span><span class=p>(</span><span class=n>cache_t</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=c1>//...
</span></span></span><span class=line><span class=cl><span class=c1>// Loop 1
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>block_idx</span> <span class=o>=</span> <span class=n>start_block_idx</span> <span class=o>+</span> <span class=n>warp_idx</span><span class=p>;</span> <span class=n>block_idx</span> <span class=o>&lt;</span> <span class=n>end_block_idx</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>block_idx</span> <span class=o>+=</span> <span class=n>NUM_WARPS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Loop 2
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_TOKENS_PER_THREAD_GROUP</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>K_vec</span> <span class=n>k_vecs</span><span class=p>[</span><span class=n>NUM_VECS_PER_THREAD</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Loop 3
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>NUM_VECS_PER_THREAD</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=k>const</span> <span class=n>cache_t</span><span class=o>*</span> <span class=n>k_ptr</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>                <span class=n>k_cache</span> <span class=o>+</span> <span class=n>physical_block_number</span> <span class=o>*</span> <span class=n>kv_block_stride</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>                <span class=n>kv_head_idx</span> <span class=o>*</span> <span class=n>kv_head_stride</span> <span class=o>+</span> <span class=n>physical_block_offset</span> <span class=o>*</span> <span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=k>const</span> <span class=kt>int</span> <span class=n>vec_idx</span> <span class=o>=</span> <span class=n>thread_group_offset</span> <span class=o>+</span> <span class=n>j</span> <span class=o>*</span> <span class=n>THREAD_GROUP_SIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=k>const</span> <span class=kt>int</span> <span class=n>offset1</span> <span class=o>=</span> <span class=p>(</span><span class=n>vec_idx</span> <span class=o>*</span> <span class=n>VEC_SIZE</span><span class=p>)</span> <span class=o>/</span> <span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=k>const</span> <span class=kt>int</span> <span class=n>offset2</span> <span class=o>=</span> <span class=p>(</span><span class=n>vec_idx</span> <span class=o>*</span> <span class=n>VEC_SIZE</span><span class=p>)</span> <span class=o>%</span> <span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=c1>// if Fp8KVCacheDataType::kAuto
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=n>k_vecs</span><span class=p>[</span><span class=n>j</span><span class=p>]</span> <span class=o>=</span> <span class=o>*</span><span class=k>reinterpret_cast</span><span class=o>&lt;</span><span class=k>const</span> <span class=n>K_vec</span><span class=o>*&gt;</span><span class=p>(</span>
</span></span><span class=line><span class=cl>              <span class=n>k_ptr</span> <span class=o>+</span> <span class=n>offset1</span> <span class=o>*</span> <span class=n>BLOCK_SIZE</span> <span class=o>*</span> <span class=n>x</span> <span class=o>+</span> <span class=n>offset2</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></div><p>è€è§„çŸ©, å…ˆçœ‹ <code>j</code>, æœ¬è´¨å°±æ˜¯ä» 0 è¿­ä»£åˆ° <code>NUM_VECS_PER_THREAD</code>, æ¯æ¬¡è¿­ä»£å½“å‰ thread è¯»å–ä¸€ä¸ª VEC å­˜å…¥ <code>k_vecs</code> ä¸­.</p><blockquote><p>ğŸ”” å›é¡¾:</p><ol><li><code>NUM_VECS_PER_THREAD</code> è¡¨ç¤ºä¸€ä¸ª head è¢«åˆ†æˆå¤šå°‘ä¸ª 16B.</li><li><code>k_cache</code> çš„ shape ä¸º <code>(num_blocks, num_kv_heads, head_size/x, block_size, x)</code>.</li></ol></blockquote><p>å…¶ä¸­çš„ <code>x</code> è¡¨ç¤ºä¸€ä¸ª thread group éœ€è¦è¯»å–çš„å…ƒç´ æ•°é‡ (<code>VEC_SIZE</code> * <code>THREAD_GROUP_SIZE</code>); å› æ­¤ä½œè€…å°† K Cache çš„ layout çš„æœ€åä¸€ç»´è®¾ç½®ä¸º <code>x</code> å…¶å®ä¹Ÿæ˜¯æ–¹ä¾¿åç»­ thread group å¯¹ K cache çš„è¯»å–.</p><p>ä¸‹å›¾å…·ä½“å±•ç¤ºäº†å¯»å€çš„è¿‡ç¨‹:</p><p><img alt=pa-cal-kq-03.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/pa-cal-kq-03.png></p><p>å…¶ä¸­:</p><ul><li>åœ¨ MHSA ä¸­, <code>num_kv_heads</code> ç­‰äº <code>num_heads</code>; è€Œåœ¨ GQA, MQA ä¸­, <code>num_kv_heads</code> å°äº <code>num_heads</code>.</li><li>(1) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread å±äºçš„ warp è¦å¤„ç†å“ªä¸ª PA block.</li><li>(2) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread è¦è®¡ç®—çš„ head åœ¨ K cache ä¸­çš„ä½ç½®. è¿™ä¸ª head çš„ç´¢å¼•å’Œ Q ä¸­ head çš„ç´¢å¼•åœ¨ MHSA ä¸­ç›¸åŒ.</li><li>(3) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread group è¦è®¡ç®—çš„ token åœ¨å½“å‰ PA block ä¸­çš„ä½ç½®.</li><li>(5) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread åœ¨éœ€è¦è¯»å–çš„ head (è“è‰²é•¿æ–¹ä½“) ä¸­ x çš„åç§», é€šè¿‡ <code>j</code> è¿›è¡Œè¿­ä»£è¯»å–. <strong>æ¯æ¬¡å¾ªç¯ thread group ä¸­çš„æ‰€æœ‰ thread å–ä¸€ä¸ª x.</strong></li><li>(6) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread åœ¨ thread gruop ä¸­è¯»å–çš„ x ä¸­ VEC çš„åç§»; thread ä¸€æ¬¡è¯»å–ä¸€ä¸ª VEC.</li></ul><p>ğŸ¤” <strong>ä¸ºä»€ä¹ˆ (5) åœ¨å®é™…å¯»å€æ—¶éœ€è¦ <code>* BLOCK_SIZE * x</code> ?</strong><br>ç­”: è¿™æ˜¯æ ¹æ® <code>k_cache</code> çš„ layout å¾—åˆ°çš„ stride. åŒç† (3) <code>* x</code> ä¹Ÿæ˜¯ stride.</p><p>ç¬¬ 3 ä¸ªå¾ªç¯ç»“æŸæ—¶å½“å‰ warp è´Ÿè´£çš„æ¯ä¸ª token ä¸­éœ€è¦çš„ K cache head å·²ç»å…¨è¢«åŠ è½½å…¥ thread æœ¬åœ°çš„ <code>k_vecs</code> ä¸­äº†.</p><p>ç”±äºä¸€ä¸ª thread group çš„ <code>k_vecs</code> æ‰èƒ½çœŸæ­£ç»„æˆä¸€ä¸ª head, åœ¨é€€å›ç¬¬äºŒä¸ªå¾ªç¯è¿›è¡Œ QK dot çš„æ—¶å€™, éœ€è¦åšä¸ª reduction, å…·ä½“çš„èŒƒå›´å°±æ˜¯ <code>THREAD_GROUP_SIZE</code> ä¸ª thread:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Loop 1
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>block_idx</span> <span class=o>=</span> <span class=n>start_block_idx</span> <span class=o>+</span> <span class=n>warp_idx</span><span class=p>;</span> <span class=n>block_idx</span> <span class=o>&lt;</span> <span class=n>end_block_idx</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>block_idx</span> <span class=o>+=</span> <span class=n>NUM_WARPS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Loop 2
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_TOKENS_PER_THREAD_GROUP</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>K_vec</span> <span class=n>k_vecs</span><span class=p>[</span><span class=n>NUM_VECS_PER_THREAD</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Loop 3
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>NUM_VECS_PER_THREAD</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=kt>float</span> <span class=n>qk</span> <span class=o>=</span> <span class=n>scale</span> <span class=o>*</span> <span class=n>Qk_dot</span><span class=o>&lt;</span><span class=n>scalar_t</span><span class=p>,</span> <span class=n>THREAD_GROUP_SIZE</span><span class=o>&gt;::</span><span class=n>dot</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                             <span class=n>q_vecs</span><span class=p>[</span><span class=n>thread_group_offset</span><span class=p>],</span> <span class=n>k_vecs</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></div><p>è®¡ç®—å®Œ <code>qk</code> å, ç”±å½“å‰ thread group ä¸­ç¬¬ä¸€ä¸ª (offset ä¸º 0) çš„ thread å¯¹è‡ªå·±åˆšæ‰ç®—å‡ºæ¥çš„ <code>qk</code> è¿›è¡Œ mask, é¡ºä¾¿çœ‹çœ‹å¦‚æœæ²¡æœ‰ mask æ‰, æŠŠ <code>qk_max</code> èµ‹å€¼ä¸º <code>qk</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>thread_group_offset</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Store the partial reductions to shared memory.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// NOTE(woosuk): It is required to zero out the masked logits.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>const</span> <span class=kt>bool</span> <span class=n>mask</span> <span class=o>=</span> <span class=n>token_idx</span> <span class=o>&gt;=</span> <span class=n>seq_len</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span><span class=p>[</span><span class=n>token_idx</span> <span class=o>-</span> <span class=n>start_token_idx</span><span class=p>]</span> <span class=o>=</span> <span class=n>mask</span> <span class=o>?</span> <span class=mf>0.f</span> <span class=o>:</span> <span class=n>qk</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Update the max value.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>qk_max</span> <span class=o>=</span> <span class=n>mask</span> <span class=o>?</span> <span class=nl>qk_max</span> <span class=p>:</span> <span class=n>fmaxf</span><span class=p>(</span><span class=n>qk_max</span><span class=p>,</span> <span class=n>qk</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>ğŸ§ <strong>ä¸ºä»€ä¹ˆè¦åš mask?</strong></p><ul><li>å› ä¸ºä¸€ä¸ª seq çš„æœ€åä¸€ä¸ª PA block å¯èƒ½è¦†ç›–ä¸æ»¡ <code>BLOCK_SIZE</code> ä¸ª token. è¿™é‡Œçš„ mask å°±æ˜¯æŠŠé‚£éƒ¨åˆ† qk ç½®é›¶.</li></ul><h3 id=54-softmax>5.4. Softmax<a hidden class=anchor aria-hidden=true href=#54-softmax>#</a></h3><p>æˆ‘å‹’ä¸ª QK å•Š, æ€»ç®—ç®—å®Œäº†, é”å…‹ five éƒ½è¦è¢«æŠ½æ¸…ä»“äº†. é¡µæ„ä¸çœŸ, é‰´å®šä¸ºå¼€ç®— softmax.</p><p>ä¸»è¦æ­¥éª¤å°±æ˜¯å¹¿æ’­ç„¶åç®—, ç®— softmax éœ€è¦çŸ¥é“æ¯ä¸ª head å¯¹åº”çš„ qk çš„æœ€å¤§å€¼. ç”±äºä¸€ä¸ª cuda block è´Ÿè´£çš„å°±æ˜¯ä¸€ä¸ª head, å¯¹äºè¿™ä¸ª head ä¸Šé¢çš„è®¡ç®—æ­¥éª¤ä¸€å…±ç®—äº† <code>cache_len</code>ä¸ª token çš„ qk, å› æ­¤éœ€è¦åšä¸€ä¸ª cuda block èŒƒå›´çš„è§„çº¦, æ‰¾åˆ°å…¶ä¸­æœ€å¤§çš„ qk å€¼.</p><p>å…ˆåœ¨ warp å±‚é¢è§„çº¦.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>__shared__</span> <span class=kt>float</span> <span class=n>red_smem</span><span class=p>[</span><span class=mi>2</span> <span class=o>*</span> <span class=n>NUM_WARPS</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=c1>// Perform reduction across the threads in the same warp to get the
</span></span></span><span class=line><span class=cl><span class=c1>// max qk value for each &#34;warp&#34; (not across the thread block yet).
</span></span></span><span class=line><span class=cl><span class=c1>// The 0-th thread of each thread group already has its max qk value.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>mask</span> <span class=o>=</span> <span class=n>WARP_SIZE</span> <span class=o>/</span> <span class=mi>2</span><span class=p>;</span> <span class=n>mask</span> <span class=o>&gt;=</span> <span class=n>THREAD_GROUP_SIZE</span><span class=p>;</span> <span class=n>mask</span> <span class=o>/=</span> <span class=mi>2</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>qk_max</span> <span class=o>=</span> <span class=n>fmaxf</span><span class=p>(</span><span class=n>qk_max</span><span class=p>,</span> <span class=n>VLLM_SHFL_XOR_SYNC</span><span class=p>(</span><span class=n>qk_max</span><span class=p>,</span> <span class=n>mask</span><span class=p>));</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>lane</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>red_smem</span><span class=p>[</span><span class=n>warp_idx</span><span class=p>]</span> <span class=o>=</span> <span class=n>qk_max</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>__syncthreads</span><span class=p>();</span>
</span></span></code></pre></div><ul><li><code>red_smem</code> æ˜¯ä¹‹å‰ç”³è¯·çš„ shared memory.</li><li><code>VLLM_SHFL_XOR_SYNC</code> æ˜¯ä¸€ä¸ª warp å†…çš„ shuffle æ“ä½œ, å…·ä½“æ¥è¯´, åœ¨æ¯æ¬¡å¾ªç¯æ—¶, æ¯ä¸ª thread å’Œè‡ªå·±ç›¸è· <code>mask</code> ä½ç½®çš„çº¿ç¨‹äº¤æ¢æ•°æ® (äº¤æ¢æ¥çš„æ•°æ®é€šè¿‡ <code>fmaxf</code> æ¯”è¾ƒ), å¹¶ä¸” <code>mask</code> ä¼šé€æ¸å‡åŠ, ç›´åˆ° <code>THREAD_GROUP_SIZE</code> ä¸ºæ­¢.</li><li><code>lane</code> è¡¨ç¤ºå½“å‰ warp ä¸­çš„çº¿ç¨‹ç´¢å¼•.</li></ul><p>æ¥ç€å†å¯¹æ¯ä¸ª warp çš„æœ€å¤§å€¼è¿›è¡Œè§„çº¦, ç”±äºæ¯ä¸ª warp çš„æœ€å¤§å€¼éƒ½è¢«å­˜å…¥äº† <code>red_smem</code> ä¸­, æ‰€ä»¥åªéœ€è¦å†æ¬¡è¿›è¡Œ shuffle æ“ä½œå³å¯.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// TODO(woosuk): Refactor this part.
</span></span></span><span class=line><span class=cl><span class=c1>// Get the max qk value for the sequence.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>qk_max</span> <span class=o>=</span> <span class=n>lane</span> <span class=o>&lt;</span> <span class=n>NUM_WARPS</span> <span class=o>?</span> <span class=n>red_smem</span><span class=p>[</span><span class=n>lane</span><span class=p>]</span> <span class=o>:</span> <span class=o>-</span><span class=n>FLT_MAX</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>mask</span> <span class=o>=</span> <span class=n>NUM_WARPS</span> <span class=o>/</span> <span class=mi>2</span><span class=p>;</span> <span class=n>mask</span> <span class=o>&gt;=</span> <span class=mi>1</span><span class=p>;</span> <span class=n>mask</span> <span class=o>/=</span> <span class=mi>2</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>qk_max</span> <span class=o>=</span> <span class=n>fmaxf</span><span class=p>(</span><span class=n>qk_max</span><span class=p>,</span> <span class=n>VLLM_SHFL_XOR_SYNC</span><span class=p>(</span><span class=n>qk_max</span><span class=p>,</span> <span class=n>mask</span><span class=p>));</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>æ­¤æ—¶, ç¬¬ 1 ä¸ªçº¿ç¨‹çš„ <code>qk_max</code> å°±æ˜¯å½“å‰ cuda block ä¸­æ‰€æœ‰ warp ä¸­æœ€å¤§çš„ qk å€¼. å°†å…¶å¹¿æ’­ç»™æ‰€æœ‰çº¿ç¨‹:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Broadcast the max qk value to all threads.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>qk_max</span> <span class=o>=</span> <span class=n>VLLM_SHFL_SYNC</span><span class=p>(</span><span class=n>qk_max</span><span class=p>,</span> <span class=mi>0</span><span class=p>);</span>
</span></span></code></pre></div><p>åœ¨è·å¾—äº† <code>qk_max</code> å, å°±å¯ä»¥è®¡ç®— softmax äº†:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Get the sum of the exp values.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>float</span> <span class=n>exp_sum</span> <span class=o>=</span> <span class=mf>0.f</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=n>thread_idx</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>num_tokens</span><span class=p>;</span> <span class=n>i</span> <span class=o>+=</span> <span class=n>NUM_THREADS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>float</span> <span class=n>val</span> <span class=o>=</span> <span class=n>__expf</span><span class=p>(</span><span class=n>logits</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>-</span> <span class=n>qk_max</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>val</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>exp_sum</span> <span class=o>+=</span> <span class=n>val</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>exp_sum</span> <span class=o>=</span> <span class=n>block_sum</span><span class=o>&lt;</span><span class=n>NUM_WARPS</span><span class=o>&gt;</span><span class=p>(</span><span class=o>&amp;</span><span class=n>red_smem</span><span class=p>[</span><span class=n>NUM_WARPS</span><span class=p>],</span> <span class=n>exp_sum</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// Compute softmax.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=kt>float</span> <span class=n>inv_sum</span> <span class=o>=</span> <span class=n>__fdividef</span><span class=p>(</span><span class=mf>1.f</span><span class=p>,</span> <span class=n>exp_sum</span> <span class=o>+</span> <span class=mf>1e-6</span><span class=n>f</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=n>thread_idx</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>num_tokens</span><span class=p>;</span> <span class=n>i</span> <span class=o>+=</span> <span class=n>NUM_THREADS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>*=</span> <span class=n>inv_sum</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>__syncthreads</span><span class=p>();</span>
</span></span></code></pre></div><h3 id=55-lv-logits--value>5.5. LV (Logits * Value)<a hidden class=anchor aria-hidden=true href=#55-lv-logits--value>#</a></h3><p><img alt=pa-cal.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/pa-cal.png></p><p>ä¸Šå›¾å±•ç¤ºäº† LV çš„è®¡ç®—è¿‡ç¨‹, ä¸»è¦åŒºåˆ«æ˜¯ç”±äºè¦è®¡ç®— Logits çš„ shape å¯ä»¥è¡¨ç¤ºä¸º <code>(num_heads, num_seqs, cache_len)</code>, è€Œ V çš„ shape å¯ä»¥è¡¨ç¤ºä¸º <code>(num_heads, cache_len, head_size)</code>, å› æ­¤ LV çš„çŸ©é˜µä¹˜æ³•ä¸­, æ¯è®¡ç®—ä¸€ä¸ªå…ƒç´ éœ€è¦è¯»å– logits çš„ä¸€è¡Œå’Œ V çš„ä¸€åˆ—è¿›è¡Œè®¡ç®—.</p><p>æ­¤æ—¶, ä¸€ä¸ª cuda block çš„èŒè´£ä» &ldquo;è‡ª Q ä¸­è¯»å–ä¸€ä¸ª head&rdquo; è½¬å˜ä¸º &ldquo;è®¡ç®— output ä¸­çš„ä¸€ä¸ª head&rdquo;.</p><p>ğŸ§ <strong>ä¸ºä»€ä¹ˆåœ¨è®¡ç®— LV æ—¶, å»æ‰äº† thread group çš„æ¦‚å¿µ, æ¯ä¸ª thread éƒ½è¢«è®¾å®šä¸ºæ¯æ¬¡è¯»å– 16B?</strong></p><ul><li>å› ä¸ºç°åœ¨æ¯è®¡ç®—ä¸€ä¸ªå…ƒç´ , éœ€è¦çš„è®¿å­˜é‡æ›´å¤§, å› æ­¤ç»™æ¯ä¸ª thread åˆ†é…äº†æ›´å¤šçš„æ•°æ®è¯»å–é‡. ä¹Ÿå°±æ˜¯è¯´, <code>V_VEC_SIZE</code> æ¯” <code>VEC_SIZE</code> æ›´å¤§.</li></ul><p>ç”±äº cuda è®¿å­˜æ¨¡å¼æŒ‰è¡Œè¯»å–æ›´å¿«, æ‰€ä»¥å®é™…çš„è®¡ç®—ç»“æœåœ¨éå† PA block æ—¶çº¿ç¨‹å†…éƒ¨åˆ©ç”¨ <code>accs</code> è¿›è¡Œç´¯è®¡ (ä»¥å®ç°ä¸ V çš„ä¸€åˆ—è¿›è¡Œè®¡ç®—çš„è¡Œä¸º):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>constexpr</span> <span class=kt>int</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>=</span> <span class=n>BLOCK_SIZE</span> <span class=o>/</span> <span class=n>V_VEC_SIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>constexpr</span> <span class=kt>int</span> <span class=n>NUM_ROWS_PER_ITER</span> <span class=o>=</span> <span class=n>WARP_SIZE</span> <span class=o>/</span> <span class=n>NUM_V_VECS_PER_ROW</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>constexpr</span> <span class=kt>int</span> <span class=n>NUM_ROWS_PER_THREAD</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>    <span class=n>DIVIDE_ROUND_UP</span><span class=p>(</span><span class=n>HEAD_SIZE</span><span class=p>,</span> <span class=n>NUM_ROWS_PER_ITER</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// NOTE(woosuk): We use FP32 for the accumulator for better accuracy.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>float</span> <span class=n>accs</span><span class=p>[</span><span class=n>NUM_ROWS_PER_THREAD</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>block_idx</span> <span class=o>=</span> <span class=n>start_block_idx</span> <span class=o>+</span> <span class=n>warp_idx</span><span class=p>;</span> <span class=n>block_idx</span> <span class=o>&lt;</span> <span class=n>end_block_idx</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>block_idx</span> <span class=o>+=</span> <span class=n>NUM_WARPS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>V_vec</span> <span class=n>v_vec</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_ROWS_PER_THREAD</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>V_VEC_SIZE</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=c1>// Load V to `v_vec` ...
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=n>v_vec_ptr</span><span class=p>[</span><span class=n>j</span><span class=p>]</span> <span class=o>=</span> <span class=n>token_idx</span> <span class=o>+</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>seq_len</span> <span class=o>?</span> <span class=n>v_vec_ptr</span><span class=p>[</span><span class=n>j</span><span class=p>]</span> <span class=o>:</span> <span class=n>zero_value</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Accumulate the dot product.
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>accs</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+=</span> <span class=n>dot</span><span class=p>(</span><span class=n>logits_vec</span><span class=p>,</span> <span class=n>v_vec</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>ç”±äºæ¯ä¸ªçº¿ç¨‹è´Ÿè´£çš„ç´¯è®¡éƒ¨åˆ†ä¸æ»¡ä¸€æ•´è¡Œ/åˆ—, æ‰€ä»¥è¿›è¡Œè§„çº¦:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Perform reduction within each warp.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span>  <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_ROWS_PER_THREAD</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>float</span> <span class=n>acc</span> <span class=o>=</span> <span class=n>accs</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>mask</span> <span class=o>=</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>/</span> <span class=mi>2</span><span class=p>;</span> <span class=n>mask</span> <span class=o>&gt;=</span> <span class=mi>1</span><span class=p>;</span> <span class=n>mask</span> <span class=o>/=</span> <span class=mi>2</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=n>acc</span> <span class=o>+=</span> <span class=n>VLLM_SHFL_XOR_SYNC</span><span class=p>(</span><span class=n>acc</span><span class=p>,</span> <span class=n>mask</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>accs</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>acc</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>// NOTE(woosuk): A barrier is required because the shared memory space for
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>// logits is reused for the output.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>// Perform reduction across warps.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=kt>float</span><span class=o>*</span> <span class=n>out_smem</span> <span class=o>=</span> <span class=k>reinterpret_cast</span><span class=o>&lt;</span><span class=kt>float</span><span class=o>*&gt;</span><span class=p>(</span><span class=n>shared_mem</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span>  <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=n>NUM_WARPS</span><span class=p>;</span> <span class=n>i</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>;</span> <span class=n>i</span> <span class=o>/=</span> <span class=mi>2</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>mid</span> <span class=o>=</span> <span class=n>i</span> <span class=o>/</span> <span class=mi>2</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Upper warps write to shared memory.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>if</span> <span class=p>(</span><span class=n>warp_idx</span> <span class=o>&gt;=</span> <span class=n>mid</span> <span class=o>&amp;&amp;</span> <span class=n>warp_idx</span> <span class=o>&lt;</span> <span class=n>i</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=kt>float</span><span class=o>*</span> <span class=n>dst</span> <span class=o>=</span> <span class=o>&amp;</span><span class=n>out_smem</span><span class=p>[(</span><span class=n>warp_idx</span> <span class=o>-</span> <span class=n>mid</span><span class=p>)</span> <span class=o>*</span> <span class=n>HEAD_SIZE</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span>      <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_ROWS_PER_THREAD</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>const</span> <span class=kt>int</span> <span class=n>row_idx</span> <span class=o>=</span> <span class=n>lane</span> <span class=o>/</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>+</span> <span class=n>i</span> <span class=o>*</span> <span class=n>NUM_ROWS_PER_ITER</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=n>row_idx</span> <span class=o>&lt;</span> <span class=n>HEAD_SIZE</span> <span class=o>&amp;&amp;</span> <span class=n>lane</span> <span class=o>%</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>          <span class=n>dst</span><span class=p>[</span><span class=n>row_idx</span><span class=p>]</span> <span class=o>=</span> <span class=n>accs</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// Lower warps update the output.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>if</span> <span class=p>(</span><span class=n>warp_idx</span> <span class=o>&lt;</span> <span class=n>mid</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=k>const</span> <span class=kt>float</span><span class=o>*</span> <span class=n>src</span> <span class=o>=</span> <span class=o>&amp;</span><span class=n>out_smem</span><span class=p>[</span><span class=n>warp_idx</span> <span class=o>*</span> <span class=n>HEAD_SIZE</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span>      <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_ROWS_PER_THREAD</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>const</span> <span class=kt>int</span> <span class=n>row_idx</span> <span class=o>=</span> <span class=n>lane</span> <span class=o>/</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>+</span> <span class=n>i</span> <span class=o>*</span> <span class=n>NUM_ROWS_PER_ITER</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=n>row_idx</span> <span class=o>&lt;</span> <span class=n>HEAD_SIZE</span> <span class=o>&amp;&amp;</span> <span class=n>lane</span> <span class=o>%</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>          <span class=n>accs</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+=</span> <span class=n>src</span><span class=p>[</span><span class=n>row_idx</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span></code></pre></div><p>æœ€åå†™å…¥åˆ°è¾“å‡ºä¸­:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl>  <span class=c1>// Write the final output.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>if</span> <span class=p>(</span><span class=n>warp_idx</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>scalar_t</span><span class=o>*</span> <span class=n>out_ptr</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>+</span> <span class=n>seq_idx</span> <span class=o>*</span> <span class=n>num_heads</span> <span class=o>*</span> <span class=n>max_num_partitions</span> <span class=o>*</span> <span class=n>HEAD_SIZE</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>        <span class=n>head_idx</span> <span class=o>*</span> <span class=n>max_num_partitions</span> <span class=o>*</span> <span class=n>HEAD_SIZE</span> <span class=o>+</span> <span class=n>partition_idx</span> <span class=o>*</span> <span class=n>HEAD_SIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_ROWS_PER_THREAD</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=k>const</span> <span class=kt>int</span> <span class=n>row_idx</span> <span class=o>=</span> <span class=n>lane</span> <span class=o>/</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>+</span> <span class=n>i</span> <span class=o>*</span> <span class=n>NUM_ROWS_PER_ITER</span><span class=p>;</span>
</span></span><span class=line><span class=cl>      <span class=k>if</span> <span class=p>(</span><span class=n>row_idx</span> <span class=o>&lt;</span> <span class=n>HEAD_SIZE</span> <span class=o>&amp;&amp;</span> <span class=n>lane</span> <span class=o>%</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>from_float</span><span class=p>(</span><span class=o>*</span><span class=p>(</span><span class=n>out_ptr</span> <span class=o>+</span> <span class=n>row_idx</span><span class=p>),</span> <span class=n>accs</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://jamesnulliu.github.io/zh/tags/python/>Python</a></li><li><a href=https://jamesnulliu.github.io/zh/tags/vllm/>Vllm</a></li><li><a href=https://jamesnulliu.github.io/zh/tags/attention/>Attention</a></li></ul><nav class=paginav><a class=prev href=https://jamesnulliu.github.io/zh/blogs/a-brief-talk-on-speculative-decoding/><span class=title>Â« ä¸Šä¸€é¡µ</span><br><span>æµ…è°ˆæŠ•æœºæ¨ç†</span></a></nav></footer><script src=https://giscus.app/client.js data-repo=jamesnulliu/jamesnulliu.github.io data-repo-id=R_kgDOMPCQIw data-category=Announcements data-category-id=DIC_kwDOMPCQI84Cgb2t data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async></script></article></main><footer class=footer><span>Â© 2024-2025 JamesNULLiu</span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="å¤åˆ¶";function s(){t.innerHTML="å·²å¤åˆ¶ï¼",setTimeout(()=>{t.innerHTML="å¤åˆ¶"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>