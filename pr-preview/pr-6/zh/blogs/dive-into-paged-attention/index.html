<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Dive into Paged Attention | 秋水·JamesNULLiu</title><meta name=keywords content="vllm,continuous-batching,paged-attention"><meta name=description content="Dive into the paged attention mechanism of vLLM."><meta name=author content="jamesnulliu"><link rel=canonical href=https://jamesnulliu.github.io/zh/blogs/dive-into-paged-attention/><link crossorigin=anonymous href=/assets/css/stylesheet.62cb9c488bb33c0e9a9d3c29b7f4259cbb0db25aaa19ba672188203d3d5bcaf9.css integrity="sha256-YsucSIuzPA6anTwpt/QlnLsNslqqGbpnIYggPT1byvk=" rel="preload stylesheet" as=style><link rel=icon href=https://jamesnulliu.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://jamesnulliu.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://jamesnulliu.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://jamesnulliu.github.io/apple-touch-icon.png><link rel=mask-icon href=https://jamesnulliu.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://jamesnulliu.github.io/blogs/dive-into-paged-attention/><link rel=alternate hreflang=zh href=https://jamesnulliu.github.io/zh/blogs/dive-into-paged-attention/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]]}}</script><meta property="og:url" content="https://jamesnulliu.github.io/zh/blogs/dive-into-paged-attention/"><meta property="og:site_name" content="秋水·JamesNULLiu"><meta property="og:title" content="Dive into Paged Attention"><meta property="og:description" content="Dive into the paged attention mechanism of vLLM."><meta property="og:locale" content="zh"><meta property="og:type" content="article"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2024-10-07T12:00:00+08:00"><meta property="article:modified_time" content="2025-09-12T22:39:56+00:00"><meta property="article:tag" content="Python"><meta property="article:tag" content="Vllm"><meta property="article:tag" content="Attention"><meta name=twitter:card content="summary"><meta name=twitter:title content="Dive into Paged Attention"><meta name=twitter:description content="Dive into the paged attention mechanism of vLLM."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://jamesnulliu.github.io/zh/blogs/"},{"@type":"ListItem","position":2,"name":"Dive into Paged Attention","item":"https://jamesnulliu.github.io/zh/blogs/dive-into-paged-attention/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Dive into Paged Attention","name":"Dive into Paged Attention","description":"Dive into the paged attention mechanism of vLLM.","keywords":["vllm","continuous-batching","paged-attention"],"articleBody":"1. 证明 Attention 的 $O_i$ 只与 $Q_i$ 有关 Attention 的公式如下:\n$$ O=Attention(Q,K,V)=softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$假设 $Q=\\begin{bmatrix}Q_0\\\\Q_1\\end{bmatrix}$, $K=\\begin{bmatrix}K_0\\\\K_1\\end{bmatrix}$\n那么:\n$$ O=softmax(\\frac{\\begin{bmatrix}Q_0K_0^T\u0026Q_0K_1^T\\\\Q_1K_0^T\u0026Q_1K_1^T\\end{bmatrix}}{\\sqrt{d_k}})V $$令:\n$$ A=\\begin{bmatrix}A_0\\\\A_1\\end{bmatrix}=\\begin{bmatrix}Q_0K_0^T\u0026Q_0K_1^T\\\\Q_1K_0^T\u0026Q_1K_1^T\\end{bmatrix},f(x)=\\frac{softmax(x)}{\\sqrt{d_k}} $$此时, $A_1$ 只和 $Q_1$ 有关, 和 $Q_0$ 无关, 那么:\n$$ \\begin{bmatrix}O_0\\\\O_1\\end{bmatrix}=O=\\begin{bmatrix}f(A_0)\\\\f(A_1)\\end{bmatrix}V=\\begin{bmatrix}f(A_0)V\\\\f(A_1)V\\end{bmatrix} $$因此, $O_i$ 只和 $A_i$ 相关, 而根据 $A$ 的设定, $A_i$ 只和 $Q_i$ 相关, 即:\nAttention 矩阵的第 $i$ 个输出只和第 $i$ 个 $Q$ 有关, 和之前的 $Q$ 无关.\n总结:\n在预测下一个 token 时，只需对新 token 计算对应的 Q_new，并与之前已经缓存的 K_cache 和 V_cache 进行注意力计算。 新的 K_new 和 V_new 会被加入到缓存中，继续为下一个 token 生成提供基础。 整个过程避免了对所有历史 token 的重复计算，大幅提高了效率。 2. KV Cache 的增量过程 2.1. 初始输入（完整序列）计算： 对于初始的输入序列 (seq_len, embed_dim)，我们通过线性变换得到 Q、K 和 V，它们的形状都是 (seq_len, embed_dim)。 使用 Q 和 K 进行点积计算注意力分数，然后结合 V 计算得到输出 (seq_len, embed_dim)，这是第一次对初始序列的完整计算。 2.2. 预测下一个 token 时的增量计算： 在预测下一个 token 时，不需要对整个序列再进行完整的 Q、K、V 计算，而是只需对新生成的 token 进行一次增量计算。这时的操作流程如下：\n输入新的 token：将已经生成的 token（其形状为 (embed_dim,)）作为输入，通过线性变换得到该 token 对应的 Q_new，形状为 (embed_dim,)。 与之前缓存的 K 和 V 进行注意力计算：使用 Q_new 与之前已经计算并缓存的 K_cache 和 V_cache 进行注意力计算。这里的 K_cache 和 V_cache 分别是之前每次生成 token 时得到的 K 和 V，它们的形状是 (seq_len, embed_dim)，即缓存了从最初输入序列到当前已经生成的所有 token 的 K 和 V。Q_new 可以直接与 K_cache 进行点积，得到注意力分数，然后结合 V_cache 得到新的输出。 更新 KV Cache：新的 K_new 和 V_new 会通过线性变换得到（形状为 (embed_dim,)），并将它们添加到 K_cache 和 V_cache 的末尾，使得缓存的 K_cache 和 V_cache 不断增大，以备后续使用。 输出：通过注意力计算后的输出形状为 (embed_dim,)，即新生成的 token。 4. vllm 中的 Paged Attention 4.1. 动机: Memory Wastes 上图展示了可能的内存浪费情况, 主要时输入 sequence 不知道 eos 在哪里, 如果随机申请内存, 可能导致大量内存碎片, 因此吞吐量下降.\n4.2. 解决方案: 用 Page 管理内存 上图展示了 vLLM 用 Paged 管理内存具体怎么做的.\n简单来说, vLLM 在开始推理前为每个 Decoder Layer 申请两个巨长的 Tensor (k_cache 和 v_cache), 把 Tensor 分割成连续等长的 PA blocks (图中的一行为一个 PA Block); 每个 PA Block 能够存放 BLOCK_SIZE 个 token 的 K 或 V cache (每个 cache 的形状可以理解为 (num_heads, head_size)).\n因此, k_cache 和 v_cache 的形状可以理解为 (num_blocks, num_heads, head_size).\n对于一个连续的 sequnce, 在 prefill 阶段前就会分配好它的 PA blocks, 之后推理时:\n若是计算 prompt 的 Attention, 则先把传入的 K 和 V 按照 PA blocks 存入 k_cache 和 v_cache 中; 然后利用整段的 QKV 计算 attention. 若是计算新 token, 则利用 Q 和 block table 计算 decode 阶段的 attntion; 此时访存的就是 k_cache 和 v_cache 中的 PA blocks. 5. Paged Attention Kernel 详解 References:\nvLLM Paged Attention vLLM皇冠上的明珠：深入浅出理解PagedAttention CUDA实现 先看下整体计算流程图 (这个图后面也会出现这里先看一眼):\n5.1. 输入输出输出分析和参数说明 // Grid: (num_heads, num_seqs, 1). template\u003c typename scalar_t, int HEAD_SIZE, int BLOCK_SIZE, int NUM_THREADS, int PARTITION_SIZE = 0\u003e __device__ void paged_attention_kernel( ... // Other side args. const scalar_t* __restrict__ out, // [num_seqs, num_heads, max_num_partitions, head_size] const scalar_t* __restrict__ q, // [num_seqs, num_heads, head_size] const scalar_t* __restrict__ k_cache, // [num_blocks, num_kv_heads, head_size/x, block_size, x] const scalar_t* __restrict__ v_cache, // [num_blocks, num_kv_heads, head_size, block_size] ... // Other side args. ) 模板参数说明:\nscalar_t 元素类型 (实际代码中还有 cache_t 表示 KV cache 的元素类型). HEAD_SIZE 每个 head 中元素数量. BLOCK_SIZE 每个 PA block 中的 token 数量. KV cache 被存储在不同 PA blocks. 每个 PA block 存储一个 head 中 BLOCK_SIZE 个 token.\n例如, 若 BLOCK_SIZE=16, HEAD_SIZE=128, 则一个 PA block 能存储一个 head 的 16 * 128 = 2048 个元素. 每个 PA block 可能只包含一部分的 context tokens. 从 page 角度看, KV cache 是若干个 page 的集合; NUM_THREADS 每个 CUDA thread block 中 thread 的数量. PARTITION_SIZE 参与 TP 的 GPU 数量, 默认 0 表示单卡. (以下都以单卡为例说明) 额外的一些参数:\nnum_seqs: 本次推理请求 sequence 数目. 由于这个 kernel 只处理 decode 阶段单 query attention, 所以实际上每个 sequence 只有一个 query token.\nnum_heads: Q 的 head 数目 num_kv_heads: KV 的 head 数目, 对于 MHA 其值和 num_heads 相同; 如果是 GQA, MQA 则 num_kv_heads 小于 num_head. head_size: 即 HEAD_SIZE k_cache: (num_blocks, num_kv_heads, head_size/x, block_size, x), 其中 x 表示 THREAD_GROUP_SIZE * VEC_SIZE 的大小 (后面会细说). 下面结合 GPU architecture 初步分析一下参数.\n🧐 为什么要分 thread group?\n因为当一个 cuda block 要取的数据比较少的时候 (计算 QK), 一个 thread group 分别一次取 Q 和 K 中 16B; 当一个 cuda block 要取的数据比较多的时候 (计算 LV), 一个 thread 取 16B. 5.2.Shared Memory: q_vecs 的写入 从 kernel 中的第一个申请的 shared memory 开始说.\n关于 shared memeory:\n在 kernel 中申请的 shared memory 被当前 cuda block 中的所有 thread 共享. shared memory 的作用是为了减少 global memory 的访问次数，提高访存效率. 以下代码申请了一块 shared memroy 被整个 CUDA Block 中所有 kernel 共享:\n__shared__ Q_vec q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD]; 首先, q_vecs 覆盖了 Q 中 head_size 个元素 - 这也是一个 cuda block 需要处理的数据量.\n接着再说两个维度的参数的意思:\nconstexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE; constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE; THREAD_GROUP_SIZE: 每个 thread group 中的 thread 数量. 注意, 一个 cuda block 中有 NUM_THREADS 个 thread, NUM_THREAD_GROUPS 个 thread group. THREAD_GROUP_SIZE = MAX(WARP_SIZE/BLOCK_SIZE, 1). NUM_VECS_PER_THREAD: HEAD_SIZE 能被分成多少个 16B. (这个变量这么命名的理由是后面读取 K 的时候每个 thread 会往自己的寄存器内读 NUM_VECS_PER_THREAD 个 k_vec.) 证明: q_vecs 覆盖 Q 的一个 head, 并且 NUM_VECS_PER_THREAD 表示 Q 的一个 head 被分成多少个 16B.\n=\u003e THREAD_GROUP_SIZE * VEC_SIZE = 16B / sizeof(scalar_t);\n=\u003e NUM_VECS_PER_THREAD * 16B / sizeof(scalar_t) = HEAD_SIZE;\n然后看 load Q 的代码, 建议结合下面的图一起看:\n// Load Q to shmem #pragma unroll for (int i = thread_group_idx; i \u003c NUM_VECS_PER_THREAD; i += NUM_THREAD_GROUPS) { const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE; q_vecs[thread_group_offset][i] = *reinterpret_cast\u003cconst Q_vec*\u003e(q_ptr + vec_idx * VEC_SIZE); } thread_group_idx 表示当前 thread 属于当前 cuda block 中第几个 thread group. thread_group_offset 表示当前 thread 在当前 thread group 中是第几个 thread. 上图展示了循环具体是怎么跑的.\n一个紫色箭头表示一个 thread group. NUM_VECS_PER_THREAD 表示 HEAD_SIZE 能被分成多少个 16B. 实际读取 Q 的内存时, 所有 thread group 从 Q 的起始位置紧密排列, 根据图上看的话一共有 NUM_THREAD_GROUPS 个紫色箭头. 所有 thread group 读取一次 Q 并存入 q_vecs 对应循环中的一次迭代; 因此下次迭代 thread group 需要向后偏移 NUM_THREAD_GROUPS 个位置 (例如 i 从 1 变为 7). 此外, 读一次 16B 对应一个 thread 来说自然也是取一个 VEC. 对应到 kernel 编写, 还需要计算当前 thread 具体读取哪个 vec; 因此得到 vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE. 🤔 这里会不会有 bank conflict?\n总之现在我们把 (1, head_size) 大小的元素读到了 cuda block 共享的 shared memory q_vecs 中.\n5.3. 读取 K Cache 并计算 QK 现在从 cuda block 的角度看, 当前 block 已经获得了自己要算的 Q 中的一个 head (形状为 (1, head_size)), 接下来就是计算 Q 和 K 的点积.\n点积过程是把当前 block 拥有的 Q head 和整个 K Cache (迭代地) 进行点积运算. 参考下图:\nQK 乘积实际上被暂存在 logits (也是一块 shared memory) 中, 之后会被用来计算 softmax.\n😇 看下循环的具体代码吧:\n// Loop 1 for (int block_idx = start_block_idx + warp_idx; block_idx \u003c end_block_idx; block_idx += NUM_WARPS) { // Physical block calculation ... // Loop 2 for (int i = 0; i \u003c NUM_TOKENS_PER_THREAD_GROUP; i++) { // Offset calculation ... K_vec k_vecs[NUM_VECS_PER_THREAD]; // Loop 3 #pragma unroll for (int j = 0; j \u003c NUM_VECS_PER_THREAD; j++) { // Load K to `k_vecs` ... } float qk = scale * Qk_dot\u003cscalar_t, THREAD_GROUP_SIZE\u003e::dot( q_vecs[thread_group_offset], k_vecs); // Add the ALiBi bias if slopes are given. qk += (alibi_slope != 0) ? alibi_slope * (token_idx - seq_len + 1) : 0; if (thread_group_offset == 0) { // Store the partial reductions to shared memory. // Mask // Update the max value. } } } 先说第一个循环, 其中比较重要的几个参数定义如下:\n// [start_block_idx, end_block_idx) is the range of blocks to process. const int start_block_idx = USE_PARTITIONING ? partition_idx * num_blocks_per_partition : 0; // If not using partitioning, `end_block_idx` should be equal to `num_seq_blocks`. const int end_block_idx = MIN(start_block_idx + num_blocks_per_partition, num_seq_blocks); // Number of blocks to process. const int num_blocks = end_block_idx - start_block_idx; 用文字描述就是:\nblk_idx 表示当前 thread 所在 warp 需要处理的 PA block 的在 block_table 中索引 (逻辑上的索引). start_block_idx 和 end_block_idx 表示当前 cuda block 需要处理的 block 范围. num_blocks 表示当前 cuda block 需要处理的 block 数量. NUM_WARPS 表示当前 cuda block 中 warp 的数量. 一个 warp 包含 32 个 thread. warp_idx 表示当前 warp 在当前 cuda block 中的索引. 说人话就是每个 warp 处理一个 PA block, 一开始 cuda block 中的所有 warp 紧密地指向最前面的 NUM_WARPS 个 PA block, 每次循环所有 warp 向后偏移 NUM_WARPS 个 PA block 的长度. 参考下图:\n🔔 这里再回顾一下, 一个 PA block 里存放了 BLOCK_SIZE 个 token 的 K 或 V cache.\n所以说这个循环和上面读取 Q 的循环一个尿性🤮, 不过是以 warp 的粒度处理数据;\n进入了第一个循环内部, 第一步当然是计算当前 thread 对应的 warp 应该计算哪个 PA block (物理上的索引), 因此得到了 physical_block_number:\nconst int64_t physical_block_number = static_cast\u003cint64_t\u003e(block_table[block_idx]); 然后解释第二个循环, 第二个循环的整体目标就是让当前 warp 计算好自己负责的 PA block 中 BLOCK_SIZE 个 token 的 QK 乘积.\n先看一下 i 的上界:\nconstexpr int NUM_TOKENS_PER_THREAD_GROUP = DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE); // ... // Loop 1 for (int block_idx = start_block_idx + warp_idx; block_idx \u003c end_block_idx; block_idx += NUM_WARPS) { // Loop 2 for (int i = 0; i \u003c NUM_TOKENS_PER_THREAD_GROUP; i++) { // ... } // ... } 从 kernel 角度看, 每个 thread 需要辅助当前 warp 计算自己负责的一整个 PA block (包含 BLOCK_SIZE 个 token), 而我们把这个过程拆分为 Loop 2 中的 NUM_TOKEN_PER_THREAD_GROUP (也就是 ceil(BLOCK_SIZE / 32)) 次循环;\n说人话就是一个 thread group 对应一个 token 中的一个 head, 如果 BLOCK SIZE 太大了后面每个 thread 向后偏移 i * WARP_SIZE 个 token 继续狠狠算🤣.\n也因此第二个循环内部一上来先计算了几个偏移量, 并且申请了 thread 内部私有的 k_vecs 数组:\nconst int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE; const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset; K_vec k_vecs[NUM_VECS_PER_THREAD]; thread_group_idx 表示当前 thread group 在整个 cuda block 中的索引. ☢️ 一个 thread group 在一次循环中负责 fetch 一个 PA block 中 K cache 的一个 token 中自己负责的 head. ☢️ 一个 thread group 负责计算一个 qk 值; 这个值显然是由一个 Q head 和一个 K head 点积得到的. physical_block_offset 表示当前要算的 token 在当前 PA block 中的偏移量 (注意和前面的 physical_block_number 区分). 加 i * WARP_SIZE 的原因是如果 BLOCK_SIZE 大于 32, 那么一个 warp 要多次循环才能处理完一个 PA block 中的所有 token, 对应 thread_group_idx 需要做偏移. token_idx 表示当前要算的 token 在整个 seq 的 KV cache 中的索引. k_vecs 中能存放 NUM_VECS_PER_THREAD 个 VEC, 而一整个 thread group 中所有的 thread 的 k_vecs 合起来才能组成一个 K 的 head (推导参考上面 Q 的 😇). 这就是为什么后面算 QK 的时候要 reduce. 🤔 看到这里读者可能有一个问题: 一个 token 的 K cache 应该对应多个 head, 为什么上面说一个 thread group 只负责一个 head?\n答: 因为实际计算的时候, 一个 cuda block 只负责计算一个 head, 对应到 K Cache 乃至后面 V Cache 的位置也是一样的.\n这里额外说一下, 读 K 的 head 的一个目标应该是在尽量少的 register 中装下一个 head 的所有元素, 这样后续和 shared memory 中的 Q 做点乘并规约的速度更快. 假设一个 head 有 128 个 float16, 则占用 256B, 而 A100 中一个 thread 最多能有 255 个 32-bit register (也就是 1020B), 此时可以认为一个 thread 能装下一个 head 的所有元素.\n但是由于目前 PA kernel 在 BLOCK_SIZE 为 16 的情况下 THREAD_GROUP_SIZE 等于 2, 因此一个 thread 只会装一个 head 的一半元素, 这样可能会导致 register 的使用率不高.\n接着进入第三个循环, 目的是让 thread group 从 K cache 中读一个 head, 并存入 k_vecs 中:\n// x == THREAD_GROUP_SIZE * VEC_SIZE // Each thread group fetches x elements from the key at a time. constexpr int x = 16 / sizeof(cache_t); //... // Loop 1 for (int block_idx = start_block_idx + warp_idx; block_idx \u003c end_block_idx; block_idx += NUM_WARPS) { // Loop 2 for (int i = 0; i \u003c NUM_TOKENS_PER_THREAD_GROUP; i++) { K_vec k_vecs[NUM_VECS_PER_THREAD]; // Loop 3 for (int j = 0; j \u003c NUM_VECS_PER_THREAD; j++) { const cache_t* k_ptr = k_cache + physical_block_number * kv_block_stride + kv_head_idx * kv_head_stride + physical_block_offset * x; const int vec_idx = thread_group_offset + j * THREAD_GROUP_SIZE; const int offset1 = (vec_idx * VEC_SIZE) / x; const int offset2 = (vec_idx * VEC_SIZE) % x; // if Fp8KVCacheDataType::kAuto k_vecs[j] = *reinterpret_cast\u003cconst K_vec*\u003e( k_ptr + offset1 * BLOCK_SIZE * x + offset2); } // ... } // ... } 老规矩, 先看 j, 本质就是从 0 迭代到 NUM_VECS_PER_THREAD, 每次迭代当前 thread 读取一个 VEC 存入 k_vecs 中.\n🔔 回顾:\nNUM_VECS_PER_THREAD 表示一个 head 被分成多少个 16B. k_cache 的 shape 为 (num_blocks, num_kv_heads, head_size/x, block_size, x). 其中的 x 表示一个 thread group 需要读取的元素数量 (VEC_SIZE * THREAD_GROUP_SIZE); 因此作者将 K Cache 的 layout 的最后一维设置为 x 其实也是方便后续 thread group 对 K cache 的读取.\n下图具体展示了寻址的过程:\n其中:\n在 MHSA 中, num_kv_heads 等于 num_heads; 而在 GQA, MQA 中, num_kv_heads 小于 num_heads. (1) 负责找到当前 thread 属于的 warp 要处理哪个 PA block. (2) 负责找到当前 thread 要计算的 head 在 K cache 中的位置. 这个 head 的索引和 Q 中 head 的索引在 MHSA 中相同. (3) 负责找到当前 thread group 要计算的 token 在当前 PA block 中的位置. (5) 负责找到当前 thread 在需要读取的 head (蓝色长方体) 中 x 的偏移, 通过 j 进行迭代读取. 每次循环 thread group 中的所有 thread 取一个 x. (6) 负责找到当前 thread 在 thread gruop 中读取的 x 中 VEC 的偏移; thread 一次读取一个 VEC. 🤔 为什么 (5) 在实际寻址时需要 * BLOCK_SIZE * x ?\n答: 这是根据 k_cache 的 layout 得到的 stride. 同理 (3) * x 也是 stride.\n第 3 个循环结束时当前 warp 负责的每个 token 中需要的 K cache head 已经全被加载入 thread 本地的 k_vecs 中了.\n由于一个 thread group 的 k_vecs 才能真正组成一个 head, 在退回第二个循环进行 QK dot 的时候, 需要做个 reduction, 具体的范围就是 THREAD_GROUP_SIZE 个 thread:\n// Loop 1 for (int block_idx = start_block_idx + warp_idx; block_idx \u003c end_block_idx; block_idx += NUM_WARPS) { // Loop 2 for (int i = 0; i \u003c NUM_TOKENS_PER_THREAD_GROUP; i++) { K_vec k_vecs[NUM_VECS_PER_THREAD]; // Loop 3 for (int j = 0; j \u003c NUM_VECS_PER_THREAD; j++) { // ... } float qk = scale * Qk_dot\u003cscalar_t, THREAD_GROUP_SIZE\u003e::dot( q_vecs[thread_group_offset], k_vecs); } // ... } 计算完 qk 后, 由当前 thread group 中第一个 (offset 为 0) 的 thread 对自己刚才算出来的 qk 进行 mask, 顺便看看如果没有 mask 掉, 把 qk_max 赋值为 qk:\nif (thread_group_offset == 0) { // Store the partial reductions to shared memory. // NOTE(woosuk): It is required to zero out the masked logits. const bool mask = token_idx \u003e= seq_len; logits[token_idx - start_token_idx] = mask ? 0.f : qk; // Update the max value. qk_max = mask ? qk_max : fmaxf(qk_max, qk); } 🧐 为什么要做 mask?\n因为一个 seq 的最后一个 PA block 可能覆盖不满 BLOCK_SIZE 个 token. 这里的 mask 就是把那部分 qk 置零. 5.4. Softmax 我勒个 QK 啊, 总算算完了, 锐克 five 都要被抽清仓了. 页意丁真, 鉴定为开算 softmax.\n主要步骤就是广播然后算, 算 softmax 需要知道每个 head 对应的 qk 的最大值. 由于一个 cuda block 负责的就是一个 head, 对于这个 head 上面的计算步骤一共算了 cache_len个 token 的 qk, 因此需要做一个 cuda block 范围的规约, 找到其中最大的 qk 值.\n先在 warp 层面规约.\n__shared__ float red_smem[2 * NUM_WARPS]; // ... // Perform reduction across the threads in the same warp to get the // max qk value for each \"warp\" (not across the thread block yet). // The 0-th thread of each thread group already has its max qk value. #pragma unroll for (int mask = WARP_SIZE / 2; mask \u003e= THREAD_GROUP_SIZE; mask /= 2) { qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask)); } if (lane == 0) { red_smem[warp_idx] = qk_max; } __syncthreads(); red_smem 是之前申请的 shared memory. VLLM_SHFL_XOR_SYNC 是一个 warp 内的 shuffle 操作, 具体来说, 在每次循环时, 每个 thread 和自己相距 mask 位置的线程交换数据 (交换来的数据通过 fmaxf 比较), 并且 mask 会逐渐减半, 直到 THREAD_GROUP_SIZE 为止. lane 表示当前 warp 中的线程索引. 接着再对每个 warp 的最大值进行规约, 由于每个 warp 的最大值都被存入了 red_smem 中, 所以只需要再次进行 shuffle 操作即可.\n// TODO(woosuk): Refactor this part. // Get the max qk value for the sequence. qk_max = lane \u003c NUM_WARPS ? red_smem[lane] : -FLT_MAX; #pragma unroll for (int mask = NUM_WARPS / 2; mask \u003e= 1; mask /= 2) { qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask)); } 此时, 第 1 个线程的 qk_max 就是当前 cuda block 中所有 warp 中最大的 qk 值. 将其广播给所有线程:\n// Broadcast the max qk value to all threads. qk_max = VLLM_SHFL_SYNC(qk_max, 0); 在获得了 qk_max 后, 就可以计算 softmax 了:\n// Get the sum of the exp values. float exp_sum = 0.f; for (int i = thread_idx; i \u003c num_tokens; i += NUM_THREADS) { float val = __expf(logits[i] - qk_max); logits[i] = val; exp_sum += val; } exp_sum = block_sum\u003cNUM_WARPS\u003e(\u0026red_smem[NUM_WARPS], exp_sum); // Compute softmax. const float inv_sum = __fdividef(1.f, exp_sum + 1e-6f); for (int i = thread_idx; i \u003c num_tokens; i += NUM_THREADS) { logits[i] *= inv_sum; } __syncthreads(); 5.5. LV (Logits * Value) 上图展示了 LV 的计算过程, 主要区别是由于要计算 Logits 的 shape 可以表示为 (num_heads, num_seqs, cache_len), 而 V 的 shape 可以表示为 (num_heads, cache_len, head_size), 因此 LV 的矩阵乘法中, 每计算一个元素需要读取 logits 的一行和 V 的一列进行计算.\n此时, 一个 cuda block 的职责从 “自 Q 中读取一个 head” 转变为 “计算 output 中的一个 head”.\n🧐 为什么在计算 LV 时, 去掉了 thread group 的概念, 每个 thread 都被设定为每次读取 16B?\n因为现在每计算一个元素, 需要的访存量更大, 因此给每个 thread 分配了更多的数据读取量. 也就是说, V_VEC_SIZE 比 VEC_SIZE 更大. 由于 cuda 访存模式按行读取更快, 所以实际的计算结果在遍历 PA block 时线程内部利用 accs 进行累计 (以实现与 V 的一列进行计算的行为):\nconstexpr int NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE; constexpr int NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW; constexpr int NUM_ROWS_PER_THREAD = DIVIDE_ROUND_UP(HEAD_SIZE, NUM_ROWS_PER_ITER); // NOTE(woosuk): We use FP32 for the accumulator for better accuracy. float accs[NUM_ROWS_PER_THREAD]; for (int block_idx = start_block_idx + warp_idx; block_idx \u003c end_block_idx; block_idx += NUM_WARPS) { V_vec v_vec; // ... for (int i = 0; i \u003c NUM_ROWS_PER_THREAD; i++) { // ... for (int j = 0; j \u003c V_VEC_SIZE; j++) { // Load V to `v_vec` ... v_vec_ptr[j] = token_idx + j \u003c seq_len ? v_vec_ptr[j] : zero_value; } // Accumulate the dot product. accs[i] += dot(logits_vec, v_vec); } } 由于每个线程负责的累计部分不满一整行/列, 所以进行规约:\n// Perform reduction within each warp. #pragma unroll for (int i = 0; i \u003c NUM_ROWS_PER_THREAD; i++) { float acc = accs[i]; #pragma unroll for (int mask = NUM_V_VECS_PER_ROW / 2; mask \u003e= 1; mask /= 2) { acc += VLLM_SHFL_XOR_SYNC(acc, mask); } accs[i] = acc; } // NOTE(woosuk): A barrier is required because the shared memory space for // logits is reused for the output. __syncthreads(); // Perform reduction across warps. float* out_smem = reinterpret_cast\u003cfloat*\u003e(shared_mem); #pragma unroll for (int i = NUM_WARPS; i \u003e 1; i /= 2) { int mid = i / 2; // Upper warps write to shared memory. if (warp_idx \u003e= mid \u0026\u0026 warp_idx \u003c i) { float* dst = \u0026out_smem[(warp_idx - mid) * HEAD_SIZE]; #pragma unroll for (int i = 0; i \u003c NUM_ROWS_PER_THREAD; i++) { const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER; if (row_idx \u003c HEAD_SIZE \u0026\u0026 lane % NUM_V_VECS_PER_ROW == 0) { dst[row_idx] = accs[i]; } } } __syncthreads(); // Lower warps update the output. if (warp_idx \u003c mid) { const float* src = \u0026out_smem[warp_idx * HEAD_SIZE]; #pragma unroll for (int i = 0; i \u003c NUM_ROWS_PER_THREAD; i++) { const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER; if (row_idx \u003c HEAD_SIZE \u0026\u0026 lane % NUM_V_VECS_PER_ROW == 0) { accs[i] += src[row_idx]; } } } __syncthreads(); } 最后写入到输出中:\n// Write the final output. if (warp_idx == 0) { scalar_t* out_ptr = out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE + head_idx * max_num_partitions * HEAD_SIZE + partition_idx * HEAD_SIZE; #pragma unroll for (int i = 0; i \u003c NUM_ROWS_PER_THREAD; i++) { const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER; if (row_idx \u003c HEAD_SIZE \u0026\u0026 lane % NUM_V_VECS_PER_ROW == 0) { from_float(*(out_ptr + row_idx), accs[i]); } } } ","wordCount":"5628","inLanguage":"zh","datePublished":"2024-10-07T12:00:00+08:00","dateModified":"2025-09-12T22:39:56Z","author":[{"@type":"Person","name":"jamesnulliu"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://jamesnulliu.github.io/zh/blogs/dive-into-paged-attention/"},"publisher":{"@type":"Organization","name":"秋水·JamesNULLiu","logo":{"@type":"ImageObject","url":"https://jamesnulliu.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://jamesnulliu.github.io/zh/ accesskey=h title="秋水·JamesNULLiu (Alt + H)">秋水·JamesNULLiu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://jamesnulliu.github.io/ title=English aria-label=English>English</a></li></ul></div></div><ul id=menu><li><a href=https://jamesnulliu.github.io/zh/ title=Home><span>Home</span></a></li><li><a href=https://jamesnulliu.github.io/zh/about_me/ title="About Me"><span>About Me</span></a></li><li><a href=https://jamesnulliu.github.io/zh/blogs/ title=Blogs><span>Blogs</span></a></li><li><a href=https://jamesnulliu.github.io/zh/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://jamesnulliu.github.io/zh/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://jamesnulliu.github.io/zh/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://jamesnulliu.github.io/zh/search/ title=Search><span>Search</span></a></li><li><a href=https://jamesnulliu.github.io/zh/friends/ title=Friends><span>Friends</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://jamesnulliu.github.io/zh/>主页</a>&nbsp;»&nbsp;<a href=https://jamesnulliu.github.io/zh/blogs/>Blogs</a></div><h1 class="post-title entry-hint-parent">Dive into Paged Attention</h1><div class=post-description>Dive into the paged attention mechanism of vLLM.</div><div class=post-meta><span title='2024-10-07 12:00:00 +0800 +0800'>10月-07-2024</span>&nbsp;·&nbsp;12 分钟&nbsp;·&nbsp;5628 字&nbsp;·&nbsp;jamesnulliu&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://jamesnulliu.github.io/blogs/dive-into-paged-attention/>English</a></li></ul></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><ul><li><a href=#1-%e8%af%81%e6%98%8e-attention-%e7%9a%84--%e5%8f%aa%e4%b8%8e--%e6%9c%89%e5%85%b3 aria-label="1. 证明 Attention 的 $O_i$ 只与 $Q_i$ 有关">1. 证明 Attention 的 $O_i$ 只与 $Q_i$ 有关</a></li><li><a href=#2-kv-cache-%e7%9a%84%e5%a2%9e%e9%87%8f%e8%bf%87%e7%a8%8b aria-label="2. KV Cache 的增量过程">2. KV Cache 的增量过程</a><ul><li><a href=#21-%e5%88%9d%e5%a7%8b%e8%be%93%e5%85%a5%e5%ae%8c%e6%95%b4%e5%ba%8f%e5%88%97%e8%ae%a1%e7%ae%97 aria-label="2.1. 初始输入（完整序列）计算：">2.1. 初始输入（完整序列）计算：</a></li><li><a href=#22-%e9%a2%84%e6%b5%8b%e4%b8%8b%e4%b8%80%e4%b8%aa-token-%e6%97%b6%e7%9a%84%e5%a2%9e%e9%87%8f%e8%ae%a1%e7%ae%97 aria-label="2.2. 预测下一个 token 时的增量计算：">2.2. 预测下一个 token 时的增量计算：</a></li></ul></li><li><a href=#4-vllm-%e4%b8%ad%e7%9a%84-paged-attention aria-label="4. vllm 中的 Paged Attention">4. vllm 中的 Paged Attention</a><ul><li><a href=#41-%e5%8a%a8%e6%9c%ba-memory-wastes aria-label="4.1. 动机: Memory Wastes">4.1. 动机: Memory Wastes</a></li><li><a href=#42-%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88-%e7%94%a8-page-%e7%ae%a1%e7%90%86%e5%86%85%e5%ad%98 aria-label="4.2. 解决方案: 用 Page 管理内存">4.2. 解决方案: 用 Page 管理内存</a></li></ul></li><li><a href=#5-paged-attention-kernel-%e8%af%a6%e8%a7%a3 aria-label="5. Paged Attention Kernel 详解">5. Paged Attention Kernel 详解</a><ul><li><a href=#51-%e8%be%93%e5%85%a5%e8%be%93%e5%87%ba%e8%be%93%e5%87%ba%e5%88%86%e6%9e%90%e5%92%8c%e5%8f%82%e6%95%b0%e8%af%b4%e6%98%8e aria-label="5.1. 输入输出输出分析和参数说明">5.1. 输入输出输出分析和参数说明</a></li><li><a href=#52shared-memory-q_vecs-%e7%9a%84%e5%86%99%e5%85%a5 aria-label="5.2.Shared Memory: q_vecs 的写入">5.2.Shared Memory: <code>q_vecs</code> 的写入</a></li><li><a href=#53-%e8%af%bb%e5%8f%96-k-cache-%e5%b9%b6%e8%ae%a1%e7%ae%97-qk aria-label="5.3. 读取 K Cache 并计算 QK">5.3. 读取 K Cache 并计算 QK</a></li><li><a href=#54-softmax aria-label="5.4. Softmax">5.4. Softmax</a></li><li><a href=#55-lv-logits--value aria-label="5.5. LV (Logits * Value)">5.5. LV (Logits * Value)</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{let e=null;const t=window.innerHeight+window.pageYOffset>=document.body.offsetHeight-100;if(t)e=elements[elements.length-1];else{let t=null,n=1/0;elements.forEach(e=>{const s=getOffsetTop(e)-window.pageYOffset;if(s<=window.innerHeight*.3){const o=Math.abs(s);o<n&&(n=o,t=e)}}),e=t||elements[0]}if(e&&e!==activeElement){if(activeElement){const t=encodeURI(activeElement.getAttribute("id")).toLowerCase(),e=document.querySelector(`.inner ul li a[href="#${t}"]`);e&&e.classList.remove("active")}activeElement=e;const n=encodeURI(activeElement.getAttribute("id")).toLowerCase(),t=document.querySelector(`.inner ul li a[href="#${n}"]`);t&&(t.classList.add("active"),document.getElementById("toc-container").classList.contains("wide")&&scrollTocToActiveItem(t))}},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}function scrollTocToActiveItem(e){const t=document.querySelector(".toc .inner");if(!t||!e)return;const n=t.getBoundingClientRect(),s=e.getBoundingClientRect(),o=n.height/2,i=s.top-n.top+t.scrollTop,a=i-o;t.scrollTo({top:Math.max(0,a),behavior:"smooth"})}</script><div class=post-content><h2 id=1-证明-attention-的--只与--有关>1. 证明 Attention 的 $O_i$ 只与 $Q_i$ 有关<a hidden class=anchor aria-hidden=true href=#1-证明-attention-的--只与--有关>#</a></h2><p>Attention 的公式如下:</p>$$
O=Attention(Q,K,V)=softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$<p>假设 $Q=\begin{bmatrix}Q_0\\Q_1\end{bmatrix}$, $K=\begin{bmatrix}K_0\\K_1\end{bmatrix}$</p><p>那么:</p>$$
O=softmax(\frac{\begin{bmatrix}Q_0K_0^T&Q_0K_1^T\\Q_1K_0^T&Q_1K_1^T\end{bmatrix}}{\sqrt{d_k}})V
$$<p>令:</p>$$
A=\begin{bmatrix}A_0\\A_1\end{bmatrix}=\begin{bmatrix}Q_0K_0^T&Q_0K_1^T\\Q_1K_0^T&Q_1K_1^T\end{bmatrix},f(x)=\frac{softmax(x)}{\sqrt{d_k}}
$$<p>此时, $A_1$ 只和 $Q_1$ 有关, 和 $Q_0$ 无关, 那么:</p>$$
\begin{bmatrix}O_0\\O_1\end{bmatrix}=O=\begin{bmatrix}f(A_0)\\f(A_1)\end{bmatrix}V=\begin{bmatrix}f(A_0)V\\f(A_1)V\end{bmatrix}
$$<p>因此, $O_i$ 只和 $A_i$ 相关, 而根据 $A$ 的设定, $A_i$ 只和 $Q_i$ 相关, 即:</p><p>Attention 矩阵的第 $i$ 个输出只和第 $i$ 个 $Q$ 有关, 和之前的 $Q$ 无关.</p><p><strong>总结</strong>:</p><ul><li>在预测下一个 token 时，只需对新 token 计算对应的 <code>Q_new</code>，并与之前已经缓存的 <code>K_cache</code> 和 <code>V_cache</code> 进行注意力计算。</li><li>新的 <code>K_new</code> 和 <code>V_new</code> 会被加入到缓存中，继续为下一个 token 生成提供基础。</li><li>整个过程避免了对所有历史 token 的重复计算，大幅提高了效率。</li></ul><h2 id=2-kv-cache-的增量过程>2. KV Cache 的增量过程<a hidden class=anchor aria-hidden=true href=#2-kv-cache-的增量过程>#</a></h2><h3 id=21-初始输入完整序列计算>2.1. 初始输入（完整序列）计算：<a hidden class=anchor aria-hidden=true href=#21-初始输入完整序列计算>#</a></h3><ul><li>对于初始的输入序列 <code>(seq_len, embed_dim)</code>，我们通过线性变换得到 <code>Q</code>、<code>K</code> 和 <code>V</code>，它们的形状都是 <code>(seq_len, embed_dim)</code>。</li><li>使用 <code>Q</code> 和 <code>K</code> 进行点积计算注意力分数，然后结合 <code>V</code> 计算得到输出 <code>(seq_len, embed_dim)</code>，这是第一次对初始序列的完整计算。</li></ul><h3 id=22-预测下一个-token-时的增量计算>2.2. 预测下一个 token 时的增量计算：<a hidden class=anchor aria-hidden=true href=#22-预测下一个-token-时的增量计算>#</a></h3><p>在预测下一个 token 时，不需要对整个序列再进行完整的 <code>Q</code>、<code>K</code>、<code>V</code> 计算，而是只需对新生成的 token 进行一次增量计算。这时的操作流程如下：</p><ol><li><strong>输入新的 token</strong>：将已经生成的 token（其形状为 <code>(embed_dim,)</code>）作为输入，通过线性变换得到该 token 对应的 <code>Q_new</code>，形状为 <code>(embed_dim,)</code>。</li><li><strong>与之前缓存的 <code>K</code> 和 <code>V</code> 进行注意力计算</strong>：使用 <code>Q_new</code> 与之前已经计算并缓存的 <code>K_cache</code> 和 <code>V_cache</code> 进行注意力计算。这里的 <code>K_cache</code> 和 <code>V_cache</code> 分别是之前每次生成 token 时得到的 <code>K</code> 和 <code>V</code>，它们的形状是 <code>(seq_len, embed_dim)</code>，即缓存了从最初输入序列到当前已经生成的所有 token 的 <code>K</code> 和 <code>V</code>。<code>Q_new</code> 可以直接与 <code>K_cache</code> 进行点积，得到注意力分数，然后结合 <code>V_cache</code> 得到新的输出。</li><li><strong>更新 <code>KV Cache</code></strong>：新的 <code>K_new</code> 和 <code>V_new</code> 会通过线性变换得到（形状为 <code>(embed_dim,)</code>），并将它们添加到 <code>K_cache</code> 和 <code>V_cache</code> 的末尾，使得缓存的 <code>K_cache</code> 和 <code>V_cache</code> 不断增大，以备后续使用。</li><li><strong>输出</strong>：通过注意力计算后的输出形状为 <code>(embed_dim,)</code>，即新生成的 token。</li></ol><h2 id=4-vllm-中的-paged-attention>4. vllm 中的 Paged Attention<a hidden class=anchor aria-hidden=true href=#4-vllm-中的-paged-attention>#</a></h2><h3 id=41-动机-memory-wastes>4.1. 动机: Memory Wastes<a hidden class=anchor aria-hidden=true href=#41-动机-memory-wastes>#</a></h3><p><img alt=memory-wastes.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/memory-wastes.png></p><p>上图展示了可能的内存浪费情况, 主要时输入 sequence 不知道 eos 在哪里, 如果随机申请内存, 可能导致大量内存碎片, 因此吞吐量下降.</p><h3 id=42-解决方案-用-page-管理内存>4.2. 解决方案: 用 Page 管理内存<a hidden class=anchor aria-hidden=true href=#42-解决方案-用-page-管理内存>#</a></h3><p><img alt=paged-attention-animation.webp loading=lazy src=/imgs/blogs/dive-into-paged-attention/paged-attention-animation.webp></p><p>上图展示了 vLLM 用 Paged 管理内存具体怎么做的.</p><p>简单来说, vLLM 在开始推理前为每个 Decoder Layer 申请两个巨长的 Tensor (<code>k_cache</code> 和 <code>v_cache</code>), 把 Tensor 分割成连续等长的 PA blocks (图中的一行为一个 PA Block); 每个 PA Block 能够存放 <code>BLOCK_SIZE</code> 个 token 的 K 或 V cache (每个 cache 的形状可以理解为 <code>(num_heads, head_size)</code>).</p><p>因此, <code>k_cache</code> 和 <code>v_cache</code> 的形状可以理解为 <code>(num_blocks, num_heads, head_size)</code>.</p><p>对于一个连续的 sequnce, 在 prefill 阶段前就会分配好它的 PA blocks, 之后推理时:</p><ul><li>若是计算 prompt 的 Attention, 则先把传入的 K 和 V 按照 PA blocks 存入 <code>k_cache</code> 和 <code>v_cache</code> 中; 然后利用整段的 QKV 计算 attention.</li><li>若是计算新 token, 则利用 Q 和 block table 计算 decode 阶段的 attntion; 此时访存的就是 <code>k_cache</code> 和 <code>v_cache</code> 中的 PA blocks.</li></ul><h2 id=5-paged-attention-kernel-详解>5. Paged Attention Kernel 详解<a hidden class=anchor aria-hidden=true href=#5-paged-attention-kernel-详解>#</a></h2><blockquote><p>References:</p><ul><li><a href=https://docs.vllm.ai/en/latest/dev/kernel/paged_attention.html>vLLM Paged Attention</a></li><li><a href=https://zhuanlan.zhihu.com/p/673284781>vLLM皇冠上的明珠：深入浅出理解PagedAttention CUDA实现</a></li></ul></blockquote><p>先看下整体计算流程图 (这个图后面也会出现这里先看一眼):</p><p><img alt=pa-cal.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/pa-cal.png></p><h3 id=51-输入输出输出分析和参数说明>5.1. 输入输出输出分析和参数说明<a hidden class=anchor aria-hidden=true href=#51-输入输出输出分析和参数说明>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Grid: (num_heads, num_seqs, 1).
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>template</span><span class=o>&lt;</span>
</span></span><span class=line><span class=cl><span class=k>typename</span> <span class=n>scalar_t</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>HEAD_SIZE</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>BLOCK_SIZE</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>NUM_THREADS</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>PARTITION_SIZE</span> <span class=o>=</span> <span class=mi>0</span><span class=o>&gt;</span>
</span></span><span class=line><span class=cl><span class=n>__device__</span> <span class=kt>void</span> <span class=n>paged_attention_kernel</span><span class=p>(</span>
</span></span><span class=line><span class=cl><span class=p>...</span> <span class=c1>// Other side args.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=n>scalar_t</span><span class=o>*</span> <span class=n>__restrict__</span> <span class=n>out</span><span class=p>,</span>       <span class=c1>// [num_seqs, num_heads, max_num_partitions, head_size]
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=n>scalar_t</span><span class=o>*</span> <span class=n>__restrict__</span> <span class=n>q</span><span class=p>,</span>         <span class=c1>// [num_seqs, num_heads, head_size]
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=n>scalar_t</span><span class=o>*</span> <span class=n>__restrict__</span> <span class=n>k_cache</span><span class=p>,</span>   <span class=c1>// [num_blocks, num_kv_heads, head_size/x, block_size, x]
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=n>scalar_t</span><span class=o>*</span> <span class=n>__restrict__</span> <span class=n>v_cache</span><span class=p>,</span>   <span class=c1>// [num_blocks, num_kv_heads, head_size, block_size]
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>...</span> <span class=c1>// Other side args.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>)</span>
</span></span></code></pre></div><p>模板参数说明:</p><ul><li><code>scalar_t</code> 元素类型 (实际代码中还有 <code>cache_t</code> 表示 KV cache 的元素类型).</li><li><code>HEAD_SIZE</code> 每个 head 中元素数量.</li><li><code>BLOCK_SIZE</code> 每个 PA block 中的 token 数量.<blockquote><ol><li>KV cache 被存储在不同 PA blocks. 每个 PA block 存储一个 head 中 <code>BLOCK_SIZE</code> 个 token.<br>例如, 若 <code>BLOCK_SIZE=16</code>, <code>HEAD_SIZE=128</code>, 则一个 PA block 能存储一个 head 的 <code>16 * 128 = 2048</code> 个元素.</li><li>每个 PA block 可能只包含一部分的 context tokens.</li><li>从 page 角度看, KV cache 是若干个 page 的集合;</li></ol></blockquote></li><li><code>NUM_THREADS</code> 每个 CUDA thread block 中 thread 的数量.</li><li><code>PARTITION_SIZE</code> 参与 TP 的 GPU 数量, 默认 0 表示单卡. (以下都以单卡为例说明)</li></ul><p>额外的一些参数:</p><ul><li><code>num_seqs</code>: 本次推理请求 sequence 数目.<blockquote><p>由于这个 kernel 只处理 decode 阶段单 query attention, 所以实际上每个 sequence 只有一个 query token.</p></blockquote></li><li><code>num_heads</code>: Q 的 head 数目</li><li><code>num_kv_heads</code>: KV 的 head 数目, 对于 MHA 其值和 <code>num_heads</code> 相同; 如果是 GQA, MQA 则 <code>num_kv_heads</code> 小于 <code>num_head</code>.</li><li><code>head_size</code>: 即 <code>HEAD_SIZE</code></li><li><code>k_cache: (num_blocks, num_kv_heads, head_size/x, block_size, x)</code>, 其中 <code>x</code> 表示 <code>THREAD_GROUP_SIZE * VEC_SIZE</code> 的大小 (后面会细说).</li></ul><p>下面结合 GPU architecture 初步分析一下参数.</p><p><img alt=gpu-archi.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/gpu-archi.png></p><p>🧐 <strong>为什么要分 thread group?</strong></p><ul><li>因为当一个 cuda block 要取的数据比较少的时候 (计算 QK), 一个 thread group 分别一次取 Q 和 K 中 16B; 当一个 cuda block 要取的数据比较多的时候 (计算 LV), 一个 thread 取 16B.</li></ul><h3 id=52shared-memory-q_vecs-的写入>5.2.Shared Memory: <code>q_vecs</code> 的写入<a hidden class=anchor aria-hidden=true href=#52shared-memory-q_vecs-的写入>#</a></h3><p>从 kernel 中的第一个申请的 shared memory 开始说.</p><blockquote><p>关于 shared memeory:</p><ol><li>在 kernel 中申请的 shared memory 被当前 cuda block 中的所有 thread 共享.</li><li>shared memory 的作用是为了减少 global memory 的访问次数，提高访存效率.</li></ol></blockquote><p>以下代码申请了一块 shared memroy 被整个 CUDA Block 中所有 kernel 共享:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>__shared__</span> <span class=n>Q_vec</span> <span class=n>q_vecs</span><span class=p>[</span><span class=n>THREAD_GROUP_SIZE</span><span class=p>][</span><span class=n>NUM_VECS_PER_THREAD</span><span class=p>];</span>
</span></span></code></pre></div><p>首先, <code>q_vecs</code> 覆盖了 Q 中 <code>head_size</code> 个元素 - 这也是一个 cuda block 需要处理的数据量.</p><p>接着再说两个维度的参数的意思:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>constexpr</span> <span class=kt>int</span> <span class=n>NUM_ELEMS_PER_THREAD</span> <span class=o>=</span> <span class=n>HEAD_SIZE</span> <span class=o>/</span> <span class=n>THREAD_GROUP_SIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>constexpr</span> <span class=kt>int</span> <span class=n>NUM_VECS_PER_THREAD</span> <span class=o>=</span> <span class=n>NUM_ELEMS_PER_THREAD</span> <span class=o>/</span> <span class=n>VEC_SIZE</span><span class=p>;</span>
</span></span></code></pre></div><ul><li><code>THREAD_GROUP_SIZE</code>: 每个 thread group 中的 thread 数量. 注意, 一个 cuda block 中有 <code>NUM_THREADS</code> 个 thread, <code>NUM_THREAD_GROUPS</code> 个 thread group. <code>THREAD_GROUP_SIZE = MAX(WARP_SIZE/BLOCK_SIZE, 1)</code>.</li><li><code>NUM_VECS_PER_THREAD</code>: <code>HEAD_SIZE</code> 能被分成多少个 16B. (这个变量这么命名的理由是后面读取 K 的时候每个 thread 会往自己的寄存器内读 <code>NUM_VECS_PER_THREAD</code> 个 k_vec.)</li></ul><blockquote><p>证明: <code>q_vecs</code> 覆盖 Q 的一个 head, 并且 <code>NUM_VECS_PER_THREAD</code> 表示 Q 的一个 head 被分成多少个 16B.<br>=> <code>THREAD_GROUP_SIZE</code> * <code>VEC_SIZE</code> = 16B / <code>sizeof(scalar_t)</code>;<br>=> <code>NUM_VECS_PER_THREAD</code> * 16B / <code>sizeof(scalar_t)</code> = <code>HEAD_SIZE</code>;</p></blockquote><p>然后看 load Q 的代码, 建议结合下面的图一起看:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl>  <span class=c1>// Load Q to shmem
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=n>thread_group_idx</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_VECS_PER_THREAD</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>i</span> <span class=o>+=</span> <span class=n>NUM_THREAD_GROUPS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>int</span> <span class=n>vec_idx</span> <span class=o>=</span> <span class=n>thread_group_offset</span> <span class=o>+</span> <span class=n>i</span> <span class=o>*</span> <span class=n>THREAD_GROUP_SIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>q_vecs</span><span class=p>[</span><span class=n>thread_group_offset</span><span class=p>][</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>        <span class=o>*</span><span class=k>reinterpret_cast</span><span class=o>&lt;</span><span class=k>const</span> <span class=n>Q_vec</span><span class=o>*&gt;</span><span class=p>(</span><span class=n>q_ptr</span> <span class=o>+</span> <span class=n>vec_idx</span> <span class=o>*</span> <span class=n>VEC_SIZE</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><ul><li><code>thread_group_idx</code> 表示当前 thread 属于当前 cuda block 中第几个 thread group.</li><li><code>thread_group_offset</code> 表示当前 thread 在当前 thread group 中是第几个 thread.</li></ul><p><img alt=pa-load-q.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/pa-load-q.png></p><p>上图展示了循环具体是怎么跑的.</p><ul><li>一个紫色箭头表示一个 thread group.</li><li><code>NUM_VECS_PER_THREAD</code> 表示 <code>HEAD_SIZE</code> 能被分成多少个 16B.</li><li>实际读取 Q 的内存时, 所有 thread group 从 Q 的起始位置紧密排列, 根据图上看的话一共有 <code>NUM_THREAD_GROUPS</code> 个紫色箭头.</li><li>所有 thread group 读取一次 Q 并存入 <code>q_vecs</code> 对应循环中的一次迭代; 因此下次迭代 thread group 需要向后偏移 <code>NUM_THREAD_GROUPS</code> 个位置 (例如 <code>i</code> 从 1 变为 7).</li><li>此外, 读一次 16B 对应一个 thread 来说自然也是取一个 VEC.</li><li>对应到 kernel 编写, 还需要计算当前 thread 具体读取哪个 vec; 因此得到 <code>vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE</code>.</li></ul><blockquote><p>🤔 这里会不会有 bank conflict?</p></blockquote><p>总之现在我们把 <code>(1, head_size)</code> 大小的元素读到了 cuda block 共享的 shared memory <code>q_vecs</code> 中.</p><h3 id=53-读取-k-cache-并计算-qk>5.3. 读取 K Cache 并计算 QK<a hidden class=anchor aria-hidden=true href=#53-读取-k-cache-并计算-qk>#</a></h3><p>现在从 cuda block 的角度看, 当前 block 已经获得了自己要算的 Q 中的一个 head (形状为 <code>(1, head_size)</code>), 接下来就是计算 Q 和 K 的点积.</p><p>点积过程是把当前 block 拥有的 Q head 和整个 K Cache (迭代地) 进行点积运算. 参考下图:</p><p><img alt=pa-cal-kq-01.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/pa-cal-kq-01.png></p><p>QK 乘积实际上被暂存在 <code>logits</code> (也是一块 shared memory) 中, 之后会被用来计算 softmax.</p><p>😇 看下循环的具体代码吧:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Loop 1
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>block_idx</span> <span class=o>=</span> <span class=n>start_block_idx</span> <span class=o>+</span> <span class=n>warp_idx</span><span class=p>;</span> <span class=n>block_idx</span> <span class=o>&lt;</span> <span class=n>end_block_idx</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>block_idx</span> <span class=o>+=</span> <span class=n>NUM_WARPS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Physical block calculation ...
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// Loop 2
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_TOKENS_PER_THREAD_GROUP</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Offset calculation ...
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>K_vec</span> <span class=n>k_vecs</span><span class=p>[</span><span class=n>NUM_VECS_PER_THREAD</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Loop 3
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>NUM_VECS_PER_THREAD</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=c1>// Load K to `k_vecs` ...
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=kt>float</span> <span class=n>qk</span> <span class=o>=</span> <span class=n>scale</span> <span class=o>*</span> <span class=n>Qk_dot</span><span class=o>&lt;</span><span class=n>scalar_t</span><span class=p>,</span> <span class=n>THREAD_GROUP_SIZE</span><span class=o>&gt;::</span><span class=n>dot</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                        <span class=n>q_vecs</span><span class=p>[</span><span class=n>thread_group_offset</span><span class=p>],</span> <span class=n>k_vecs</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Add the ALiBi bias if slopes are given.
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>qk</span> <span class=o>+=</span> <span class=p>(</span><span class=n>alibi_slope</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>)</span> <span class=o>?</span> <span class=n>alibi_slope</span> <span class=o>*</span> <span class=p>(</span><span class=n>token_idx</span> <span class=o>-</span> <span class=n>seq_len</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>:</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=n>thread_group_offset</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=c1>// Store the partial reductions to shared memory.
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=c1>// Mask
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=c1>// Update the max value.
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>先说第一个循环, 其中比较重要的几个参数定义如下:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// [start_block_idx, end_block_idx) is the range of blocks to process.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=kt>int</span> <span class=n>start_block_idx</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>    <span class=n>USE_PARTITIONING</span> <span class=o>?</span> <span class=n>partition_idx</span> <span class=o>*</span> <span class=nl>num_blocks_per_partition</span> <span class=p>:</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=c1>// If not using partitioning, `end_block_idx` should be equal to `num_seq_blocks`.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=kt>int</span> <span class=n>end_block_idx</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>    <span class=n>MIN</span><span class=p>(</span><span class=n>start_block_idx</span> <span class=o>+</span> <span class=n>num_blocks_per_partition</span><span class=p>,</span> <span class=n>num_seq_blocks</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=c1>// Number of blocks to process.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=kt>int</span> <span class=n>num_blocks</span> <span class=o>=</span> <span class=n>end_block_idx</span> <span class=o>-</span> <span class=n>start_block_idx</span><span class=p>;</span>
</span></span></code></pre></div><p>用文字描述就是:</p><ul><li><code>blk_idx</code> 表示当前 thread 所在 warp 需要处理的 PA block 的在 <code>block_table</code> 中索引 (逻辑上的索引).</li><li><code>start_block_idx</code> 和 <code>end_block_idx</code> 表示当前 cuda block 需要处理的 block 范围.</li><li><code>num_blocks</code> 表示当前 cuda block 需要处理的 block 数量.</li><li><code>NUM_WARPS</code> 表示当前 cuda block 中 warp 的数量. 一个 warp 包含 32 个 thread.</li><li><code>warp_idx</code> 表示当前 warp 在当前 cuda block 中的索引.</li></ul><p>说人话就是每个 warp 处理一个 PA block, 一开始 cuda block 中的所有 warp 紧密地指向最前面的 <code>NUM_WARPS</code> 个 PA block, 每次循环所有 warp 向后偏移 <code>NUM_WARPS</code> 个 PA block 的长度. 参考下图:</p><p><img alt=pa-cal-kq-02.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/pa-cal-kq-02.png></p><blockquote><p>🔔 这里再回顾一下, 一个 PA block 里存放了 <code>BLOCK_SIZE</code> 个 token 的 K 或 V cache.</p></blockquote><p>所以说这个循环和上面读取 Q 的循环一个尿性🤮, 不过是以 warp 的粒度处理数据;</p><p>进入了第一个循环内部, 第一步当然是计算当前 thread 对应的 warp 应该计算哪个 PA block (物理上的索引), 因此得到了 <code>physical_block_number</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>const</span> <span class=kt>int64_t</span> <span class=n>physical_block_number</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>    <span class=k>static_cast</span><span class=o>&lt;</span><span class=kt>int64_t</span><span class=o>&gt;</span><span class=p>(</span><span class=n>block_table</span><span class=p>[</span><span class=n>block_idx</span><span class=p>]);</span>
</span></span></code></pre></div><hr><p>然后解释第二个循环, 第二个循环的整体目标就是让当前 warp 计算好自己负责的 PA block 中 <code>BLOCK_SIZE</code> 个 token 的 QK 乘积.</p><p>先看一下 <code>i</code> 的上界:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>constexpr</span> <span class=kt>int</span> <span class=n>NUM_TOKENS_PER_THREAD_GROUP</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>    <span class=n>DIVIDE_ROUND_UP</span><span class=p>(</span><span class=n>BLOCK_SIZE</span><span class=p>,</span> <span class=n>WARP_SIZE</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=c1>// Loop 1
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>block_idx</span> <span class=o>=</span> <span class=n>start_block_idx</span> <span class=o>+</span> <span class=n>warp_idx</span><span class=p>;</span> <span class=n>block_idx</span> <span class=o>&lt;</span> <span class=n>end_block_idx</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>block_idx</span> <span class=o>+=</span> <span class=n>NUM_WARPS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Loop 2
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_TOKENS_PER_THREAD_GROUP</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></div><p>从 kernel 角度看, 每个 thread 需要辅助当前 warp 计算自己负责的一整个 PA block (包含 <code>BLOCK_SIZE</code> 个 token), 而我们把这个过程拆分为 Loop 2 中的 <code>NUM_TOKEN_PER_THREAD_GROUP</code> (也就是 <code>ceil(BLOCK_SIZE / 32)</code>) 次循环;</p><p>说人话就是<strong>一个 thread group 对应一个 token 中的一个 head</strong>, 如果 BLOCK SIZE 太大了后面每个 thread 向后偏移 <code>i * WARP_SIZE</code> 个 token 继续狠狠算🤣.</p><p>也因此第二个循环内部一上来先计算了几个偏移量, 并且申请了 thread 内部私有的 <code>k_vecs</code> 数组:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>const</span> <span class=kt>int</span> <span class=n>physical_block_offset</span> <span class=o>=</span> <span class=p>(</span><span class=n>thread_group_idx</span> <span class=o>+</span> <span class=n>i</span> <span class=o>*</span> <span class=n>WARP_SIZE</span><span class=p>)</span> <span class=o>%</span> <span class=n>BLOCK_SIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>const</span> <span class=kt>int</span> <span class=n>token_idx</span> <span class=o>=</span> <span class=n>block_idx</span> <span class=o>*</span> <span class=n>BLOCK_SIZE</span> <span class=o>+</span> <span class=n>physical_block_offset</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>K_vec</span> <span class=n>k_vecs</span><span class=p>[</span><span class=n>NUM_VECS_PER_THREAD</span><span class=p>];</span>
</span></span></code></pre></div><ul><li><code>thread_group_idx</code> 表示当前 thread group 在整个 cuda block 中的索引.</li><li>☢️ 一个 thread group 在一次循环中负责 fetch 一个 PA block 中 K cache 的一个 token 中<strong>自己负责的 head</strong>.</li><li>☢️ 一个 thread group 负责计算一个 qk 值; 这个值显然是由一个 Q head 和一个 K head 点积得到的.</li><li><code>physical_block_offset</code> 表示当前要算的 token 在当前 PA block 中的偏移量 (注意和前面的 <code>physical_block_number</code> 区分).</li><li>加 <code>i * WARP_SIZE</code> 的原因是如果 <code>BLOCK_SIZE</code> 大于 32, 那么一个 warp 要多次循环才能处理完一个 PA block 中的所有 token, 对应 <code>thread_group_idx</code> 需要做偏移.</li><li><code>token_idx</code> 表示当前要算的 token 在整个 seq 的 KV cache 中的索引.</li><li><code>k_vecs</code> 中能存放 <code>NUM_VECS_PER_THREAD</code> 个 VEC, 而一整个 thread group 中所有的 thread 的 <code>k_vecs</code> 合起来才能组成一个 K 的 head (推导参考上面 Q 的 😇). 这就是为什么后面算 QK 的时候要 reduce.</li></ul><p>🤔 <strong>看到这里读者可能有一个问题: 一个 token 的 K cache 应该对应多个 head, 为什么上面说一个 thread group 只负责一个 head?</strong><br>答: 因为实际计算的时候, 一个 cuda block 只负责计算一个 head, 对应到 K Cache 乃至后面 V Cache 的位置也是一样的.</p><blockquote><p>这里额外说一下, 读 K 的 head 的一个目标应该是在尽量少的 register 中装下一个 head 的所有元素, 这样后续和 shared memory 中的 Q 做点乘并规约的速度更快. 假设一个 head 有 128 个 float16, 则占用 256B, 而 A100 中一个 thread 最多能有 255 个 32-bit register (也就是 1020B), 此时可以认为一个 thread 能装下一个 head 的所有元素.<br>但是由于目前 PA kernel 在 <code>BLOCK_SIZE</code> 为 16 的情况下 <code>THREAD_GROUP_SIZE</code> 等于 2, 因此一个 thread 只会装一个 head 的一半元素, 这样可能会导致 register 的使用率不高.</p></blockquote><hr><p>接着进入第三个循环, 目的是让 thread group 从 K cache 中读一个 head, 并存入 <code>k_vecs</code> 中:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// x == THREAD_GROUP_SIZE * VEC_SIZE
</span></span></span><span class=line><span class=cl><span class=c1>// Each thread group fetches x elements from the key at a time.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>constexpr</span> <span class=kt>int</span> <span class=n>x</span> <span class=o>=</span> <span class=mi>16</span> <span class=o>/</span> <span class=k>sizeof</span><span class=p>(</span><span class=n>cache_t</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=c1>//...
</span></span></span><span class=line><span class=cl><span class=c1>// Loop 1
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>block_idx</span> <span class=o>=</span> <span class=n>start_block_idx</span> <span class=o>+</span> <span class=n>warp_idx</span><span class=p>;</span> <span class=n>block_idx</span> <span class=o>&lt;</span> <span class=n>end_block_idx</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>block_idx</span> <span class=o>+=</span> <span class=n>NUM_WARPS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Loop 2
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_TOKENS_PER_THREAD_GROUP</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>K_vec</span> <span class=n>k_vecs</span><span class=p>[</span><span class=n>NUM_VECS_PER_THREAD</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Loop 3
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>NUM_VECS_PER_THREAD</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=k>const</span> <span class=n>cache_t</span><span class=o>*</span> <span class=n>k_ptr</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>                <span class=n>k_cache</span> <span class=o>+</span> <span class=n>physical_block_number</span> <span class=o>*</span> <span class=n>kv_block_stride</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>                <span class=n>kv_head_idx</span> <span class=o>*</span> <span class=n>kv_head_stride</span> <span class=o>+</span> <span class=n>physical_block_offset</span> <span class=o>*</span> <span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=k>const</span> <span class=kt>int</span> <span class=n>vec_idx</span> <span class=o>=</span> <span class=n>thread_group_offset</span> <span class=o>+</span> <span class=n>j</span> <span class=o>*</span> <span class=n>THREAD_GROUP_SIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=k>const</span> <span class=kt>int</span> <span class=n>offset1</span> <span class=o>=</span> <span class=p>(</span><span class=n>vec_idx</span> <span class=o>*</span> <span class=n>VEC_SIZE</span><span class=p>)</span> <span class=o>/</span> <span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=k>const</span> <span class=kt>int</span> <span class=n>offset2</span> <span class=o>=</span> <span class=p>(</span><span class=n>vec_idx</span> <span class=o>*</span> <span class=n>VEC_SIZE</span><span class=p>)</span> <span class=o>%</span> <span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=c1>// if Fp8KVCacheDataType::kAuto
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=n>k_vecs</span><span class=p>[</span><span class=n>j</span><span class=p>]</span> <span class=o>=</span> <span class=o>*</span><span class=k>reinterpret_cast</span><span class=o>&lt;</span><span class=k>const</span> <span class=n>K_vec</span><span class=o>*&gt;</span><span class=p>(</span>
</span></span><span class=line><span class=cl>              <span class=n>k_ptr</span> <span class=o>+</span> <span class=n>offset1</span> <span class=o>*</span> <span class=n>BLOCK_SIZE</span> <span class=o>*</span> <span class=n>x</span> <span class=o>+</span> <span class=n>offset2</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></div><p>老规矩, 先看 <code>j</code>, 本质就是从 0 迭代到 <code>NUM_VECS_PER_THREAD</code>, 每次迭代当前 thread 读取一个 VEC 存入 <code>k_vecs</code> 中.</p><blockquote><p>🔔 回顾:</p><ol><li><code>NUM_VECS_PER_THREAD</code> 表示一个 head 被分成多少个 16B.</li><li><code>k_cache</code> 的 shape 为 <code>(num_blocks, num_kv_heads, head_size/x, block_size, x)</code>.</li></ol></blockquote><p>其中的 <code>x</code> 表示一个 thread group 需要读取的元素数量 (<code>VEC_SIZE</code> * <code>THREAD_GROUP_SIZE</code>); 因此作者将 K Cache 的 layout 的最后一维设置为 <code>x</code> 其实也是方便后续 thread group 对 K cache 的读取.</p><p>下图具体展示了寻址的过程:</p><p><img alt=pa-cal-kq-03.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/pa-cal-kq-03.png></p><p>其中:</p><ul><li>在 MHSA 中, <code>num_kv_heads</code> 等于 <code>num_heads</code>; 而在 GQA, MQA 中, <code>num_kv_heads</code> 小于 <code>num_heads</code>.</li><li>(1) 负责找到当前 thread 属于的 warp 要处理哪个 PA block.</li><li>(2) 负责找到当前 thread 要计算的 head 在 K cache 中的位置. 这个 head 的索引和 Q 中 head 的索引在 MHSA 中相同.</li><li>(3) 负责找到当前 thread group 要计算的 token 在当前 PA block 中的位置.</li><li>(5) 负责找到当前 thread 在需要读取的 head (蓝色长方体) 中 x 的偏移, 通过 <code>j</code> 进行迭代读取. <strong>每次循环 thread group 中的所有 thread 取一个 x.</strong></li><li>(6) 负责找到当前 thread 在 thread gruop 中读取的 x 中 VEC 的偏移; thread 一次读取一个 VEC.</li></ul><p>🤔 <strong>为什么 (5) 在实际寻址时需要 <code>* BLOCK_SIZE * x</code> ?</strong><br>答: 这是根据 <code>k_cache</code> 的 layout 得到的 stride. 同理 (3) <code>* x</code> 也是 stride.</p><p>第 3 个循环结束时当前 warp 负责的每个 token 中需要的 K cache head 已经全被加载入 thread 本地的 <code>k_vecs</code> 中了.</p><p>由于一个 thread group 的 <code>k_vecs</code> 才能真正组成一个 head, 在退回第二个循环进行 QK dot 的时候, 需要做个 reduction, 具体的范围就是 <code>THREAD_GROUP_SIZE</code> 个 thread:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Loop 1
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>block_idx</span> <span class=o>=</span> <span class=n>start_block_idx</span> <span class=o>+</span> <span class=n>warp_idx</span><span class=p>;</span> <span class=n>block_idx</span> <span class=o>&lt;</span> <span class=n>end_block_idx</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>block_idx</span> <span class=o>+=</span> <span class=n>NUM_WARPS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Loop 2
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_TOKENS_PER_THREAD_GROUP</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>K_vec</span> <span class=n>k_vecs</span><span class=p>[</span><span class=n>NUM_VECS_PER_THREAD</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Loop 3
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>NUM_VECS_PER_THREAD</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=kt>float</span> <span class=n>qk</span> <span class=o>=</span> <span class=n>scale</span> <span class=o>*</span> <span class=n>Qk_dot</span><span class=o>&lt;</span><span class=n>scalar_t</span><span class=p>,</span> <span class=n>THREAD_GROUP_SIZE</span><span class=o>&gt;::</span><span class=n>dot</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                             <span class=n>q_vecs</span><span class=p>[</span><span class=n>thread_group_offset</span><span class=p>],</span> <span class=n>k_vecs</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></div><p>计算完 <code>qk</code> 后, 由当前 thread group 中第一个 (offset 为 0) 的 thread 对自己刚才算出来的 <code>qk</code> 进行 mask, 顺便看看如果没有 mask 掉, 把 <code>qk_max</code> 赋值为 <code>qk</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>thread_group_offset</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Store the partial reductions to shared memory.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// NOTE(woosuk): It is required to zero out the masked logits.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>const</span> <span class=kt>bool</span> <span class=n>mask</span> <span class=o>=</span> <span class=n>token_idx</span> <span class=o>&gt;=</span> <span class=n>seq_len</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span><span class=p>[</span><span class=n>token_idx</span> <span class=o>-</span> <span class=n>start_token_idx</span><span class=p>]</span> <span class=o>=</span> <span class=n>mask</span> <span class=o>?</span> <span class=mf>0.f</span> <span class=o>:</span> <span class=n>qk</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Update the max value.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>qk_max</span> <span class=o>=</span> <span class=n>mask</span> <span class=o>?</span> <span class=nl>qk_max</span> <span class=p>:</span> <span class=n>fmaxf</span><span class=p>(</span><span class=n>qk_max</span><span class=p>,</span> <span class=n>qk</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>🧐 <strong>为什么要做 mask?</strong></p><ul><li>因为一个 seq 的最后一个 PA block 可能覆盖不满 <code>BLOCK_SIZE</code> 个 token. 这里的 mask 就是把那部分 qk 置零.</li></ul><h3 id=54-softmax>5.4. Softmax<a hidden class=anchor aria-hidden=true href=#54-softmax>#</a></h3><p>我勒个 QK 啊, 总算算完了, 锐克 five 都要被抽清仓了. 页意丁真, 鉴定为开算 softmax.</p><p>主要步骤就是广播然后算, 算 softmax 需要知道每个 head 对应的 qk 的最大值. 由于一个 cuda block 负责的就是一个 head, 对于这个 head 上面的计算步骤一共算了 <code>cache_len</code>个 token 的 qk, 因此需要做一个 cuda block 范围的规约, 找到其中最大的 qk 值.</p><p>先在 warp 层面规约.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>__shared__</span> <span class=kt>float</span> <span class=n>red_smem</span><span class=p>[</span><span class=mi>2</span> <span class=o>*</span> <span class=n>NUM_WARPS</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=c1>// Perform reduction across the threads in the same warp to get the
</span></span></span><span class=line><span class=cl><span class=c1>// max qk value for each &#34;warp&#34; (not across the thread block yet).
</span></span></span><span class=line><span class=cl><span class=c1>// The 0-th thread of each thread group already has its max qk value.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>mask</span> <span class=o>=</span> <span class=n>WARP_SIZE</span> <span class=o>/</span> <span class=mi>2</span><span class=p>;</span> <span class=n>mask</span> <span class=o>&gt;=</span> <span class=n>THREAD_GROUP_SIZE</span><span class=p>;</span> <span class=n>mask</span> <span class=o>/=</span> <span class=mi>2</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>qk_max</span> <span class=o>=</span> <span class=n>fmaxf</span><span class=p>(</span><span class=n>qk_max</span><span class=p>,</span> <span class=n>VLLM_SHFL_XOR_SYNC</span><span class=p>(</span><span class=n>qk_max</span><span class=p>,</span> <span class=n>mask</span><span class=p>));</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>lane</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>red_smem</span><span class=p>[</span><span class=n>warp_idx</span><span class=p>]</span> <span class=o>=</span> <span class=n>qk_max</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>__syncthreads</span><span class=p>();</span>
</span></span></code></pre></div><ul><li><code>red_smem</code> 是之前申请的 shared memory.</li><li><code>VLLM_SHFL_XOR_SYNC</code> 是一个 warp 内的 shuffle 操作, 具体来说, 在每次循环时, 每个 thread 和自己相距 <code>mask</code> 位置的线程交换数据 (交换来的数据通过 <code>fmaxf</code> 比较), 并且 <code>mask</code> 会逐渐减半, 直到 <code>THREAD_GROUP_SIZE</code> 为止.</li><li><code>lane</code> 表示当前 warp 中的线程索引.</li></ul><p>接着再对每个 warp 的最大值进行规约, 由于每个 warp 的最大值都被存入了 <code>red_smem</code> 中, 所以只需要再次进行 shuffle 操作即可.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// TODO(woosuk): Refactor this part.
</span></span></span><span class=line><span class=cl><span class=c1>// Get the max qk value for the sequence.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>qk_max</span> <span class=o>=</span> <span class=n>lane</span> <span class=o>&lt;</span> <span class=n>NUM_WARPS</span> <span class=o>?</span> <span class=n>red_smem</span><span class=p>[</span><span class=n>lane</span><span class=p>]</span> <span class=o>:</span> <span class=o>-</span><span class=n>FLT_MAX</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>mask</span> <span class=o>=</span> <span class=n>NUM_WARPS</span> <span class=o>/</span> <span class=mi>2</span><span class=p>;</span> <span class=n>mask</span> <span class=o>&gt;=</span> <span class=mi>1</span><span class=p>;</span> <span class=n>mask</span> <span class=o>/=</span> <span class=mi>2</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>qk_max</span> <span class=o>=</span> <span class=n>fmaxf</span><span class=p>(</span><span class=n>qk_max</span><span class=p>,</span> <span class=n>VLLM_SHFL_XOR_SYNC</span><span class=p>(</span><span class=n>qk_max</span><span class=p>,</span> <span class=n>mask</span><span class=p>));</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>此时, 第 1 个线程的 <code>qk_max</code> 就是当前 cuda block 中所有 warp 中最大的 qk 值. 将其广播给所有线程:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Broadcast the max qk value to all threads.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>qk_max</span> <span class=o>=</span> <span class=n>VLLM_SHFL_SYNC</span><span class=p>(</span><span class=n>qk_max</span><span class=p>,</span> <span class=mi>0</span><span class=p>);</span>
</span></span></code></pre></div><p>在获得了 <code>qk_max</code> 后, 就可以计算 softmax 了:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Get the sum of the exp values.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>float</span> <span class=n>exp_sum</span> <span class=o>=</span> <span class=mf>0.f</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=n>thread_idx</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>num_tokens</span><span class=p>;</span> <span class=n>i</span> <span class=o>+=</span> <span class=n>NUM_THREADS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>float</span> <span class=n>val</span> <span class=o>=</span> <span class=n>__expf</span><span class=p>(</span><span class=n>logits</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>-</span> <span class=n>qk_max</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>val</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>exp_sum</span> <span class=o>+=</span> <span class=n>val</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>exp_sum</span> <span class=o>=</span> <span class=n>block_sum</span><span class=o>&lt;</span><span class=n>NUM_WARPS</span><span class=o>&gt;</span><span class=p>(</span><span class=o>&amp;</span><span class=n>red_smem</span><span class=p>[</span><span class=n>NUM_WARPS</span><span class=p>],</span> <span class=n>exp_sum</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// Compute softmax.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=kt>float</span> <span class=n>inv_sum</span> <span class=o>=</span> <span class=n>__fdividef</span><span class=p>(</span><span class=mf>1.f</span><span class=p>,</span> <span class=n>exp_sum</span> <span class=o>+</span> <span class=mf>1e-6</span><span class=n>f</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=n>thread_idx</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>num_tokens</span><span class=p>;</span> <span class=n>i</span> <span class=o>+=</span> <span class=n>NUM_THREADS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>*=</span> <span class=n>inv_sum</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>__syncthreads</span><span class=p>();</span>
</span></span></code></pre></div><h3 id=55-lv-logits--value>5.5. LV (Logits * Value)<a hidden class=anchor aria-hidden=true href=#55-lv-logits--value>#</a></h3><p><img alt=pa-cal.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/pa-cal.png></p><p>上图展示了 LV 的计算过程, 主要区别是由于要计算 Logits 的 shape 可以表示为 <code>(num_heads, num_seqs, cache_len)</code>, 而 V 的 shape 可以表示为 <code>(num_heads, cache_len, head_size)</code>, 因此 LV 的矩阵乘法中, 每计算一个元素需要读取 logits 的一行和 V 的一列进行计算.</p><p>此时, 一个 cuda block 的职责从 &ldquo;自 Q 中读取一个 head&rdquo; 转变为 &ldquo;计算 output 中的一个 head&rdquo;.</p><p>🧐 <strong>为什么在计算 LV 时, 去掉了 thread group 的概念, 每个 thread 都被设定为每次读取 16B?</strong></p><ul><li>因为现在每计算一个元素, 需要的访存量更大, 因此给每个 thread 分配了更多的数据读取量. 也就是说, <code>V_VEC_SIZE</code> 比 <code>VEC_SIZE</code> 更大.</li></ul><p>由于 cuda 访存模式按行读取更快, 所以实际的计算结果在遍历 PA block 时线程内部利用 <code>accs</code> 进行累计 (以实现与 V 的一列进行计算的行为):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>constexpr</span> <span class=kt>int</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>=</span> <span class=n>BLOCK_SIZE</span> <span class=o>/</span> <span class=n>V_VEC_SIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>constexpr</span> <span class=kt>int</span> <span class=n>NUM_ROWS_PER_ITER</span> <span class=o>=</span> <span class=n>WARP_SIZE</span> <span class=o>/</span> <span class=n>NUM_V_VECS_PER_ROW</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>constexpr</span> <span class=kt>int</span> <span class=n>NUM_ROWS_PER_THREAD</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>    <span class=n>DIVIDE_ROUND_UP</span><span class=p>(</span><span class=n>HEAD_SIZE</span><span class=p>,</span> <span class=n>NUM_ROWS_PER_ITER</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// NOTE(woosuk): We use FP32 for the accumulator for better accuracy.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>float</span> <span class=n>accs</span><span class=p>[</span><span class=n>NUM_ROWS_PER_THREAD</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>block_idx</span> <span class=o>=</span> <span class=n>start_block_idx</span> <span class=o>+</span> <span class=n>warp_idx</span><span class=p>;</span> <span class=n>block_idx</span> <span class=o>&lt;</span> <span class=n>end_block_idx</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>block_idx</span> <span class=o>+=</span> <span class=n>NUM_WARPS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>V_vec</span> <span class=n>v_vec</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_ROWS_PER_THREAD</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>V_VEC_SIZE</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=c1>// Load V to `v_vec` ...
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=n>v_vec_ptr</span><span class=p>[</span><span class=n>j</span><span class=p>]</span> <span class=o>=</span> <span class=n>token_idx</span> <span class=o>+</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>seq_len</span> <span class=o>?</span> <span class=n>v_vec_ptr</span><span class=p>[</span><span class=n>j</span><span class=p>]</span> <span class=o>:</span> <span class=n>zero_value</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Accumulate the dot product.
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>accs</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+=</span> <span class=n>dot</span><span class=p>(</span><span class=n>logits_vec</span><span class=p>,</span> <span class=n>v_vec</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>由于每个线程负责的累计部分不满一整行/列, 所以进行规约:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Perform reduction within each warp.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span>  <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_ROWS_PER_THREAD</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>float</span> <span class=n>acc</span> <span class=o>=</span> <span class=n>accs</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>mask</span> <span class=o>=</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>/</span> <span class=mi>2</span><span class=p>;</span> <span class=n>mask</span> <span class=o>&gt;=</span> <span class=mi>1</span><span class=p>;</span> <span class=n>mask</span> <span class=o>/=</span> <span class=mi>2</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=n>acc</span> <span class=o>+=</span> <span class=n>VLLM_SHFL_XOR_SYNC</span><span class=p>(</span><span class=n>acc</span><span class=p>,</span> <span class=n>mask</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>accs</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>acc</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>// NOTE(woosuk): A barrier is required because the shared memory space for
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>// logits is reused for the output.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>// Perform reduction across warps.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=kt>float</span><span class=o>*</span> <span class=n>out_smem</span> <span class=o>=</span> <span class=k>reinterpret_cast</span><span class=o>&lt;</span><span class=kt>float</span><span class=o>*&gt;</span><span class=p>(</span><span class=n>shared_mem</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span>  <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=n>NUM_WARPS</span><span class=p>;</span> <span class=n>i</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>;</span> <span class=n>i</span> <span class=o>/=</span> <span class=mi>2</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>mid</span> <span class=o>=</span> <span class=n>i</span> <span class=o>/</span> <span class=mi>2</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Upper warps write to shared memory.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>if</span> <span class=p>(</span><span class=n>warp_idx</span> <span class=o>&gt;=</span> <span class=n>mid</span> <span class=o>&amp;&amp;</span> <span class=n>warp_idx</span> <span class=o>&lt;</span> <span class=n>i</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=kt>float</span><span class=o>*</span> <span class=n>dst</span> <span class=o>=</span> <span class=o>&amp;</span><span class=n>out_smem</span><span class=p>[(</span><span class=n>warp_idx</span> <span class=o>-</span> <span class=n>mid</span><span class=p>)</span> <span class=o>*</span> <span class=n>HEAD_SIZE</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span>      <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_ROWS_PER_THREAD</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>const</span> <span class=kt>int</span> <span class=n>row_idx</span> <span class=o>=</span> <span class=n>lane</span> <span class=o>/</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>+</span> <span class=n>i</span> <span class=o>*</span> <span class=n>NUM_ROWS_PER_ITER</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=n>row_idx</span> <span class=o>&lt;</span> <span class=n>HEAD_SIZE</span> <span class=o>&amp;&amp;</span> <span class=n>lane</span> <span class=o>%</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>          <span class=n>dst</span><span class=p>[</span><span class=n>row_idx</span><span class=p>]</span> <span class=o>=</span> <span class=n>accs</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// Lower warps update the output.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>if</span> <span class=p>(</span><span class=n>warp_idx</span> <span class=o>&lt;</span> <span class=n>mid</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=k>const</span> <span class=kt>float</span><span class=o>*</span> <span class=n>src</span> <span class=o>=</span> <span class=o>&amp;</span><span class=n>out_smem</span><span class=p>[</span><span class=n>warp_idx</span> <span class=o>*</span> <span class=n>HEAD_SIZE</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span>      <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_ROWS_PER_THREAD</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>const</span> <span class=kt>int</span> <span class=n>row_idx</span> <span class=o>=</span> <span class=n>lane</span> <span class=o>/</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>+</span> <span class=n>i</span> <span class=o>*</span> <span class=n>NUM_ROWS_PER_ITER</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=n>row_idx</span> <span class=o>&lt;</span> <span class=n>HEAD_SIZE</span> <span class=o>&amp;&amp;</span> <span class=n>lane</span> <span class=o>%</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>          <span class=n>accs</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+=</span> <span class=n>src</span><span class=p>[</span><span class=n>row_idx</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span></code></pre></div><p>最后写入到输出中:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl>  <span class=c1>// Write the final output.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>if</span> <span class=p>(</span><span class=n>warp_idx</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>scalar_t</span><span class=o>*</span> <span class=n>out_ptr</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>+</span> <span class=n>seq_idx</span> <span class=o>*</span> <span class=n>num_heads</span> <span class=o>*</span> <span class=n>max_num_partitions</span> <span class=o>*</span> <span class=n>HEAD_SIZE</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>        <span class=n>head_idx</span> <span class=o>*</span> <span class=n>max_num_partitions</span> <span class=o>*</span> <span class=n>HEAD_SIZE</span> <span class=o>+</span> <span class=n>partition_idx</span> <span class=o>*</span> <span class=n>HEAD_SIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_ROWS_PER_THREAD</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=k>const</span> <span class=kt>int</span> <span class=n>row_idx</span> <span class=o>=</span> <span class=n>lane</span> <span class=o>/</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>+</span> <span class=n>i</span> <span class=o>*</span> <span class=n>NUM_ROWS_PER_ITER</span><span class=p>;</span>
</span></span><span class=line><span class=cl>      <span class=k>if</span> <span class=p>(</span><span class=n>row_idx</span> <span class=o>&lt;</span> <span class=n>HEAD_SIZE</span> <span class=o>&amp;&amp;</span> <span class=n>lane</span> <span class=o>%</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>from_float</span><span class=p>(</span><span class=o>*</span><span class=p>(</span><span class=n>out_ptr</span> <span class=o>+</span> <span class=n>row_idx</span><span class=p>),</span> <span class=n>accs</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://jamesnulliu.github.io/zh/tags/python/>Python</a></li><li><a href=https://jamesnulliu.github.io/zh/tags/vllm/>Vllm</a></li><li><a href=https://jamesnulliu.github.io/zh/tags/attention/>Attention</a></li></ul><nav class=paginav><a class=prev href=https://jamesnulliu.github.io/zh/blogs/a-brief-talk-on-speculative-decoding/><span class=title>« 上一页</span><br><span>浅谈投机推理</span></a></nav></footer><script src=https://giscus.app/client.js data-repo=jamesnulliu/jamesnulliu.github.io data-repo-id=R_kgDOMPCQIw data-category=Announcements data-category-id=DIC_kwDOMPCQI84Cgb2t data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async></script></article></main><footer class=footer><span>© 2024-2025 JamesNULLiu</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>