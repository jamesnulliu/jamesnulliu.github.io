<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Python on ÁßãÊ∞¥¬∑JamesNULLiu</title><link>https://jamesnulliu.github.io/tags/python/</link><description>Recent content in Python on ÁßãÊ∞¥¬∑JamesNULLiu</description><generator>Hugo -- 0.148.2</generator><language>en</language><copyright>2024-2025 JamesNULLiu</copyright><lastBuildDate>Fri, 12 Sep 2025 22:46:13 +0000</lastBuildDate><atom:link href="https://jamesnulliu.github.io/tags/python/index.xml" rel="self" type="application/rss+xml"/><item><title>Create A LibTorch Project</title><link>https://jamesnulliu.github.io/blogs/create-a-libtorch-project/</link><pubDate>Mon, 23 Dec 2024 01:00:00 +0800</pubDate><guid>https://jamesnulliu.github.io/blogs/create-a-libtorch-project/</guid><description>How to create a LibTorch project.</description><content:encoded><![CDATA[<h2 id="0-introduction">0. Introduction</h2>
<p>These days I am reading <a href="https://www.elsevier.com/books/programming-massively-parallel-processors/kirk/978-0-12-811986-0" target="_blank" rel="noopener noreferrer">
    Programming Massively Parallel Processors: A Hands-on Approach, 4th Edition
</a>
, and created a <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors" target="_blank" rel="noopener noreferrer">
    project
</a>
 to store my notes as I learn.</p>
<p>One of the most important parts in the book is writing <strong>cuda kernels</strong>, so I decided to build all kernels into shared libraries and test those implementations both in C++ and Python.</p>
<p>I generated my project using <a href="https://github.com/jamesnulliu/VSC-Python-Project-Template" target="_blank" rel="noopener noreferrer">
    this template
</a>
 specifically tailored for the similar scenario, but still met some problems such as conflicts when linking libtorch and gtest ü§Ø.</p>
<p><strong>So the purpose of this blog is to provide a concise guide to:</strong></p>
<ol>
<li>Build a C++, CUDA and LibTorch library, test it with gtest.</li>
<li>Load the library into torch, call the operaters in Python.</li>
<li>Resolve problems when linking all the libraries.</li>
</ol>
<blockquote>
<p>‚ö†Ô∏è<strong>WARNING</strong><br>
Find some tutorials on how to use cmake and vcpkg before reading this blog.</p></blockquote>
<h2 id="1-environment-and-quick-start">1. Environment and Quick Start</h2>
<p>Check <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/tree/c648649/README.md" target="_blank" rel="noopener noreferrer">
    README.md
</a>
 of the project repository.</p>
<h2 id="2-create-a-c-cuda-and-libtorch-project">2. Create a C++, CUDA and LibTorch Project</h2>
<p>I put all C++ codes in &ldquo;<a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/tree/96685ab/csrc" target="_blank" rel="noopener noreferrer">
    ./csrc/
</a>
&rdquo; and build them with cmake. The intermediate files should be generated in &ldquo;./build/&rdquo; and that is just about using some command-line arguments, see <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/scripts/build.sh#L42" target="_blank" rel="noopener noreferrer">
    this line
</a>
.</p>
<p>Vcpkg is used to manage the dependencies of the project. I am not going to teach you how to use vcpkg in this blog, but I will mention some pitfalls I met when using it.</p>
<blockquote>
<p>üòçÔ∏è I really enjoy building C++ projects with cmake and vcpkg. Have a try if you haven&rsquo;t used them before.</p></blockquote>
<h3 id="21-how-to-link-against-libtorch">2.1. How to Link against LibTorch</h3>
<p>Since you have installed pytorch in <a href="#1-environment">
    1. Environment
</a>
, now you already have libtorch installed in your conda environment. Run this command, and you will get the cmake prefix path of libtorch:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">python -c <span class="s2">&#34;import torch;print(torch.utils.cmake_prefix_path)&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>To integrate libtorch into cmake, I create <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/cmake/utils/run-python.cmake" target="_blank" rel="noopener noreferrer">
    this file
</a>
 and <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/cmake/libraries/libtorch.cmake" target="_blank" rel="noopener noreferrer">
    this file
</a>
 to find libtorch in the current project and use them <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/CMakeLists.txt#L27" target="_blank" rel="noopener noreferrer">
    here
</a>
.</p>
<p>Now you can link your targets against libtorch simply like what I do <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/CMakeLists.txt#L19" target="_blank" rel="noopener noreferrer">
    here
</a>
.</p>
<blockquote>
<p>üìù<strong>NOTE</strong><br>
When you link your target against <code>${TORCH_LIBRARIES}</code>, cuda libraries are being linked automatically, which means you don&rsquo;t have to find and link cuda using something like I write <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab6/csrc/cmake/libraries/libcuda.cmake" target="_blank" rel="noopener noreferrer">
    here
</a>
</p></blockquote>
<h3 id="22-cmake-and-vcpkg-configuration">2.2. CMake and VCPKG Configuration</h3>
<p>Currently, I am planning to use the C/C++ packages listed in <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/vcpkg.json" target="_blank" rel="noopener noreferrer">
    this file
</a>
. I load the packages with <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/CMakeLists.txt#L30-L36" target="_blank" rel="noopener noreferrer">
    these lines in &#34;./csrc/CMakeLists.txt&#34;
</a>
 . Then I link those packages to my targets <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/CMakeLists.txt#L20-L21" target="_blank" rel="noopener noreferrer">
    here
</a>
 and <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/test/CMakeLists.txt#L11-L13" target="_blank" rel="noopener noreferrer">
    here
</a>
.</p>
<blockquote>
<p>üìù<strong>NOTE</strong><br>
<code>libtorch &lt; 2.6</code> is compiled with <code>_GLIBCXX_USE_CXX11_ABI=0</code> to use legacy ABI before C++11, which conflicts with the packages managed by vcpkg in default. Consequentially, you have to create a custom vcpkg triplet to control the behaviors when vcpkg actually build the packages. The triplet file is <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/cmake/vcpkg-triplets/x64-linux.cmake" target="_blank" rel="noopener noreferrer">
    here
</a>
 and is enabled by <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/scripts/build.sh#L47-L48" target="_blank" rel="noopener noreferrer">
    these lines
</a>
 when building the C++ part.</p></blockquote>
<p>I also set <code>CMAKE_CXX_SCAN_FOR_MODULES</code> to <code>OFF</code> on <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/cmake/compilers/cxx-compiler-configs.cmake#L15" target="_blank" rel="noopener noreferrer">
    this line
</a>
 because some compile errors occurs. This is a temporary solution but I am not planning to use modules from C++20 in this project, so just ignoring it.</p>
<h3 id="23-write-and-register-custom-torch-operators">2.3. Write and Register Custom Torch Operators</h3>
<p>In order to register a custom torch <strong>operator</strong>, basically what you need to do next is to write a <strong>function</strong> that usually takes several <code>torch::Tensor</code> as input and returns a <code>torch::Tensor</code> as output, and then register this function to torch.</p>
<p>For example, I implement <code>pmpp::ops::cpu::launchVecAdd</code> in <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/ops/vecAdd/op.cpp" target="_blank" rel="noopener noreferrer">
    this cpp file
</a>
 and <code>pmpp::ops::cuda::launchVecAdd</code> in <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/ops/vecAdd/op.cu" target="_blank" rel="noopener noreferrer">
    this cu file
</a>
 and provide the corresponding torch implentations <code>pmpp::ops::cpu::vectorAddImpl</code> and <code>pmpp::ops::cuda::vectorAddImpl</code> in <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/ops/vecAdd/torch_impl.cpp" target="_blank" rel="noopener noreferrer">
    this file
</a>
.</p>
<blockquote>
<p>ü§î I didn&rsquo;t add any of those function declarations in hpp files under &ldquo;./include&rdquo; because I don&rsquo;t think they should be exposed to the users of the library. For the testing part, I will get and test the functions using <code>torch::Dispatcher</code> which aligns with the operaters invoked in python.</p></blockquote>
<p>To register these implementations as an operater into pytorch, see <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/ops/torch_bind.cpp#L10" target="_blank" rel="noopener noreferrer">
    this line
</a>
, <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/ops/torch_bind.cpp#L22" target="_blank" rel="noopener noreferrer">
    this line
</a>
, and <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/ops/torch_bind.cpp#L32" target="_blank" rel="noopener noreferrer">
    this line
</a>
, where I:</p>
<ol>
<li>Define a python function <code>vector_add</code> with signature: <code>vector_add(Tensor a, Tensor b) -&gt; Tensor</code>.</li>
<li>Register the CPU implementation of the function.</li>
<li>Register the CUDA implementation of the function.</li>
</ol>
<p>Now <code>vector_add</code> is a custom torch operator which can be called in both C++ and Python. All you need to do is to build these codes into a shared library like what I did <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/CMakeLists.txt#L7" target="_blank" rel="noopener noreferrer">
    here in cmake
</a>
.</p>
<h3 id="24-test-the-custom-torch-operators-in-c">2.4. Test the Custom Torch Operators in C++</h3>
<p>As long as a custom torch operator is registered, normally one or multiple shared libraries will be generated. For C++ users, you should link your executable target against libtorch and the generated shared libraries so that those registered operators can be called.</p>
<p>Since I have linked <code>libPmppTorchOps</code> against libtorch as <code>PUBLIC</code> in <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/CMakeLists.txt#L18" target="_blank" rel="noopener noreferrer">
    this line
</a>
, the test target will link against libtorch automatically as long as it links against <code>libPmppTorchOps</code>, see <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/test/CMakeLists.txt#L10" target="_blank" rel="noopener noreferrer">
    this line
</a>
.</p>
<blockquote>
<p>üìù<strong>NOTE</strong><br>
You may be confused about why <code>-Wl,--no-as-needed</code> is added before <code>${PROJECT_NAMESPACE}pmpp-torch-ops</code>. This is because the shared libraries are not directly used in the test target (an operator is register in the library but not called directly in the executable), and the linker will not link against them by default. This flag will force the linker to link against the shared libraries even if they are not directly used.</p></blockquote>
<p>The registered operators can be dispatched in a not-so-intuitional way ü§£ based on the official documentation, see <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/test/OpTest/vecAdd.cpp#L14-L17" target="_blank" rel="noopener noreferrer">
    here
</a>
.</p>
<p>Now the only thing is to test the operators in C++ using gtest, but this is not the focus of this blog. So let&rsquo;s move on to the next part.</p>
<h2 id="3-create-and-package-a-python-project">3. Create and Package a Python Project</h2>
<h3 id="31-pyprojecttoml-and-setuppy">3.1. <code>pyproject.toml</code> and <code>setup.py</code></h3>
<p>In modern python, pyproject.toml is a de-facto standard configuration file for packaging, and in this project, setuptools is used as the build backend because I believe it is the most popular one and is easy to cooperate with cmake.</p>
<p>Particularly, &ldquo;<a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/pyproject.toml" target="_blank" rel="noopener noreferrer">
    ./pyproject.toml
</a>
&rdquo; and &ldquo;<a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/setup.py" target="_blank" rel="noopener noreferrer">
    ./setup.py
</a>
&rdquo; defines what will happen when you run <code>pip install .</code> in the root directory of the project. I created <code>CMakeExtention</code> and <code>CMakeBuild</code> (<a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/setup.py#L23-L68" target="_blank" rel="noopener noreferrer">
    here
</a>
) and pass them to <code>setup</code> function (<a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/setup.py#L92-L105" target="_blank" rel="noopener noreferrer">
    here
</a>
) so that the C++ library <code>libPmppTorchOps</code> (under &ldquo;./csrc/&rdquo;) will be built and installed before installing the python package.</p>
<p>You can easily understand what I did by reading the source code of these two files, and there is one more thing I want to mention.</p>
<p>Based on <a href="/blogs/create-a-libtorch-project/#2-create-a-c-cuda-and-libtorch-project">2. Create a C++, CUDA and LibTorch Project</a>, you should find that the generated shared library is under <code>./build/lib</code> ending with <code>.so</code> on linux or <code>.dll</code> on windows. Additionally, I added an install procedure <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/setup.py#L62-L68" target="_blank" rel="noopener noreferrer">
    here
</a>
 which will copy the shared libraries to &ldquo;./src/pmpp/_torch_ops&rdquo;.</p>
<blockquote>
<p>Note that &ldquo;<a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/tree/96685ab/src/pmpp" target="_blank" rel="noopener noreferrer">
    ./src/pmpp
</a>
&rdquo; is already an existing directory being the root of the actual python package, and &ldquo;./src/pmpp/_torch_ops&rdquo; will be created automatically while installing the shared libraries.</p></blockquote>
<p>The problem is, when packaging the python project, only the directory containing &ldquo;__init__.py&rdquo; will be considered as a package (or module), and I don&rsquo;t want to add this file to &ldquo;./src/pmpp/_torch_ops&rdquo; due to my mysophobia üò∑. Therefore, I used <code>find_namespace_packages</code> instead of <code>find_packages</code> and specified <code>package_data</code> to include the shared libraries <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/setup.py#L106-L108" target="_blank" rel="noopener noreferrer">
    here
</a>
.</p>
<h3 id="32-install-the-python-package">3.2. Install the Python Package</h3>
<p>If you are planning to build your libraries with dependencies listed <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/pyproject.toml#L26-L31" target="_blank" rel="noopener noreferrer">
    here
</a>
 while installing the python project, I don&rsquo;t really suggest installing it in an isolated python environment (which is the default behavior of setuptools). All packages listed <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/pyproject.toml#L2" target="_blank" rel="noopener noreferrer">
    here
</a>
 have to be re-installed and in our case you need to at least append <code>torch</code> to that list.</p>
<p>Alternatively, try this command, which will directly use the torch installed in current conda environment:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">pip install --no-build-isolation -v .
</span></span></code></pre></div><h3 id="33-test-the-custom-torch-operators-in-python">3.3. Test the Custom Torch Operators in Python</h3>
<p>As long as you have the shared libraries built in <a href="#2-create-a-c-cuda-and-libtorch-project">
    2. Create a C&#43;&#43;, CUDA and LibTorch Project
</a>
, all you need to do is to use <code>torch.ops.load_library</code> to load the shared libraries and call the registered operators.</p>
<p>I write this process into &ldquo;<a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/src/pmpp/__init__.py" target="_blank" rel="noopener noreferrer">
    src/pmpp/__init__.py
</a>
&rdquo;, so the time you import <code>pmpp</code> in python, your custom torch operators will be ready to use. See <a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/test/test.py" target="_blank" rel="noopener noreferrer">
    this file
</a>
 for an example of testing the operators.</p>
]]></content:encoded></item><item><title>VSCode: Debug Python</title><link>https://jamesnulliu.github.io/blogs/vscode-debug-python/</link><pubDate>Wed, 09 Oct 2024 10:40:00 +0800</pubDate><guid>https://jamesnulliu.github.io/blogs/vscode-debug-python/</guid><description>How to configure launch.json in VSCode for debugging Python</description><content:encoded><![CDATA[<blockquote>
<p>Here is my template repository of building a Python project (with Pytorch and cutomized CUDA kernels): <a href="https://github.com/jamesnulliu/VSC-Python-Project-Template">VSC-Pytorch-Project-Template</a> !</p></blockquote>
<p>First, add the following code to &ldquo;./.vscode/launch.json&rdquo; (create the file if it does not exist):</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;version&#34;</span><span class="p">:</span> <span class="s2">&#34;0.2.0&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;configurations&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Other configurations...,
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;DebugPy: Current File&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;debugpy&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;request&#34;</span><span class="p">:</span> <span class="s2">&#34;launch&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;console&#34;</span><span class="p">:</span> <span class="s2">&#34;integratedTerminal&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Whether to jump to external code when debugging
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="nt">&#34;justMyCode&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Path to the Python file to debug; If set to &#34;${file}&#34;, it will 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// use the currently opened file
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="nt">&#34;program&#34;</span><span class="p">:</span> <span class="s2">&#34;${file}&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Arguments to pass to the program
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="nt">&#34;args&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;&lt;arg1&gt;&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;&lt;arg2&gt;&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Environment variables
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="nt">&#34;env&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="nt">&#34;&lt;YOUR_ENV_VAR&gt;&#34;</span><span class="p">:</span> <span class="s2">&#34;&lt;VALUE&gt;&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="p">},</span>
</span></span><span class="line"><span class="cl">        <span class="p">},</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Other configurations...,
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Next, click on the &ldquo;Run and Debug&rdquo; icon on the left sidebar, choose the configuration with the name you specified in &ldquo;launch.json&rdquo;, then click on the green play button to start debugging.</p>
]]></content:encoded></item><item><title>Dive into Paged Attention</title><link>https://jamesnulliu.github.io/blogs/dive-into-paged-attention/</link><pubDate>Mon, 07 Oct 2024 12:00:00 +0800</pubDate><guid>https://jamesnulliu.github.io/blogs/dive-into-paged-attention/</guid><description>Dive into the paged attention mechanism of vLLM.</description><content:encoded><![CDATA[<h2 id="1-why-attentions--only-depends-on">1. Why Attention&rsquo;s $O_i$ only depends on $Q_i$</h2>
<p>The Attention formula is:</p>
$$
O=Attention(Q,K,V)=softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$<p>Assume $Q=\begin{bmatrix}Q_0\\Q_1\end{bmatrix}$, $K=\begin{bmatrix}K_0\\K_1\end{bmatrix}$</p>
<p>Then:</p>
$$
O=softmax(\frac{\begin{bmatrix}Q_0K_0^T&Q_0K_1^T\\Q_1K_0^T&Q_1K_1^T\end{bmatrix}}{\sqrt{d_k}})V
$$<p>Let:</p>
$$
A=\begin{bmatrix}A_0\\A_1\end{bmatrix}=\begin{bmatrix}Q_0K_0^T&Q_0K_1^T\\Q_1K_0^T&Q_1K_1^T\end{bmatrix},f(x)=\frac{softmax(x)}{\sqrt{d_k}}
$$<p>At this point, $A_1$ only depends on $Q_1$ and is independent of $Q_0$, so:</p>
$$
\begin{bmatrix}O_0\\O_1\end{bmatrix}=O=\begin{bmatrix}f(A_0)\\f(A_1)\end{bmatrix}V=\begin{bmatrix}f(A_0)V\\f(A_1)V\end{bmatrix}
$$<p>Therefore, $O_i$ only depends on $A_i$, and according to the definition of $A$, $A_i$ only depends on $Q_i$, meaning:</p>
<p>The $i$-th output of the Attention matrix only depends on the $i$-th $Q$ and is independent of previous $Q$s.</p>
<p><strong>Summary</strong>:</p>
<ul>
<li>When predicting the next token, we only need to calculate the corresponding <code>Q_new</code> for the new token and perform attention calculation with the previously cached <code>K_cache</code> and <code>V_cache</code>.</li>
<li>The new <code>K_new</code> and <code>V_new</code> will be added to the cache to provide the foundation for the next token generation.</li>
<li>This process avoids repeated calculations for all historical tokens, greatly improving efficiency.</li>
</ul>
<h2 id="2-kv-cache-incremental-process">2. KV Cache Incremental Process</h2>
<p>Example code:</p>
<p>&ldquo;<a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/cf690614d004aa647aefccb8db3eac83255cb99e/src/pmpp/models/attention.py">Learning-Programming-Massively-Parallel-Processors/src/pmpp/models/attention.py</a>&rdquo;</p>
<h3 id="21-prefilling-initial-input-complete-sequence-calculation">2.1. Prefilling: Initial Input (Complete Sequence) Calculation</h3>
<ul>
<li>For the initial input sequence <code>(seq_len, vocab_size)</code>, we obtain <code>Q</code>, <code>K</code>, and <code>V</code> through linear transformations, all with shape <code>(seq_len, embed_dim)</code> (<em>see <a href="">this</a></em>).</li>
<li>Using <code>Q</code> and <code>K</code> to calculate attention scores through dot product, then combining with <code>V</code> to compute the output <code>(seq_len, embed_dim)</code> (<em>see <a href="">this</a></em>), this is the first complete calculation for the initial sequence.</li>
</ul>
<h3 id="22-decoding-incremental-calculation-when-predicting-next-token">2.2. Decoding: Incremental Calculation When Predicting Next Token:</h3>
<p>When predicting the next token, there&rsquo;s no need to perform complete <code>Q</code>, <code>K</code>, <code>V</code> calculations for the entire sequence. Instead, only an incremental calculation for the newly generated token is required. The process is as follows:</p>
<ol>
<li><strong>Input New Token</strong>: Take the generated token from last round as input sequence, obtain <code>Q_new</code>, <code>K_new</code> and <code>V_new</code> through linear transformation.</li>
<li><strong>Update KV Cache</strong>: <code>K_new</code> and <code>V_new</code> are added to the end of <code>K_cache</code> and <code>V_cache</code>, making them a pair of <code>(kv_len + 1, embed_dim)</code> vectors.</li>
<li><strong>Attention Calculation with Updated <code>K_cache</code> and <code>V_cache</code></strong>: Use <code>Q_new</code> to perform attention calculation with updated <code>K_cache</code> and <code>V_cache</code>. <code>Q_new</code> can directly perform dot product with <code>K_cache</code> to get attention scores, then combine with <code>V_cache</code> to get new output.</li>
<li><strong>Output</strong>: The output after attention calculation has shape <code>(1, embed_dim)</code>, which is the newly generated token.</li>
</ol>
<h2 id="3-paged-attention-in-vllm">3. Paged Attention in vllm</h2>
<h3 id="31-motivation-memory-wastes">3.1. Motivation: Memory Wastes</h3>
<p><img alt="memory-wastes.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/memory-wastes.png"></p>
<p>The above figure shows possible memory waste scenarios. The main issue is that we don&rsquo;t know where the EOS (end of sequence) token is. Random memory allocation may lead to significant memory fragmentation, resulting in reduced throughput.</p>
<h3 id="32-solution-managing-caches-with-pages">3.2. Solution: Managing Caches with Pages</h3>
<p><img alt="paged-attention-animation.webp" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/paged-attention-animation.webp"></p>
<p>The above figure demonstrates how vLLM manages memory using Paged Attention.</p>
<p>In simple terms, before inference begins, vLLM allocates two long Tensors (<code>k_cache</code> and <code>v_cache</code>) for each Decoder Layer, dividing these Tensors into continuous equal-length PA blocks (each row in the figure represents one PA Block). Each PA Block can store K or V cache for <code>BLOCK_SIZE</code> tokens (each token&rsquo;s shape can be recognized as <code>(num_heads, head_size)</code>).</p>
<p>Therefore, the shapes of <code>k_cache</code> and <code>v_cache</code> can be recognized as <code>(num_blocks, block_size, num_heads, head_size)</code>.</p>
<p>For a continuous sequence, PA blocks are allocated before the prefilling stage, and during inference:</p>
<ul>
<li>When computing prompt attention, the input K and V are first stored in <code>k_cache</code> and <code>v_cache</code> according to PA blocks; then attention is calculated using the entire QKV.</li>
<li>When computing new tokens, Q and the block table are used to calculate attention during the decode phase; at this point, the memory access is to the PA blocks in <code>k_cache</code> and <code>v_cache</code>.</li>
</ul>
<h2 id="5-paged-attention-kernel-in-details">5. Paged Attention Kernel in Details</h2>
<blockquote>
<p>References:</p>
<ul>
<li><a href="https://docs.vllm.ai/en/latest/dev/kernel/paged_attention.html">vLLM Paged Attention</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/673284781">vLLMÁöáÂÜ†‰∏äÁöÑÊòéÁè†ÔºöÊ∑±ÂÖ•ÊµÖÂá∫ÁêÜËß£PagedAttention CUDAÂÆûÁé∞</a></li>
</ul></blockquote>
<p>The general structure of the Paged Attention kernel is as follows:</p>
<p><img alt="pa-cal.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-cal.png"></p>
<h3 id="51-ËæìÂÖ•ËæìÂá∫ËæìÂá∫ÂàÜÊûêÂíåÂèÇÊï∞ËØ¥Êòé">5.1. ËæìÂÖ•ËæìÂá∫ËæìÂá∫ÂàÜÊûêÂíåÂèÇÊï∞ËØ¥Êòé</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Grid: (num_heads, num_seqs, 1).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">template</span><span class="o">&lt;</span>
</span></span><span class="line"><span class="cl"><span class="k">typename</span> <span class="n">scalar_t</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">HEAD_SIZE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">NUM_THREADS</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">PARTITION_SIZE</span> <span class="o">=</span> <span class="mi">0</span><span class="o">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="n">__device__</span> <span class="kt">void</span> <span class="n">paged_attention_kernel</span><span class="p">(</span>
</span></span><span class="line"><span class="cl"><span class="p">...</span> <span class="c1">// Other side args.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="n">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">out</span><span class="p">,</span>       <span class="c1">// [num_seqs, num_heads, max_num_partitions, head_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="n">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">q</span><span class="p">,</span>         <span class="c1">// [num_seqs, num_heads, head_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="n">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">k_cache</span><span class="p">,</span>   <span class="c1">// [num_blocks, num_kv_heads, head_size/x, block_size, x]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="n">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">v_cache</span><span class="p">,</span>   <span class="c1">// [num_blocks, num_kv_heads, head_size, block_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">...</span> <span class="c1">// Other side args.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">)</span>
</span></span></code></pre></div><p>Ê®°ÊùøÂèÇÊï∞ËØ¥Êòé:</p>
<ul>
<li><code>scalar_t</code> ÂÖÉÁ¥†Á±ªÂûã (ÂÆûÈôÖ‰ª£Á†Å‰∏≠ËøòÊúâ <code>cache_t</code> Ë°®Á§∫ KV cache ÁöÑÂÖÉÁ¥†Á±ªÂûã).</li>
<li><code>HEAD_SIZE</code> ÊØè‰∏™ head ‰∏≠ÂÖÉÁ¥†Êï∞Èáè.</li>
<li><code>BLOCK_SIZE</code> ÊØè‰∏™ PA block ‰∏≠ÁöÑ token Êï∞Èáè.
<blockquote>
<ol>
<li>KV cache Ë¢´Â≠òÂÇ®Âú®‰∏çÂêå PA blocks. ÊØè‰∏™ PA block Â≠òÂÇ®‰∏Ä‰∏™ head ‰∏≠ <code>BLOCK_SIZE</code> ‰∏™ token.<br>
‰æãÂ¶Ç, Ëã• <code>BLOCK_SIZE=16</code>, <code>HEAD_SIZE=128</code>, Âàô‰∏Ä‰∏™  PA block ËÉΩÂ≠òÂÇ®‰∏Ä‰∏™ head ÁöÑ <code>16 * 128 = 2048</code> ‰∏™ÂÖÉÁ¥†.</li>
<li>ÊØè‰∏™ PA block ÂèØËÉΩÂè™ÂåÖÂê´‰∏ÄÈÉ®ÂàÜÁöÑ context tokens.</li>
<li>‰ªé page ËßíÂ∫¶Áúã, KV cache ÊòØËã•Âπ≤‰∏™ page ÁöÑÈõÜÂêà;</li>
</ol></blockquote>
</li>
<li><code>NUM_THREADS</code> ÊØè‰∏™ CUDA thread block ‰∏≠ thread ÁöÑÊï∞Èáè.</li>
<li><code>PARTITION_SIZE</code> ÂèÇ‰∏é TP ÁöÑ GPU Êï∞Èáè, ÈªòËÆ§ 0 Ë°®Á§∫ÂçïÂç°. (‰ª•‰∏ãÈÉΩ‰ª•ÂçïÂç°‰∏∫‰æãËØ¥Êòé)</li>
</ul>
<p>È¢ùÂ§ñÁöÑ‰∏Ä‰∫õÂèÇÊï∞:</p>
<ul>
<li><code>num_seqs</code>: Êú¨Ê¨°Êé®ÁêÜËØ∑Ê±Ç sequence Êï∞ÁõÆ.
<blockquote>
<p>Áî±‰∫éËøô‰∏™ kernel Âè™Â§ÑÁêÜ decode Èò∂ÊÆµÂçï query attention, ÊâÄ‰ª•ÂÆûÈôÖ‰∏äÊØè‰∏™ sequence Âè™Êúâ‰∏Ä‰∏™ query token.</p></blockquote>
</li>
<li><code>num_heads</code>: Q ÁöÑ head Êï∞ÁõÆ</li>
<li><code>num_kv_heads</code>: KV ÁöÑ head Êï∞ÁõÆ, ÂØπ‰∫é MHA ÂÖ∂ÂÄºÂíå <code>num_heads</code> Áõ∏Âêå; Â¶ÇÊûúÊòØ GQA, MQA Âàô <code>num_kv_heads</code> Â∞è‰∫é <code>num_head</code>.</li>
<li><code>head_size</code>: Âç≥ <code>HEAD_SIZE</code></li>
<li><code>k_cache: (num_blocks, num_kv_heads, head_size/x, block_size, x)</code>, ÂÖ∂‰∏≠ <code>x</code> Ë°®Á§∫ <code>THREAD_GROUP_SIZE * VEC_SIZE</code> ÁöÑÂ§ßÂ∞è (ÂêéÈù¢‰ºöÁªÜËØ¥).</li>
</ul>
<p>‰∏ãÈù¢ÁªìÂêà GPU architecture ÂàùÊ≠•ÂàÜÊûê‰∏Ä‰∏ãÂèÇÊï∞.</p>
<p><img alt="gpu-archi.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/gpu-archi.png"></p>
<p>üßê <strong>‰∏∫‰ªÄ‰πàË¶ÅÂàÜ thread group?</strong></p>
<ul>
<li>Âõ†‰∏∫ÂΩì‰∏Ä‰∏™ cuda block Ë¶ÅÂèñÁöÑÊï∞ÊçÆÊØîËæÉÂ∞ëÁöÑÊó∂ÂÄô (ËÆ°ÁÆó QK), ‰∏Ä‰∏™ thread group ÂàÜÂà´‰∏ÄÊ¨°Âèñ Q Âíå K ‰∏≠ 16B; ÂΩì‰∏Ä‰∏™ cuda block Ë¶ÅÂèñÁöÑÊï∞ÊçÆÊØîËæÉÂ§öÁöÑÊó∂ÂÄô (ËÆ°ÁÆó LV), ‰∏Ä‰∏™ thread Âèñ 16B.</li>
</ul>
<h3 id="52shared-memory-q_vecs-ÁöÑÂÜôÂÖ•">5.2.Shared Memory: <code>q_vecs</code> ÁöÑÂÜôÂÖ•</h3>
<p>‰ªé kernel ‰∏≠ÁöÑÁ¨¨‰∏Ä‰∏™Áî≥ËØ∑ÁöÑ shared memory ÂºÄÂßãËØ¥.</p>
<blockquote>
<p>ÂÖ≥‰∫é shared memeory:</p>
<ol>
<li>Âú® kernel ‰∏≠Áî≥ËØ∑ÁöÑ shared memory Ë¢´ÂΩìÂâç cuda block ‰∏≠ÁöÑÊâÄÊúâ thread ÂÖ±‰∫´.</li>
<li>shared memory ÁöÑ‰ΩúÁî®ÊòØ‰∏∫‰∫ÜÂáèÂ∞ë global memory ÁöÑËÆøÈóÆÊ¨°Êï∞ÔºåÊèêÈ´òËÆøÂ≠òÊïàÁéá.</li>
</ol></blockquote>
<p>‰ª•‰∏ã‰ª£Á†ÅÁî≥ËØ∑‰∫Ü‰∏ÄÂùó shared memroy Ë¢´Êï¥‰∏™ CUDA Block ‰∏≠ÊâÄÊúâ kernel ÂÖ±‰∫´:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__shared__</span> <span class="n">Q_vec</span> <span class="n">q_vecs</span><span class="p">[</span><span class="n">THREAD_GROUP_SIZE</span><span class="p">][</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span></code></pre></div><p>È¶ñÂÖà, <code>q_vecs</code> Ë¶ÜÁõñ‰∫Ü Q ‰∏≠ <code>head_size</code> ‰∏™ÂÖÉÁ¥† - Ëøô‰πüÊòØ‰∏Ä‰∏™ cuda block ÈúÄË¶ÅÂ§ÑÁêÜÁöÑÊï∞ÊçÆÈáè.</p>
<p>Êé•ÁùÄÂÜçËØ¥‰∏§‰∏™Áª¥Â∫¶ÁöÑÂèÇÊï∞ÁöÑÊÑèÊÄù:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_ELEMS_PER_THREAD</span> <span class="o">=</span> <span class="n">HEAD_SIZE</span> <span class="o">/</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_VECS_PER_THREAD</span> <span class="o">=</span> <span class="n">NUM_ELEMS_PER_THREAD</span> <span class="o">/</span> <span class="n">VEC_SIZE</span><span class="p">;</span>
</span></span></code></pre></div><ul>
<li><code>THREAD_GROUP_SIZE</code>: ÊØè‰∏™ thread group ‰∏≠ÁöÑ thread Êï∞Èáè. Ê≥®ÊÑè, ‰∏Ä‰∏™ cuda block ‰∏≠Êúâ <code>NUM_THREADS</code> ‰∏™ thread, <code>NUM_THREAD_GROUPS</code> ‰∏™ thread group. <code>THREAD_GROUP_SIZE = MAX(WARP_SIZE/BLOCK_SIZE, 1)</code>.</li>
<li><code>NUM_VECS_PER_THREAD</code>: <code>HEAD_SIZE</code> ËÉΩË¢´ÂàÜÊàêÂ§öÂ∞ë‰∏™ 16B. (Ëøô‰∏™ÂèòÈáèËøô‰πàÂëΩÂêçÁöÑÁêÜÁî±ÊòØÂêéÈù¢ËØªÂèñ K ÁöÑÊó∂ÂÄôÊØè‰∏™ thread ‰ºöÂæÄËá™Â∑±ÁöÑÂØÑÂ≠òÂô®ÂÜÖËØª <code>NUM_VECS_PER_THREAD</code> ‰∏™ k_vec.)</li>
</ul>
<blockquote>
<p>ËØÅÊòé: <code>q_vecs</code> Ë¶ÜÁõñ Q ÁöÑ‰∏Ä‰∏™ head, Âπ∂‰∏î <code>NUM_VECS_PER_THREAD</code> Ë°®Á§∫ Q ÁöÑ‰∏Ä‰∏™ head Ë¢´ÂàÜÊàêÂ§öÂ∞ë‰∏™ 16B.<br>
=&gt; <code>THREAD_GROUP_SIZE</code> * <code>VEC_SIZE</code> = 16B / <code>sizeof(scalar_t)</code>;<br>
=&gt; <code>NUM_VECS_PER_THREAD</code> * 16B / <code>sizeof(scalar_t)</code> = <code>HEAD_SIZE</code>;</p></blockquote>
<p>ÁÑ∂ÂêéÁúã load Q ÁöÑ‰ª£Á†Å, Âª∫ËÆÆÁªìÂêà‰∏ãÈù¢ÁöÑÂõæ‰∏ÄËµ∑Áúã:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl">  <span class="c1">// Load Q to shmem
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_group_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREAD_GROUPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">vec_idx</span> <span class="o">=</span> <span class="n">thread_group_offset</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">q_vecs</span><span class="p">[</span><span class="n">thread_group_offset</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">        <span class="o">*</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="n">Q_vec</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">q_ptr</span> <span class="o">+</span> <span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><ul>
<li><code>thread_group_idx</code> Ë°®Á§∫ÂΩìÂâç thread Â±û‰∫éÂΩìÂâç cuda block ‰∏≠Á¨¨Âá†‰∏™ thread group.</li>
<li><code>thread_group_offset</code> Ë°®Á§∫ÂΩìÂâç thread Âú®ÂΩìÂâç thread group ‰∏≠ÊòØÁ¨¨Âá†‰∏™ thread.</li>
</ul>
<p><img alt="pa-load-q.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-load-q.png"></p>
<p>‰∏äÂõæÂ±ïÁ§∫‰∫ÜÂæ™ÁéØÂÖ∑‰ΩìÊòØÊÄé‰πàË∑ëÁöÑ.</p>
<ul>
<li>‰∏Ä‰∏™Á¥´Ëâ≤ÁÆ≠Â§¥Ë°®Á§∫‰∏Ä‰∏™ thread group.</li>
<li><code>NUM_VECS_PER_THREAD</code> Ë°®Á§∫ <code>HEAD_SIZE</code> ËÉΩË¢´ÂàÜÊàêÂ§öÂ∞ë‰∏™ 16B.</li>
<li>ÂÆûÈôÖËØªÂèñ Q ÁöÑÂÜÖÂ≠òÊó∂, ÊâÄÊúâ thread group ‰ªé Q ÁöÑËµ∑Âßã‰ΩçÁΩÆÁ¥ßÂØÜÊéíÂàó, Ê†πÊçÆÂõæ‰∏äÁúãÁöÑËØù‰∏ÄÂÖ±Êúâ <code>NUM_THREAD_GROUPS</code> ‰∏™Á¥´Ëâ≤ÁÆ≠Â§¥.</li>
<li>ÊâÄÊúâ thread group ËØªÂèñ‰∏ÄÊ¨° Q Âπ∂Â≠òÂÖ• <code>q_vecs</code> ÂØπÂ∫îÂæ™ÁéØ‰∏≠ÁöÑ‰∏ÄÊ¨°Ëø≠‰ª£; Âõ†Ê≠§‰∏ãÊ¨°Ëø≠‰ª£ thread group ÈúÄË¶ÅÂêëÂêéÂÅèÁßª <code>NUM_THREAD_GROUPS</code> ‰∏™‰ΩçÁΩÆ (‰æãÂ¶Ç <code>i</code> ‰ªé 1 Âèò‰∏∫ 7).</li>
<li>Ê≠§Â§ñ, ËØª‰∏ÄÊ¨° 16B ÂØπÂ∫î‰∏Ä‰∏™ thread Êù•ËØ¥Ëá™ÁÑ∂‰πüÊòØÂèñ‰∏Ä‰∏™ VEC.</li>
<li>ÂØπÂ∫îÂà∞ kernel ÁºñÂÜô, ËøòÈúÄË¶ÅËÆ°ÁÆóÂΩìÂâç thread ÂÖ∑‰ΩìËØªÂèñÂì™‰∏™ vec; Âõ†Ê≠§ÂæóÂà∞ <code>vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE</code>.</li>
</ul>
<blockquote>
<p>ü§î ËøôÈáå‰ºö‰∏ç‰ºöÊúâ bank conflict?</p></blockquote>
<p>ÊÄª‰πãÁé∞Âú®Êàë‰ª¨Êää <code>(1, head_size)</code> Â§ßÂ∞èÁöÑÂÖÉÁ¥†ËØªÂà∞‰∫Ü cuda block ÂÖ±‰∫´ÁöÑ shared memory <code>q_vecs</code> ‰∏≠.</p>
<h3 id="53-ËØªÂèñ-k-cache-Âπ∂ËÆ°ÁÆó-qk">5.3. ËØªÂèñ K Cache Âπ∂ËÆ°ÁÆó QK</h3>
<p>Áé∞Âú®‰ªé cuda block ÁöÑËßíÂ∫¶Áúã, ÂΩìÂâç block Â∑≤ÁªèËé∑Âæó‰∫ÜËá™Â∑±Ë¶ÅÁÆóÁöÑ Q ‰∏≠ÁöÑ‰∏Ä‰∏™ head (ÂΩ¢Áä∂‰∏∫ <code>(1, head_size)</code>), Êé•‰∏ãÊù•Â∞±ÊòØËÆ°ÁÆó Q Âíå K ÁöÑÁÇπÁßØ.</p>
<p>ÁÇπÁßØËøáÁ®ãÊòØÊääÂΩìÂâç block Êã•ÊúâÁöÑ Q head ÂíåÊï¥‰∏™ K Cache (Ëø≠‰ª£Âú∞) ËøõË°åÁÇπÁßØËøêÁÆó. ÂèÇËÄÉ‰∏ãÂõæ:</p>
<p><img alt="pa-cal-kq-01.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-cal-kq-01.png"></p>
<p>QK ‰πòÁßØÂÆûÈôÖ‰∏äË¢´ÊöÇÂ≠òÂú® <code>logits</code> (‰πüÊòØ‰∏ÄÂùó shared memory) ‰∏≠, ‰πãÂêé‰ºöË¢´Áî®Êù•ËÆ°ÁÆó softmax.</p>
<p>üòá Áúã‰∏ãÂæ™ÁéØÁöÑÂÖ∑‰Ωì‰ª£Á†ÅÂêß:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Loop 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Physical block calculation ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// Loop 2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Offset calculation ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">K_vec</span> <span class="n">k_vecs</span><span class="p">[</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Loop 3
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Load K to `k_vecs` ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="kt">float</span> <span class="n">qk</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">Qk_dot</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span> <span class="n">THREAD_GROUP_SIZE</span><span class="o">&gt;::</span><span class="n">dot</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                        <span class="n">q_vecs</span><span class="p">[</span><span class="n">thread_group_offset</span><span class="p">],</span> <span class="n">k_vecs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Add the ALiBi bias if slopes are given.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">qk</span> <span class="o">+=</span> <span class="p">(</span><span class="n">alibi_slope</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">?</span> <span class="n">alibi_slope</span> <span class="o">*</span> <span class="p">(</span><span class="n">token_idx</span> <span class="o">-</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">thread_group_offset</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Store the partial reductions to shared memory.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// Mask
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// Update the max value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>ÂÖàËØ¥Á¨¨‰∏Ä‰∏™Âæ™ÁéØ, ÂÖ∂‰∏≠ÊØîËæÉÈáçË¶ÅÁöÑÂá†‰∏™ÂèÇÊï∞ÂÆö‰πâÂ¶Ç‰∏ã:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// [start_block_idx, end_block_idx) is the range of blocks to process.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">int</span> <span class="n">start_block_idx</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">    <span class="n">USE_PARTITIONING</span> <span class="o">?</span> <span class="n">partition_idx</span> <span class="o">*</span> <span class="nl">num_blocks_per_partition</span> <span class="p">:</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="c1">// If not using partitioning, `end_block_idx` should be equal to `num_seq_blocks`.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">int</span> <span class="n">end_block_idx</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">    <span class="n">MIN</span><span class="p">(</span><span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">num_blocks_per_partition</span><span class="p">,</span> <span class="n">num_seq_blocks</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="c1">// Number of blocks to process.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">int</span> <span class="n">num_blocks</span> <span class="o">=</span> <span class="n">end_block_idx</span> <span class="o">-</span> <span class="n">start_block_idx</span><span class="p">;</span>
</span></span></code></pre></div><p>Áî®ÊñáÂ≠óÊèèËø∞Â∞±ÊòØ:</p>
<ul>
<li><code>blk_idx</code> Ë°®Á§∫ÂΩìÂâç thread ÊâÄÂú® warp ÈúÄË¶ÅÂ§ÑÁêÜÁöÑ PA block ÁöÑÂú® <code>block_table</code> ‰∏≠Á¥¢Âºï (ÈÄªËæë‰∏äÁöÑÁ¥¢Âºï).</li>
<li><code>start_block_idx</code> Âíå <code>end_block_idx</code> Ë°®Á§∫ÂΩìÂâç cuda block ÈúÄË¶ÅÂ§ÑÁêÜÁöÑ block ËåÉÂõ¥.</li>
<li><code>num_blocks</code> Ë°®Á§∫ÂΩìÂâç cuda block ÈúÄË¶ÅÂ§ÑÁêÜÁöÑ block Êï∞Èáè.</li>
<li><code>NUM_WARPS</code> Ë°®Á§∫ÂΩìÂâç cuda block ‰∏≠ warp ÁöÑÊï∞Èáè. ‰∏Ä‰∏™ warp ÂåÖÂê´ 32 ‰∏™ thread.</li>
<li><code>warp_idx</code> Ë°®Á§∫ÂΩìÂâç warp Âú®ÂΩìÂâç cuda block ‰∏≠ÁöÑÁ¥¢Âºï.</li>
</ul>
<p>ËØ¥‰∫∫ËØùÂ∞±ÊòØÊØè‰∏™ warp Â§ÑÁêÜ‰∏Ä‰∏™ PA block, ‰∏ÄÂºÄÂßã cuda block ‰∏≠ÁöÑÊâÄÊúâ warp Á¥ßÂØÜÂú∞ÊåáÂêëÊúÄÂâçÈù¢ÁöÑ <code>NUM_WARPS</code> ‰∏™ PA block, ÊØèÊ¨°Âæ™ÁéØÊâÄÊúâ warp ÂêëÂêéÂÅèÁßª <code>NUM_WARPS</code> ‰∏™ PA block ÁöÑÈïøÂ∫¶. ÂèÇËÄÉ‰∏ãÂõæ:</p>
<p><img alt="pa-cal-kq-02.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-cal-kq-02.png"></p>
<blockquote>
<p>üîî ËøôÈáåÂÜçÂõûÈ°æ‰∏Ä‰∏ã, ‰∏Ä‰∏™ PA block ÈáåÂ≠òÊîæ‰∫Ü <code>BLOCK_SIZE</code> ‰∏™ token ÁöÑ K Êàñ V cache.</p></blockquote>
<p>ÊâÄ‰ª•ËØ¥Ëøô‰∏™Âæ™ÁéØÂíå‰∏äÈù¢ËØªÂèñ Q ÁöÑÂæ™ÁéØ‰∏Ä‰∏™Â∞øÊÄßü§Æ, ‰∏çËøáÊòØ‰ª• warp ÁöÑÁ≤íÂ∫¶Â§ÑÁêÜÊï∞ÊçÆ;</p>
<p>ËøõÂÖ•‰∫ÜÁ¨¨‰∏Ä‰∏™Âæ™ÁéØÂÜÖÈÉ®, Á¨¨‰∏ÄÊ≠•ÂΩìÁÑ∂ÊòØËÆ°ÁÆóÂΩìÂâç thread ÂØπÂ∫îÁöÑ warp Â∫îËØ•ËÆ°ÁÆóÂì™‰∏™ PA block (Áâ©ÁêÜ‰∏äÁöÑÁ¥¢Âºï), Âõ†Ê≠§ÂæóÂà∞‰∫Ü <code>physical_block_number</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">const</span> <span class="kt">int64_t</span> <span class="n">physical_block_number</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">    <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">block_table</span><span class="p">[</span><span class="n">block_idx</span><span class="p">]);</span>
</span></span></code></pre></div><hr>
<p>ÁÑ∂ÂêéËß£ÈáäÁ¨¨‰∫å‰∏™Âæ™ÁéØ, Á¨¨‰∫å‰∏™Âæ™ÁéØÁöÑÊï¥‰ΩìÁõÆÊ†áÂ∞±ÊòØËÆ©ÂΩìÂâç warp ËÆ°ÁÆóÂ•ΩËá™Â∑±Ë¥üË¥£ÁöÑ PA block ‰∏≠ <code>BLOCK_SIZE</code> ‰∏™ token ÁöÑ QK ‰πòÁßØ.</p>
<p>ÂÖàÁúã‰∏Ä‰∏ã <code>i</code> ÁöÑ‰∏äÁïå:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">    <span class="n">DIVIDE_ROUND_UP</span><span class="p">(</span><span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">WARP_SIZE</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="c1">// Loop 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Loop 2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></div><p>‰ªé kernel ËßíÂ∫¶Áúã, ÊØè‰∏™ thread ÈúÄË¶ÅËæÖÂä©ÂΩìÂâç warp ËÆ°ÁÆóËá™Â∑±Ë¥üË¥£ÁöÑ‰∏ÄÊï¥‰∏™ PA block (ÂåÖÂê´ <code>BLOCK_SIZE</code> ‰∏™ token), ËÄåÊàë‰ª¨ÊääËøô‰∏™ËøáÁ®ãÊãÜÂàÜ‰∏∫ Loop 2 ‰∏≠ÁöÑ <code>NUM_TOKEN_PER_THREAD_GROUP</code> (‰πüÂ∞±ÊòØ <code>ceil(BLOCK_SIZE / 32)</code>) Ê¨°Âæ™ÁéØ;</p>
<p>ËØ¥‰∫∫ËØùÂ∞±ÊòØ<strong>‰∏Ä‰∏™ thread group ÂØπÂ∫î‰∏Ä‰∏™ token ‰∏≠ÁöÑ‰∏Ä‰∏™ head</strong>, Â¶ÇÊûú BLOCK SIZE Â§™Â§ß‰∫ÜÂêéÈù¢ÊØè‰∏™ thread ÂêëÂêéÂÅèÁßª <code>i * WARP_SIZE</code> ‰∏™ token ÁªßÁª≠Áã†Áã†ÁÆóü§£.</p>
<p>‰πüÂõ†Ê≠§Á¨¨‰∫å‰∏™Âæ™ÁéØÂÜÖÈÉ®‰∏Ä‰∏äÊù•ÂÖàËÆ°ÁÆó‰∫ÜÂá†‰∏™ÂÅèÁßªÈáè, Âπ∂‰∏îÁî≥ËØ∑‰∫Ü thread ÂÜÖÈÉ®ÁßÅÊúâÁöÑ <code>k_vecs</code> Êï∞ÁªÑ:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">const</span> <span class="kt">int</span> <span class="n">physical_block_offset</span> <span class="o">=</span> <span class="p">(</span><span class="n">thread_group_idx</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">WARP_SIZE</span><span class="p">)</span> <span class="o">%</span> <span class="n">BLOCK_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">const</span> <span class="kt">int</span> <span class="n">token_idx</span> <span class="o">=</span> <span class="n">block_idx</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">physical_block_offset</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">K_vec</span> <span class="n">k_vecs</span><span class="p">[</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span></code></pre></div><ul>
<li><code>thread_group_idx</code> Ë°®Á§∫ÂΩìÂâç thread group Âú®Êï¥‰∏™ cuda block ‰∏≠ÁöÑÁ¥¢Âºï.</li>
<li>‚ò¢Ô∏è ‰∏Ä‰∏™ thread group Âú®‰∏ÄÊ¨°Âæ™ÁéØ‰∏≠Ë¥üË¥£ fetch ‰∏Ä‰∏™ PA block ‰∏≠ K cache ÁöÑ‰∏Ä‰∏™ token ‰∏≠<strong>Ëá™Â∑±Ë¥üË¥£ÁöÑ head</strong>.</li>
<li>‚ò¢Ô∏è ‰∏Ä‰∏™ thread group Ë¥üË¥£ËÆ°ÁÆó‰∏Ä‰∏™ qk ÂÄº; Ëøô‰∏™ÂÄºÊòæÁÑ∂ÊòØÁî±‰∏Ä‰∏™ Q head Âíå‰∏Ä‰∏™ K head ÁÇπÁßØÂæóÂà∞ÁöÑ.</li>
<li><code>physical_block_offset</code> Ë°®Á§∫ÂΩìÂâçË¶ÅÁÆóÁöÑ token Âú®ÂΩìÂâç PA block ‰∏≠ÁöÑÂÅèÁßªÈáè (Ê≥®ÊÑèÂíåÂâçÈù¢ÁöÑ <code>physical_block_number</code> Âå∫ÂàÜ).</li>
<li>Âä† <code>i * WARP_SIZE</code> ÁöÑÂéüÂõ†ÊòØÂ¶ÇÊûú <code>BLOCK_SIZE</code> Â§ß‰∫é 32, ÈÇ£‰πà‰∏Ä‰∏™ warp Ë¶ÅÂ§öÊ¨°Âæ™ÁéØÊâçËÉΩÂ§ÑÁêÜÂÆå‰∏Ä‰∏™ PA block ‰∏≠ÁöÑÊâÄÊúâ token, ÂØπÂ∫î <code>thread_group_idx</code> ÈúÄË¶ÅÂÅöÂÅèÁßª.</li>
<li><code>token_idx</code> Ë°®Á§∫ÂΩìÂâçË¶ÅÁÆóÁöÑ token Âú®Êï¥‰∏™ seq ÁöÑ KV cache ‰∏≠ÁöÑÁ¥¢Âºï.</li>
<li><code>k_vecs</code> ‰∏≠ËÉΩÂ≠òÊîæ <code>NUM_VECS_PER_THREAD</code> ‰∏™ VEC, ËÄå‰∏ÄÊï¥‰∏™ thread group ‰∏≠ÊâÄÊúâÁöÑ thread ÁöÑ <code>k_vecs</code> ÂêàËµ∑Êù•ÊâçËÉΩÁªÑÊàê‰∏Ä‰∏™ K ÁöÑ head (Êé®ÂØºÂèÇËÄÉ‰∏äÈù¢ Q ÁöÑ üòá). ËøôÂ∞±ÊòØ‰∏∫‰ªÄ‰πàÂêéÈù¢ÁÆó QK ÁöÑÊó∂ÂÄôË¶Å reduce.</li>
</ul>
<p>ü§î <strong>ÁúãÂà∞ËøôÈáåËØªËÄÖÂèØËÉΩÊúâ‰∏Ä‰∏™ÈóÆÈ¢ò: ‰∏Ä‰∏™ token ÁöÑ K cache Â∫îËØ•ÂØπÂ∫îÂ§ö‰∏™ head, ‰∏∫‰ªÄ‰πà‰∏äÈù¢ËØ¥‰∏Ä‰∏™ thread group Âè™Ë¥üË¥£‰∏Ä‰∏™ head?</strong><br>
Á≠î: Âõ†‰∏∫ÂÆûÈôÖËÆ°ÁÆóÁöÑÊó∂ÂÄô, ‰∏Ä‰∏™ cuda block Âè™Ë¥üË¥£ËÆ°ÁÆó‰∏Ä‰∏™ head, ÂØπÂ∫îÂà∞ K Cache ‰πÉËá≥ÂêéÈù¢ V Cache ÁöÑ‰ΩçÁΩÆ‰πüÊòØ‰∏ÄÊ†∑ÁöÑ.</p>
<blockquote>
<p>ËøôÈáåÈ¢ùÂ§ñËØ¥‰∏Ä‰∏ã, ËØª K ÁöÑ head ÁöÑ‰∏Ä‰∏™ÁõÆÊ†áÂ∫îËØ•ÊòØÂú®Â∞ΩÈáèÂ∞ëÁöÑ register ‰∏≠Ë£Ö‰∏ã‰∏Ä‰∏™ head ÁöÑÊâÄÊúâÂÖÉÁ¥†, ËøôÊ†∑ÂêéÁª≠Âíå shared memory ‰∏≠ÁöÑ Q ÂÅöÁÇπ‰πòÂπ∂ËßÑÁ∫¶ÁöÑÈÄüÂ∫¶Êõ¥Âø´. ÂÅáËÆæ‰∏Ä‰∏™ head Êúâ 128 ‰∏™ float16, ÂàôÂç†Áî® 256B, ËÄå A100 ‰∏≠‰∏Ä‰∏™ thread ÊúÄÂ§öËÉΩÊúâ 255 ‰∏™ 32-bit register (‰πüÂ∞±ÊòØ 1020B), Ê≠§Êó∂ÂèØ‰ª•ËÆ§‰∏∫‰∏Ä‰∏™ thread ËÉΩË£Ö‰∏ã‰∏Ä‰∏™ head ÁöÑÊâÄÊúâÂÖÉÁ¥†.<br>
‰ΩÜÊòØÁî±‰∫éÁõÆÂâç PA kernel Âú® <code>BLOCK_SIZE</code> ‰∏∫ 16 ÁöÑÊÉÖÂÜµ‰∏ã <code>THREAD_GROUP_SIZE</code> Á≠â‰∫é 2, Âõ†Ê≠§‰∏Ä‰∏™ thread Âè™‰ºöË£Ö‰∏Ä‰∏™ head ÁöÑ‰∏ÄÂçäÂÖÉÁ¥†, ËøôÊ†∑ÂèØËÉΩ‰ºöÂØºËá¥ register ÁöÑ‰ΩøÁî®Áéá‰∏çÈ´ò.</p></blockquote>
<hr>
<p>Êé•ÁùÄËøõÂÖ•Á¨¨‰∏â‰∏™Âæ™ÁéØ, ÁõÆÁöÑÊòØËÆ© thread group ‰ªé K cache ‰∏≠ËØª‰∏Ä‰∏™ head, Âπ∂Â≠òÂÖ• <code>k_vecs</code> ‰∏≠:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// x == THREAD_GROUP_SIZE * VEC_SIZE
</span></span></span><span class="line"><span class="cl"><span class="c1">// Each thread group fetches x elements from the key at a time.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">16</span> <span class="o">/</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">cache_t</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="c1">//...
</span></span></span><span class="line"><span class="cl"><span class="c1">// Loop 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Loop 2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">K_vec</span> <span class="n">k_vecs</span><span class="p">[</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Loop 3
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">const</span> <span class="n">cache_t</span><span class="o">*</span> <span class="n">k_ptr</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">                <span class="n">k_cache</span> <span class="o">+</span> <span class="n">physical_block_number</span> <span class="o">*</span> <span class="n">kv_block_stride</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">                <span class="n">kv_head_idx</span> <span class="o">*</span> <span class="n">kv_head_stride</span> <span class="o">+</span> <span class="n">physical_block_offset</span> <span class="o">*</span> <span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="k">const</span> <span class="kt">int</span> <span class="n">vec_idx</span> <span class="o">=</span> <span class="n">thread_group_offset</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="k">const</span> <span class="kt">int</span> <span class="n">offset1</span> <span class="o">=</span> <span class="p">(</span><span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">)</span> <span class="o">/</span> <span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="k">const</span> <span class="kt">int</span> <span class="n">offset2</span> <span class="o">=</span> <span class="p">(</span><span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">)</span> <span class="o">%</span> <span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// if Fp8KVCacheDataType::kAuto
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">k_vecs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="o">*</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="n">K_vec</span><span class="o">*&gt;</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">              <span class="n">k_ptr</span> <span class="o">+</span> <span class="n">offset1</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">offset2</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></div><p>ËÄÅËßÑÁü©, ÂÖàÁúã <code>j</code>, Êú¨Ë¥®Â∞±ÊòØ‰ªé 0 Ëø≠‰ª£Âà∞ <code>NUM_VECS_PER_THREAD</code>, ÊØèÊ¨°Ëø≠‰ª£ÂΩìÂâç thread ËØªÂèñ‰∏Ä‰∏™ VEC Â≠òÂÖ• <code>k_vecs</code> ‰∏≠.</p>
<blockquote>
<p>üîî ÂõûÈ°æ:</p>
<ol>
<li><code>NUM_VECS_PER_THREAD</code> Ë°®Á§∫‰∏Ä‰∏™ head Ë¢´ÂàÜÊàêÂ§öÂ∞ë‰∏™ 16B.</li>
<li><code>k_cache</code> ÁöÑ shape ‰∏∫ <code>(num_blocks, num_kv_heads, head_size/x, block_size, x)</code>.</li>
</ol></blockquote>
<p>ÂÖ∂‰∏≠ÁöÑ <code>x</code> Ë°®Á§∫‰∏Ä‰∏™ thread group ÈúÄË¶ÅËØªÂèñÁöÑÂÖÉÁ¥†Êï∞Èáè (<code>VEC_SIZE</code> * <code>THREAD_GROUP_SIZE</code>); Âõ†Ê≠§‰ΩúËÄÖÂ∞Ü K Cache ÁöÑ layout ÁöÑÊúÄÂêé‰∏ÄÁª¥ËÆæÁΩÆ‰∏∫ <code>x</code> ÂÖ∂ÂÆû‰πüÊòØÊñπ‰æøÂêéÁª≠ thread group ÂØπ K cache ÁöÑËØªÂèñ.</p>
<p>‰∏ãÂõæÂÖ∑‰ΩìÂ±ïÁ§∫‰∫ÜÂØªÂùÄÁöÑËøáÁ®ã:</p>
<p><img alt="pa-cal-kq-03.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-cal-kq-03.png"></p>
<p>ÂÖ∂‰∏≠:</p>
<ul>
<li>Âú® MHSA ‰∏≠, <code>num_kv_heads</code> Á≠â‰∫é <code>num_heads</code>; ËÄåÂú® GQA, MQA ‰∏≠, <code>num_kv_heads</code> Â∞è‰∫é <code>num_heads</code>.</li>
<li>(1) Ë¥üË¥£ÊâæÂà∞ÂΩìÂâç thread Â±û‰∫éÁöÑ warp Ë¶ÅÂ§ÑÁêÜÂì™‰∏™ PA block.</li>
<li>(2) Ë¥üË¥£ÊâæÂà∞ÂΩìÂâç thread Ë¶ÅËÆ°ÁÆóÁöÑ head Âú® K cache ‰∏≠ÁöÑ‰ΩçÁΩÆ. Ëøô‰∏™ head ÁöÑÁ¥¢ÂºïÂíå Q ‰∏≠ head ÁöÑÁ¥¢ÂºïÂú® MHSA ‰∏≠Áõ∏Âêå.</li>
<li>(3) Ë¥üË¥£ÊâæÂà∞ÂΩìÂâç thread group Ë¶ÅËÆ°ÁÆóÁöÑ token Âú®ÂΩìÂâç PA block ‰∏≠ÁöÑ‰ΩçÁΩÆ.</li>
<li>(5) Ë¥üË¥£ÊâæÂà∞ÂΩìÂâç thread Âú®ÈúÄË¶ÅËØªÂèñÁöÑ head (ËìùËâ≤ÈïøÊñπ‰Ωì) ‰∏≠ x ÁöÑÂÅèÁßª, ÈÄöËøá <code>j</code> ËøõË°åËø≠‰ª£ËØªÂèñ. <strong>ÊØèÊ¨°Âæ™ÁéØ thread group ‰∏≠ÁöÑÊâÄÊúâ thread Âèñ‰∏Ä‰∏™ x.</strong></li>
<li>(6) Ë¥üË¥£ÊâæÂà∞ÂΩìÂâç thread Âú® thread gruop ‰∏≠ËØªÂèñÁöÑ x ‰∏≠ VEC ÁöÑÂÅèÁßª; thread ‰∏ÄÊ¨°ËØªÂèñ‰∏Ä‰∏™ VEC.</li>
</ul>
<p>ü§î <strong>‰∏∫‰ªÄ‰πà (5) Âú®ÂÆûÈôÖÂØªÂùÄÊó∂ÈúÄË¶Å <code>* BLOCK_SIZE * x</code> ?</strong><br>
Á≠î: ËøôÊòØÊ†πÊçÆ <code>k_cache</code> ÁöÑ layout ÂæóÂà∞ÁöÑ stride. ÂêåÁêÜ (3) <code>* x</code> ‰πüÊòØ stride.</p>
<p>Á¨¨ 3 ‰∏™Âæ™ÁéØÁªìÊùüÊó∂ÂΩìÂâç warp Ë¥üË¥£ÁöÑÊØè‰∏™ token ‰∏≠ÈúÄË¶ÅÁöÑ K cache head Â∑≤ÁªèÂÖ®Ë¢´Âä†ËΩΩÂÖ• thread Êú¨Âú∞ÁöÑ <code>k_vecs</code> ‰∏≠‰∫Ü.</p>
<p>Áî±‰∫é‰∏Ä‰∏™ thread group ÁöÑ <code>k_vecs</code> ÊâçËÉΩÁúüÊ≠£ÁªÑÊàê‰∏Ä‰∏™ head, Âú®ÈÄÄÂõûÁ¨¨‰∫å‰∏™Âæ™ÁéØËøõË°å QK dot ÁöÑÊó∂ÂÄô, ÈúÄË¶ÅÂÅö‰∏™ reduction, ÂÖ∑‰ΩìÁöÑËåÉÂõ¥Â∞±ÊòØ <code>THREAD_GROUP_SIZE</code> ‰∏™ thread:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Loop 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Loop 2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">K_vec</span> <span class="n">k_vecs</span><span class="p">[</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Loop 3
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="kt">float</span> <span class="n">qk</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">Qk_dot</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span> <span class="n">THREAD_GROUP_SIZE</span><span class="o">&gt;::</span><span class="n">dot</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                             <span class="n">q_vecs</span><span class="p">[</span><span class="n">thread_group_offset</span><span class="p">],</span> <span class="n">k_vecs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></div><p>ËÆ°ÁÆóÂÆå <code>qk</code> Âêé, Áî±ÂΩìÂâç thread group ‰∏≠Á¨¨‰∏Ä‰∏™ (offset ‰∏∫ 0) ÁöÑ thread ÂØπËá™Â∑±ÂàöÊâçÁÆóÂá∫Êù•ÁöÑ <code>qk</code> ËøõË°å mask, È°∫‰æøÁúãÁúãÂ¶ÇÊûúÊ≤°Êúâ mask Êéâ, Êää <code>qk_max</code> ËµãÂÄº‰∏∫ <code>qk</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">thread_group_offset</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Store the partial reductions to shared memory.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// NOTE(woosuk): It is required to zero out the masked logits.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">bool</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">token_idx</span> <span class="o">&gt;=</span> <span class="n">seq_len</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">[</span><span class="n">token_idx</span> <span class="o">-</span> <span class="n">start_token_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">?</span> <span class="mf">0.f</span> <span class="o">:</span> <span class="n">qk</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Update the max value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">qk_max</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">?</span> <span class="nl">qk_max</span> <span class="p">:</span> <span class="n">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">qk</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>üßê <strong>‰∏∫‰ªÄ‰πàË¶ÅÂÅö mask?</strong></p>
<ul>
<li>Âõ†‰∏∫‰∏Ä‰∏™ seq ÁöÑÊúÄÂêé‰∏Ä‰∏™ PA block ÂèØËÉΩË¶ÜÁõñ‰∏çÊª° <code>BLOCK_SIZE</code> ‰∏™ token. ËøôÈáåÁöÑ mask Â∞±ÊòØÊääÈÇ£ÈÉ®ÂàÜ qk ÁΩÆÈõ∂.</li>
</ul>
<h3 id="54-softmax">5.4. Softmax</h3>
<p>ÊàëÂãí‰∏™ QK Âïä, ÊÄªÁÆóÁÆóÂÆå‰∫Ü, ÈîêÂÖã five ÈÉΩË¶ÅË¢´ÊäΩÊ∏Ö‰ªì‰∫Ü. È°µÊÑè‰∏ÅÁúü, Èâ¥ÂÆö‰∏∫ÂºÄÁÆó softmax.</p>
<p>‰∏ªË¶ÅÊ≠•È™§Â∞±ÊòØÂπøÊí≠ÁÑ∂ÂêéÁÆó, ÁÆó softmax ÈúÄË¶ÅÁü•ÈÅìÊØè‰∏™ head ÂØπÂ∫îÁöÑ qk ÁöÑÊúÄÂ§ßÂÄº. Áî±‰∫é‰∏Ä‰∏™ cuda block Ë¥üË¥£ÁöÑÂ∞±ÊòØ‰∏Ä‰∏™ head, ÂØπ‰∫éËøô‰∏™ head ‰∏äÈù¢ÁöÑËÆ°ÁÆóÊ≠•È™§‰∏ÄÂÖ±ÁÆó‰∫Ü <code>cache_len</code>‰∏™ token ÁöÑ qk, Âõ†Ê≠§ÈúÄË¶ÅÂÅö‰∏Ä‰∏™ cuda block ËåÉÂõ¥ÁöÑËßÑÁ∫¶, ÊâæÂà∞ÂÖ∂‰∏≠ÊúÄÂ§ßÁöÑ qk ÂÄº.</p>
<p>ÂÖàÂú® warp Â±ÇÈù¢ËßÑÁ∫¶.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__shared__</span> <span class="kt">float</span> <span class="n">red_smem</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">NUM_WARPS</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="c1">// Perform reduction across the threads in the same warp to get the
</span></span></span><span class="line"><span class="cl"><span class="c1">// max qk value for each &#34;warp&#34; (not across the thread block yet).
</span></span></span><span class="line"><span class="cl"><span class="c1">// The 0-th thread of each thread group already has its max qk value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">WARP_SIZE</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">mask</span> <span class="o">&gt;=</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span> <span class="n">mask</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">qk_max</span> <span class="o">=</span> <span class="n">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">VLLM_SHFL_XOR_SYNC</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">mask</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">lane</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">red_smem</span><span class="p">[</span><span class="n">warp_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">qk_max</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">__syncthreads</span><span class="p">();</span>
</span></span></code></pre></div><ul>
<li><code>red_smem</code> ÊòØ‰πãÂâçÁî≥ËØ∑ÁöÑ shared memory.</li>
<li><code>VLLM_SHFL_XOR_SYNC</code> ÊòØ‰∏Ä‰∏™ warp ÂÜÖÁöÑ shuffle Êìç‰Ωú, ÂÖ∑‰ΩìÊù•ËØ¥, Âú®ÊØèÊ¨°Âæ™ÁéØÊó∂, ÊØè‰∏™ thread ÂíåËá™Â∑±Áõ∏Ë∑ù <code>mask</code> ‰ΩçÁΩÆÁöÑÁ∫øÁ®ã‰∫§Êç¢Êï∞ÊçÆ (‰∫§Êç¢Êù•ÁöÑÊï∞ÊçÆÈÄöËøá <code>fmaxf</code> ÊØîËæÉ), Âπ∂‰∏î <code>mask</code> ‰ºöÈÄêÊ∏êÂáèÂçä, Áõ¥Âà∞ <code>THREAD_GROUP_SIZE</code> ‰∏∫Ê≠¢.</li>
<li><code>lane</code> Ë°®Á§∫ÂΩìÂâç warp ‰∏≠ÁöÑÁ∫øÁ®ãÁ¥¢Âºï.</li>
</ul>
<p>Êé•ÁùÄÂÜçÂØπÊØè‰∏™ warp ÁöÑÊúÄÂ§ßÂÄºËøõË°åËßÑÁ∫¶, Áî±‰∫éÊØè‰∏™ warp ÁöÑÊúÄÂ§ßÂÄºÈÉΩË¢´Â≠òÂÖ•‰∫Ü <code>red_smem</code> ‰∏≠, ÊâÄ‰ª•Âè™ÈúÄË¶ÅÂÜçÊ¨°ËøõË°å shuffle Êìç‰ΩúÂç≥ÂèØ.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// TODO(woosuk): Refactor this part.
</span></span></span><span class="line"><span class="cl"><span class="c1">// Get the max qk value for the sequence.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">qk_max</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">&lt;</span> <span class="n">NUM_WARPS</span> <span class="o">?</span> <span class="n">red_smem</span><span class="p">[</span><span class="n">lane</span><span class="p">]</span> <span class="o">:</span> <span class="o">-</span><span class="n">FLT_MAX</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">NUM_WARPS</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">mask</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">mask</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">qk_max</span> <span class="o">=</span> <span class="n">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">VLLM_SHFL_XOR_SYNC</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">mask</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>Ê≠§Êó∂, Á¨¨ 1 ‰∏™Á∫øÁ®ãÁöÑ <code>qk_max</code> Â∞±ÊòØÂΩìÂâç cuda block ‰∏≠ÊâÄÊúâ warp ‰∏≠ÊúÄÂ§ßÁöÑ qk ÂÄº. Â∞ÜÂÖ∂ÂπøÊí≠ÁªôÊâÄÊúâÁ∫øÁ®ã:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Broadcast the max qk value to all threads.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">qk_max</span> <span class="o">=</span> <span class="n">VLLM_SHFL_SYNC</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
</span></span></code></pre></div><p>Âú®Ëé∑Âæó‰∫Ü <code>qk_max</code> Âêé, Â∞±ÂèØ‰ª•ËÆ°ÁÆó softmax ‰∫Ü:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Get the sum of the exp values.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">float</span> <span class="n">exp_sum</span> <span class="o">=</span> <span class="mf">0.f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_tokens</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREADS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">__expf</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">qk_max</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">exp_sum</span> <span class="o">+=</span> <span class="n">val</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">exp_sum</span> <span class="o">=</span> <span class="n">block_sum</span><span class="o">&lt;</span><span class="n">NUM_WARPS</span><span class="o">&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="n">red_smem</span><span class="p">[</span><span class="n">NUM_WARPS</span><span class="p">],</span> <span class="n">exp_sum</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Compute softmax.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">float</span> <span class="n">inv_sum</span> <span class="o">=</span> <span class="n">__fdividef</span><span class="p">(</span><span class="mf">1.f</span><span class="p">,</span> <span class="n">exp_sum</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="n">f</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_tokens</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREADS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*=</span> <span class="n">inv_sum</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">__syncthreads</span><span class="p">();</span>
</span></span></code></pre></div><h3 id="55-lv-logits--value">5.5. LV (Logits * Value)</h3>
<p><img alt="pa-cal.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-cal.png"></p>
<p>‰∏äÂõæÂ±ïÁ§∫‰∫Ü LV ÁöÑËÆ°ÁÆóËøáÁ®ã, ‰∏ªË¶ÅÂå∫Âà´ÊòØÁî±‰∫éË¶ÅËÆ°ÁÆó Logits ÁöÑ shape ÂèØ‰ª•Ë°®Á§∫‰∏∫ <code>(num_heads, num_seqs, cache_len)</code>, ËÄå V ÁöÑ shape ÂèØ‰ª•Ë°®Á§∫‰∏∫ <code>(num_heads, cache_len, head_size)</code>, Âõ†Ê≠§ LV ÁöÑÁü©Èòµ‰πòÊ≥ï‰∏≠, ÊØèËÆ°ÁÆó‰∏Ä‰∏™ÂÖÉÁ¥†ÈúÄË¶ÅËØªÂèñ logits ÁöÑ‰∏ÄË°åÂíå V ÁöÑ‰∏ÄÂàóËøõË°åËÆ°ÁÆó.</p>
<p>Ê≠§Êó∂, ‰∏Ä‰∏™ cuda block ÁöÑËÅåË¥£‰ªé &ldquo;Ëá™ Q ‰∏≠ËØªÂèñ‰∏Ä‰∏™ head&rdquo; ËΩ¨Âèò‰∏∫ &ldquo;ËÆ°ÁÆó output ‰∏≠ÁöÑ‰∏Ä‰∏™ head&rdquo;.</p>
<p>üßê <strong>‰∏∫‰ªÄ‰πàÂú®ËÆ°ÁÆó LV Êó∂, ÂéªÊéâ‰∫Ü thread group ÁöÑÊ¶ÇÂøµ, ÊØè‰∏™ thread ÈÉΩË¢´ËÆæÂÆö‰∏∫ÊØèÊ¨°ËØªÂèñ 16B?</strong></p>
<ul>
<li>Âõ†‰∏∫Áé∞Âú®ÊØèËÆ°ÁÆó‰∏Ä‰∏™ÂÖÉÁ¥†, ÈúÄË¶ÅÁöÑËÆøÂ≠òÈáèÊõ¥Â§ß, Âõ†Ê≠§ÁªôÊØè‰∏™ thread ÂàÜÈÖç‰∫ÜÊõ¥Â§öÁöÑÊï∞ÊçÆËØªÂèñÈáè. ‰πüÂ∞±ÊòØËØ¥, <code>V_VEC_SIZE</code> ÊØî <code>VEC_SIZE</code> Êõ¥Â§ß.</li>
</ul>
<p>Áî±‰∫é cuda ËÆøÂ≠òÊ®°ÂºèÊåâË°åËØªÂèñÊõ¥Âø´, ÊâÄ‰ª•ÂÆûÈôÖÁöÑËÆ°ÁÆóÁªìÊûúÂú®ÈÅçÂéÜ PA block Êó∂Á∫øÁ®ãÂÜÖÈÉ®Âà©Áî® <code>accs</code> ËøõË°åÁ¥ØËÆ° (‰ª•ÂÆûÁé∞‰∏é V ÁöÑ‰∏ÄÂàóËøõË°åËÆ°ÁÆóÁöÑË°å‰∏∫):</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">=</span> <span class="n">BLOCK_SIZE</span> <span class="o">/</span> <span class="n">V_VEC_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_ROWS_PER_ITER</span> <span class="o">=</span> <span class="n">WARP_SIZE</span> <span class="o">/</span> <span class="n">NUM_V_VECS_PER_ROW</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_ROWS_PER_THREAD</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">    <span class="n">DIVIDE_ROUND_UP</span><span class="p">(</span><span class="n">HEAD_SIZE</span><span class="p">,</span> <span class="n">NUM_ROWS_PER_ITER</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// NOTE(woosuk): We use FP32 for the accumulator for better accuracy.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">float</span> <span class="n">accs</span><span class="p">[</span><span class="n">NUM_ROWS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">V_vec</span> <span class="n">v_vec</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">V_VEC_SIZE</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Load V to `v_vec` ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">v_vec_ptr</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">token_idx</span> <span class="o">+</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">seq_len</span> <span class="o">?</span> <span class="n">v_vec_ptr</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">:</span> <span class="n">zero_value</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Accumulate the dot product.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">dot</span><span class="p">(</span><span class="n">logits_vec</span><span class="p">,</span> <span class="n">v_vec</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>Áî±‰∫éÊØè‰∏™Á∫øÁ®ãË¥üË¥£ÁöÑÁ¥ØËÆ°ÈÉ®ÂàÜ‰∏çÊª°‰∏ÄÊï¥Ë°å/Âàó, ÊâÄ‰ª•ËøõË°åËßÑÁ∫¶:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Perform reduction within each warp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">mask</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">mask</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">acc</span> <span class="o">+=</span> <span class="n">VLLM_SHFL_XOR_SYNC</span><span class="p">(</span><span class="n">acc</span><span class="p">,</span> <span class="n">mask</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">acc</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// NOTE(woosuk): A barrier is required because the shared memory space for
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// logits is reused for the output.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Perform reduction across warps.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">float</span><span class="o">*</span> <span class="n">out_smem</span> <span class="o">=</span> <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">shared_mem</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">NUM_WARPS</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">mid</span> <span class="o">=</span> <span class="n">i</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Upper warps write to shared memory.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">warp_idx</span> <span class="o">&gt;=</span> <span class="n">mid</span> <span class="o">&amp;&amp;</span> <span class="n">warp_idx</span> <span class="o">&lt;</span> <span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="kt">float</span><span class="o">*</span> <span class="n">dst</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">out_smem</span><span class="p">[(</span><span class="n">warp_idx</span> <span class="o">-</span> <span class="n">mid</span><span class="p">)</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">const</span> <span class="kt">int</span> <span class="n">row_idx</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">/</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">NUM_ROWS_PER_ITER</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">HEAD_SIZE</span> <span class="o">&amp;&amp;</span> <span class="n">lane</span> <span class="o">%</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="n">dst</span><span class="p">[</span><span class="n">row_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Lower warps update the output.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">warp_idx</span> <span class="o">&lt;</span> <span class="n">mid</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">src</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">out_smem</span><span class="p">[</span><span class="n">warp_idx</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">const</span> <span class="kt">int</span> <span class="n">row_idx</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">/</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">NUM_ROWS_PER_ITER</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">HEAD_SIZE</span> <span class="o">&amp;&amp;</span> <span class="n">lane</span> <span class="o">%</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">src</span><span class="p">[</span><span class="n">row_idx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span></code></pre></div><p>ÊúÄÂêéÂÜôÂÖ•Âà∞ËæìÂá∫‰∏≠:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl">  <span class="c1">// Write the final output.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="n">warp_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">scalar_t</span><span class="o">*</span> <span class="n">out_ptr</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">+</span> <span class="n">seq_idx</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">max_num_partitions</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">        <span class="n">head_idx</span> <span class="o">*</span> <span class="n">max_num_partitions</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span> <span class="o">+</span> <span class="n">partition_idx</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">const</span> <span class="kt">int</span> <span class="n">row_idx</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">/</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">NUM_ROWS_PER_ITER</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="p">(</span><span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">HEAD_SIZE</span> <span class="o">&amp;&amp;</span> <span class="n">lane</span> <span class="o">%</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">from_float</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">out_ptr</span> <span class="o">+</span> <span class="n">row_idx</span><span class="p">),</span> <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span></code></pre></div>]]></content:encoded></item><item><title>A Simple Pytorch Trainpipeline</title><link>https://jamesnulliu.github.io/blogs/a-simple-pytorch-trainpipeline/</link><pubDate>Sun, 30 Jun 2024 01:52:00 +0800</pubDate><guid>https://jamesnulliu.github.io/blogs/a-simple-pytorch-trainpipeline/</guid><description>How to build a simple Pytorch trainpipeline.</description><content:encoded><![CDATA[<h2 id="1-introduction">1. Introduction</h2>
<p>In general, you will need these things to train a model:</p>
<ul>
<li>A Model</li>
<li>A Dataset</li>
<li>A Dataloader</li>
<li>A Loss Function (Criterion)</li>
<li>An Optimizer</li>
</ul>
<h2 id="2-model">2. Model</h2>
<p>We will build a simple model for demonstration. The model takes a tensor of shape <code>(batch_size, 10)</code> as input and outputs a tensor of shape <code>(batch_size, 2)</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># @file simple_model.py</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># Shape: (4, 10)</span>
</span></span><span class="line"><span class="cl">    <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Shape: (4, 2)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>You can run the script to check how the model works:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">python simple_model.py
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="3-dataset">3. Dataset</h2>
<p>We will build a simple dataset for demonstration. The dataset generates random data and labels.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># @file simple_dataset.py</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">SimpleDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># Shape: (10,); Element type: float32</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>  <span class="c1"># Shape: (1,); Element type: int64</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">dataset</span> <span class="o">=</span> <span class="n">SimpleDataset</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Shape: (10,), (1,)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>You can run the script to check how the dataset works:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">python simple_dataset.py
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="4-dataloader">4. Dataloader</h2>
<p>As long as the dataset is built, creating a dataloader is quite easy.</p>
<p>A dataloader will provide <code>batch_size</code> samples in each iteration. For example:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># @file temp.py</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">simple_dataset</span> <span class="kn">import</span> <span class="n">SimpleDataset</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">dataset</span> <span class="o">=</span> <span class="n">SimpleDataset</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Get a sample, shape: (10,), (1,)</span>
</span></span><span class="line"><span class="cl"><span class="n">sample_x</span><span class="p">,</span> <span class="n">sample_y</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Suppose batch_size is 16, the dataloader will provide 16 samples in each iteration</span>
</span></span><span class="line"><span class="cl"><span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Shape: (16, 10), (16, 1)</span>
</span></span><span class="line"><span class="cl">    <span class="k">break</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>You can run the script to check how the dataloader works:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">python temp.py
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="5-loss-function">5. Loss Function</h2>
<p>Different tasks require different loss functions. For example, a 2-class classification task can use <code>nn.CrossEntropyLoss</code>, while a regression task can use <code>nn.MSELoss</code>.</p>
<p>In our case, we will use <code>nn.CrossEntropyLoss</code>.</p>
<h2 id="6-optimizer">6. Optimizer</h2>
<p>We will use <code>torch.optim.SGD</code> as the optimizer. <code>torch.optim.Adam</code> is also a good choice. This is a hyperparameter that you can tune.</p>
<h2 id="7-trainpipeline">7. Trainpipeline</h2>
<p>Now we can build the trainpipeline. The trainpipeline will train the model on the dataset.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># @file trainpipeline.py</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># This is the model we built</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">simple_model</span> <span class="kn">import</span> <span class="n">SimpleModel</span>
</span></span><span class="line"><span class="cl"><span class="c1"># This is the dataset we built</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">simple_dataset</span> <span class="kn">import</span> <span class="n">SimpleDataset</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">DEVICE</span> <span class="o">=</span> <span class="s2">&#34;cuda&#34;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&#34;cpu&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">16</span>
</span></span><span class="line"><span class="cl"><span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">100</span>
</span></span><span class="line"><span class="cl"><span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.01</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Create a model and move it to DEVICE</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Create train dataset and dataloader</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">SimpleDataset</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">val_dataset</span> <span class="o">=</span> <span class="n">SimpleDataset</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">val_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Create a loss function and an optimizer; The optimizer will update the model&#39;s parameters</span>
</span></span><span class="line"><span class="cl">    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># Set the model to training mode</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># Set the model to evaluation mode</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>  <span class="c1"># Disable gradient calculation</span>
</span></span><span class="line"><span class="cl">            <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="n">total_correct</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="n">total_samples</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">val_dataloader</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">                <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="n">total_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="n">total_samples</span> <span class="o">+=</span> <span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">total_loss</span> <span class="o">/</span> <span class="n">total_samples</span><span class="si">}</span><span class="s2">, Accuracy: </span><span class="si">{</span><span class="n">total_correct</span> <span class="o">/</span> <span class="n">total_samples</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">train</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>You can run the script to check how the trainpipeline works:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">python trainpipeline.py
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded></item></channel></rss>