<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Vllm on ç§‹æ°´Â·JamesNULLiu</title><link>https://jamesnulliu.github.io/tags/vllm/</link><description>Recent content in Vllm on ç§‹æ°´Â·JamesNULLiu</description><generator>Hugo -- 0.148.2</generator><language>en</language><copyright>2024-2025 JamesNULLiu</copyright><lastBuildDate>Fri, 12 Sep 2025 22:14:12 +0000</lastBuildDate><atom:link href="https://jamesnulliu.github.io/tags/vllm/index.xml" rel="self" type="application/rss+xml"/><item><title>Arithmetic Intensity Estimation of Large Language Models</title><link>https://jamesnulliu.github.io/blogs/arithmetic-intensity-estimation-of-large-language-models/</link><pubDate>Thu, 13 Mar 2025 17:38:00 +0800</pubDate><guid>https://jamesnulliu.github.io/blogs/arithmetic-intensity-estimation-of-large-language-models/</guid><description>This blog post discusses the arithmetic intensity of large language models and how it affects the performance of these models.</description><content:encoded><![CDATA[<h2 id="1-estimating-total-flops">1. Estimating Total FLOPs</h2>
<p>We only consider the FLOPs of Transformer layers, excluding the embedding layer and the output layer.</p>
<ul>
<li>
<p><strong>Attention</strong>:</p>
<ol>
<li>Each projection for Q, K and V is matmul of input <code>(B, S, H)</code> and weight <code>(H, H)</code>, yielding <code>(B, S, H)</code>:<br>
$$
    \text{FLOPs} = 3 \times (2 \times B \times S \times H \times H) = 6 \times B \times S \times H^2
    $$</li>
<li>$S = QK^T$, matmul of $Q$ <code>(B, S, H)</code> and $K^T$ <code>(B, H, S)</code>, yielding <code>(B, S, S)</code>:<br>
$$
    \text{FLOPs} = 2 \times B \times S \times S \times H = 2 \times B \times S^2 \times H
    $$</li>
<li>$L = S \cdot V$, matmul of $S$ <code>(B, S, S)</code> and $V$ <code>(B, S, H)</code>, yielding <code>(B, S, H)</code>:<br>
$$
    \text{FLOPs} = 2 \times B \times S \times H \times S
    $$</li>
<li>$O = L \cdot W_O$, matmul of $L$ <code>(B, S, H)</code> and $W_O$ <code>(H, H)</code>, yielding <code>(B, S, H)</code>:<br>
$$
    \text{FLOPs} = 2 \times B \times S \times H^2
    $$</li>
<li>Total attention FLOPs per Transformer layer:
$$
    \text{FLOPs} = 8 \times B \times S \times H^2 + 4 \times B \times S^2 \times H
    $$</li>
</ol>
</li>
<li>
<p><strong>Feed-Forward Networek</strong><br>
Typically 2 linear layers, one mapping <code>(B, S, H)</code> to <code>(B, S, 4H)</code> and the other mapping <code>(B, S, 4H)</code> to <code>(B, S, H)</code>:</p>
<ul>
<li>
<p>Total FFN FLOPs per Transformer layer:</p>
$$
    \begin{align*}
    \text{FLOPs} &= 2 \times B \times S \times H \times (4 \times H) + 2 \times B \times S \times (4 \times H) \times H \\
                 &= 16 \times B \times S \times H^2
    \end{align*}
    $$</li>
</ul>
</li>
<li>
<p><strong>Total FLOPs: $N$ Layers of Transformer</strong><br>
Each Transformer layer consists of an attention mechanism and a feed-forward network</p>
<ul>
<li>
<p>When prefilling, the total FLOPs is:</p>
$$
    \text{FLOPs}_\text{total} = N (24 B S H^2 + 4 B S^2 H)
    $$</li>
<li>
<p>When decoding, suppose the input is of shape <code>(B, Si, H)</code> and KV cache is of shape <code>(B, Sc, H)</code>, the total FLOPs is:</p>
$$
    \text{FLOPs}_\text{total} = N (24 B S_i H^2 + 4 B S_i S_c H)
    $$</li>
</ul>
</li>
</ul>
<h2 id="2-estimating-total-bytes-transfered">2. Estimating Total Bytes Transfered</h2>
<p>In FP16, each parameter or activation element is 2 bytes.</p>
<p>Data transferred includes <strong>loading model weights</strong> and <strong>handling activations</strong>.</p>
<p>Suppose we have a $Z$-B-fp16 model and $N$ Transformer layers, each with input size <code>(B, S, H)</code>.</p>
<ul>
<li>
<p><strong>Model Weights</strong><br>
A $Z$-B-fp16 model has $Z \times 10^9$ <code>fp16</code> parameters, each 2 bytes:</p>
$$
  \text{Bytes}_\text{weights} = Z \times 10^9 \times 2 ~ \text{Bytes} = 2 \times Z ~ \text{GBytes}
  $$<p>In an optimized GPU inference, weights are typically loaded into high-bandwidth memory (HBM) once and reused, so we assume $2Z$ GB is read once per forward pass.</p>
</li>
<li>
<p><strong>Activations</strong></p>
<ul>
<li>For each Transfomer layer, input and output activations are of shape <code>(B, S, H)</code>, and each element is 2 bytes in <code>fp16</code>:
$$
      \text{Bytes}_\text{act-layer} = B \times S \times H \times 2 ~ \text{Bytes}
      $$</li>
<li>For $N$ layers, activations are computed sequentially. Since each layerâ€™s output becomes the next layerâ€™s input (read once, written once):<br>
$$
    \begin{align*}
    \text{Bytes}_\text{act-total} &= 2 \times N \times  \text{Bytes}_\text{act-layer} ~ \text{Bytes} \\
                                   &= 4 \times N \times B \times S \times H ~ \text{Bytes}
    \end{align*}
    $$</li>
</ul>
</li>
<li>
<p><strong>KV Caches</strong><br>
When decoding, each Transformer layer would load cached K and V both of shape <code>(B, Sc, H)</code>. After decoding, the new K and V of shape <code>(B, Si, H)</code> are computed and cached for the next layer. So the bytes transfered for one forward pass is:</p>
$$
  \text{Bytes}_\text{KV} = N \times (B \times S_c \times H + 2 \times B \times S_i \times H) \times 2 ~ \text{Bytes}
  $$</li>
<li>
<p><strong>Total Data Transferred</strong></p>
<ul>
<li>
<p>When prefilling, the total bytes transferred is:</p>
$$
    \begin{align*}
    \text{Bytes}_\text{total} &= \text{Bytes}_\text{weights} + \text{Bytes}_\text{act-total} \\
                              &= 2 Z \text{e}^9 + 4 N B S H ~ \text{Bytes}
    \end{align*}
    $$</li>
<li>
<p>When decoding, suppose cached sequence length is $S_c$ and the input sequence length is $S_i$, the total bytes transferred is:</p>
$$
    \begin{align*}
    \text{Bytes}_\text{total} &= \text{Bytes}_\text{weights} + \text{Bytes}_\text{act-total} + \text{Bytes}_\text{KV} \\
                              &= 2 Z \text{e}^{9} + 8 N B S_i H + 2 N B S_c H ~ \text{Bytes}
    \end{align*}
    $$</li>
</ul>
</li>
</ul>
<h2 id="3-arithmetic-intensity">3. Arithmetic Intensity</h2>
<p>When prefilling, there is no cached K and V, so the arithmetic intensity is:</p>
$$
\begin{align*}
\text{Arithmetic Intensity} &= \text{FLOPs}_\text{total} / \text{Bytes}_\text{total} \\
                            &= \frac{N (24 B S H^2 + 4 B S^2 H)}{2 Z 10^9 + 4 N B S H}
\end{align*}
$$<p>When decoding, suppose cached sequence length is $S_c$ and the input sequence length is $S_i$ , then the arithmetic intensity is:</p>
$$
\begin{align*}
\text{Arithmetic Intensity} &= \text{FLOPs}_\text{total} / \text{Bytes}_\text{total} \\
                            &= \frac{N (24 B S_i H^2 + 4 B S_i S_c H)}{2 Z 10^9 + 8 N B S_i H + 2 N B S_c H}
\end{align*}
$$<h2 id="4-roofline-model">4. Roofline Model</h2>
<div class="image-container">
    <img src="/imgs/blogs/arithmetic-intensity-estimation-of-large-language-models/roofline_model.png" 
        alt="" 
        class="image" 
        width="80%"/>
    <div class="image-caption">
        Roofline Model. If the arithmetic intensity is on the right side of the machine balance, the performance compute-bound. If it is on the left side, the performance is memory-bound.
    </div>
</div>
<p>A100-80GB has the following hardware `specifications:</p>
<ul>
<li><strong>Peak FLOPs</strong> ($\pi$): $312 \times 10^{12}$ FLOPs/s</li>
<li><strong>Memory Bandwidth</strong> ($\beta$): $2039 \times 10^9$ B/s</li>
<li><strong>Machine Balance</strong> ($I_{max}$): $312 \times 10^{12} / (2039 \times 10^9) \approx 153$ FLOPs/Byte</li>
</ul>
<p>Here are two examples of arithmetic intensity estimation:</p>
<ul>
<li>See: <a href="https://www.geogebra.org/calculator/uqzhngtf" target="_blank" rel="noopener noreferrer">
    Arithmetic Intensity for Prefilling
</a>
</li>
<li>See: <a href="https://www.geogebra.org/calculator/tkkekjdb" target="_blank" rel="noopener noreferrer">
    Arithmetic Intensity for Speculative Decoding
</a>
</li>
</ul>
<h2 id="5-discussion-tensor-parallelism">5. Discussion: Tensor Parallelism</h2>
<p>If the model is split across multiple GPUs using TP, the hidden size <code>H</code> and the model weight is divided by the number of GPUs.</p>
]]></content:encoded></item><item><title>A Brief Talk on Speculative Decoding</title><link>https://jamesnulliu.github.io/blogs/a-brief-talk-on-speculative-decoding/</link><pubDate>Fri, 21 Feb 2025 01:14:06 +0800</pubDate><guid>https://jamesnulliu.github.io/blogs/a-brief-talk-on-speculative-decoding/</guid><description>A brief talk on speculative decoding in large language models.</description><content:encoded><![CDATA[<iframe src="https://docs.google.com/spreadsheets/d/e/2PACX-1vS8hnf4AA8xVJCNKTACvm4H_Lnu6kXtfB7tdL4Iv90OcsuXBnMs87XaVll4Dz0XhmXbjvbjKIeu8k3r/pubhtml" frameborder="0" width="100%" height="400"></iframe>
<h2 id="1-introduction-to-speculative-decoding">1. Introduction to Speculative Decoding</h2>
<p>Given a score model <code>S</code> (for example, LLAMA-3-70B) and a draft model <code>D</code> (for example, LLAMA-3-7B), the process of <strong>speculative decoding</strong> can be described as follows:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">input_ids</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># (seq_len,)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># D generates tokens[seq_len, ..., seq_len + k]</span>
</span></span><span class="line"><span class="cl">    <span class="n">draft_outputs</span> <span class="o">=</span> <span class="n">D</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>  <span class="c1"># (k,)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Given tokens[seq_len - 1, ..., seq_len + k], S generates real </span>
</span></span><span class="line"><span class="cl">    <span class="c1"># prediction for tokens[seq_len, ..., seq_len + k, seq_len + k + 1] with</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># one forward pass.</span>
</span></span><span class="line"><span class="cl">    <span class="n">score_outputs</span> <span class="o">=</span> <span class="n">S</span><span class="p">(</span><span class="n">cat</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">draft_outputs</span><span class="p">))</span>  <span class="c1"># (k + 1,)</span>
</span></span><span class="line"><span class="cl">    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">verify</span><span class="p">(</span><span class="n">draft_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">score_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">            <span class="k">break</span>
</span></span><span class="line"><span class="cl">        <span class="n">input_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">draft_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">input_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> 
</span></span></code></pre></td></tr></table>
</div>
</div><div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/sd-workflow-in-vllm-k-3-p-1.png" 
        alt="" 
        class="image" 
        width="85%"/>
    <div class="image-caption">
        Speculative decoding workflow in vLLM (k=3, top-p=1). k=3 indicates that the draft model generates 3 tokens per forward pass, and top-p=1 means that for each token, only 1 candidate is proposed. As shown in the picture, at prefill statge, input sequence would first be fed into both draft and score models to acquire kv caches. The output of draft model at this stage is omitted. Then, T5 is fed into draft model to generate proposed T6&#39;, T7&#39;, and T8&#39;. To verify these tokens, T5, T6&#39;, T7&#39; and T8&#39; are fed into the score model to get T6, T7*, T8* and T9* in one forward pass. Note that here T6 must be correct because it is generated by T5 through the score model; However, T7*, T8* and T9* are not guaranteed to be correct. The final step is to verify T6&#39;, T7&#39; and T8&#39; to see if T7*, T8* and T9* are correct. For example, if T6&#39; and T7&#39; is correct, then the final accepted tokens would be T6&#39;, T7&#39; and T8&#39;, which means the socore model generates 3 tokens in one forward pass.
    </div>
</div>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/sd-workflow-in-vllm-k-1-p-1.png" 
        alt="" 
        class="image" 
        width="80%"/>
    <div class="image-caption">
        Workflow of spuculative decoing in vLLM (k=1, top-p=1). Like the previous picture, if T6&#39; is correct, then the final accepted tokens would be T6&#39; and T7*, one generated by the draft model and the other by the score model. The score model generates 2 tokens in one forward pass.
    </div>
</div>
<h2 id="2-how-speculative-decoding-works-in-vllm">2. How Speculative Decoding Works in vLLM</h2>
<p>In vLLM, speculative decoding is integrated with the system&rsquo;s continuous batching architecture, where different requests are processed together in a single batch, enabling higher throughput. vLLM uses two key components to implement this:</p>
<ul>
<li><strong>Draft Runner</strong>: This runner is responsible for executing <strong>the smaller proposer model</strong> to propose candidate tokens.</li>
<li><strong>Target Runner</strong>: The target runner verifies the tokens by running <strong>the larger scorer model</strong>.</li>
</ul>
<p>vLLM&rsquo;s system is optimized to handle this process efficiently, allowing speculative decoding to work seamlessly with continuous batching, which increases the overall system performance.</p>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/sd-in-vllm.png" 
        alt="" 
        class="image" 
        width=""/>
    <div class="image-caption">
        Diagram illustrating how the draft and target runners interact within the vLLM batching system.
    </div>
</div>
<p>To implement speculative decoding in vLLM, two crucial components had to be modified:</p>
<ul>
<li><strong>Scheduler</strong>: The scheduler was adjusted to handle multiple token slots within a single forward pass, enabling the simultaneous generation and verification of several tokens.</li>
<li><strong>Memory Manager</strong>: The memory manager now handles the KV cache for both the draft and scorer models, ensuring smooth processing during speculative decoding.</li>
</ul>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/vllm-sd-system-archi.png" 
        alt="" 
        class="image" 
        width="80%"/>
    <div class="image-caption">
        System architecture of speculative decoding in vLLM. 
    </div>
</div>
<h2 id="3-types-of-speculative-decoding-supported-in-vllm">3. Types of Speculative Decoding Supported in vLLM</h2>
<h3 id="31-draft-model-based-speculative-decoding">3.1. Draft Model-Based Speculative Decoding</h3>
<p>This is the most commonly used form of speculative decoding, where a smaller model predicts the next tokens, and a larger model verifies them. A common example would be using a Llama 68M model to predict tokens for a Llama 2 70B model. This approach requires careful selection of the draft model to balance accuracy and overhead.</p>
<p>Choosing the correct draft model is essential for maximizing the efficiency of speculative decoding. The draft model needs to be small enough to avoid creating significant overhead but still accurate enough to provide a meaningful performance boost.</p>
<p>However, <strong>selecting the right draft model</strong> can be challenging. For example, in models like Llama 3, finding a suitable draft model is difficult due to differences in vocabulary size. Speculative decoding requires that the draft and target models <strong>share the same vocabulary</strong>, and in some cases, this can limit the use of speculative decoding. Therefore, in the following sections, we introduce several draft-model free speculative decoding methods.</p>
<h3 id="32-prompt-lookup-decoding">3.2. Prompt Lookup Decoding</h3>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/prompt-lookup-decoding.png" 
        alt="" 
        class="image" 
        width="80%"/>
    <div class="image-caption">
        An example of prompt lookup decoding. Given the prompt, we build all 2-grams as the lookup key. The values are the three tokens following the lookup key. During generation, we will check if the current 2-gram matches any key. If so, we will propose the following tokens with the value.
    </div>
</div>
<p>Otherwise known as n-gram matching, this approach is effective for use cases like summarization and question-answering, where there is a significant overlap between the prompt and the answer. Instead of using a small model to propose tokens, the system speculates based on the information already available in the prompt. This works particularly well when the large model repeats parts of the prompt in its answers.</p>
<h2 id="4-medusa">4. MEDUSA</h2>
<h3 id="41-roadmap">4.1. Roadmap</h3>
<ol>
<li><a href="https://github.com/vllm-project/vllm/issues/1023" target="_blank" rel="noopener noreferrer">
    [vllm][ISSUE] | Can vLLM support medusa head? #1023
</a>
</li>
<li><a href="https://github.com/vllm-project/vllm/issues/1171" target="_blank" rel="noopener noreferrer">
    [vllm][ISSUE] | [Discussion] Will vLLM consider using Speculative Sampling to accelerating LLM decoding? #1171
</a>
</li>
<li><a href="https://github.com/vllm-project/vllm/pull/4978" target="_blank" rel="noopener noreferrer">
    [vllm][PR] | [Speculative Decoding] Medusa Implementation with Top-1 proposer #4978
</a>
</li>
</ol>
<h3 id="41-medusa-heads">4.1. <strong>MEDUSA Heads</strong></h3>
<p>MEDUSA heads are additional decoding heads appended to the last hidden states of the original model.</p>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/medusa.png" 
        alt="" 
        class="image" 
        width="70%"/>
    <div class="image-caption">
        Three heads are used to propose tokens for the following three positions. Head 1 is proposing [&#34;is&#34;, &#34;\&#39;&#34;, &#34;the&#34;] for the first position. Head 2 is proposing [&#34;difficult&#34;, &#34;is&#34;, &#34;\&#39;&#34;] for the second position. Head 3 is proposing [&#34;not&#34;, &#34;difficult&#34;, &#34;a&#34;] for the third position. NOTE: All heads take the output of the last transformer block as the input.
    </div>
</div>
<p>Specifically, given the original modelâ€™s last hidden states $h_t$ at position $t$, we add $K$ decoding heads to $h_t$. The $k$-th head is used to predict the token in the $(t + k + 1)$-th position of the next tokens (the original language model head is used to predict the $(t + 1)$-th position).</p>
$$
\begin{aligned}
p_{t}^{(k)} & =\mathrm{softmax}\left(W_{2}^{(k)}\cdot\left(\mathrm{SiLU}(W_{1}^{(k)}\cdot h_{t})+h_{t}\right)\right), \\
 & \mathrm{where~}W_{2}^{(k)}\in\mathbb{R}^{d\times V},W_{1}^{(k)}\in\mathbb{R}^{d\times d}.
\end{aligned}
$$<p>Unlike a draft model, MEDUSA heads are trained in conjunction with the original backbone model, which can remain frozen during training (MEDUSA-1) or be trained together (MEDUSA-2).</p>
<h3 id="42-tree-attention">4.2. <strong>Tree Attention</strong></h3>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/tree-attn.png" 
        alt="" 
        class="image" 
        width="70%"/>
    <div class="image-caption">
        
    </div>
</div>
<p>The top-2 predictions from the first MEDUSA head and the top-3 from the second result in a total of $2 \times 3 = 6$ candidates. Each of these candidates corresponds to a distinct branch within the tree structure.</p>
<p>To guarantee that each token only accesses its predecessors, an attention mask is devised that exclusively permits attention flow from the current token back to its antecedent tokens.</p>
<h2 id="5-eagle">5. EAGLE</h2>
<h3 id="51-roadmap">5.1. Roadmap</h3>
<ol>
<li><a href="https://github.com/vllm-project/vllm/pull/6830" target="_blank" rel="noopener noreferrer">
    [vllm][PR] |  [Speculative Decoding] EAGLE Implementation with Top-1 proposer #6830
</a>
</li>
</ol>
<h3 id="52-detailed-process">5.2. Detailed Process</h3>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/eagle-compare.png" 
        alt="" 
        class="image" 
        width="80%"/>
    <div class="image-caption">
        A comparison of the methods for drafting the fourth and fifth tokens, t4 and t5. t (represented by blue blocks) denotes tokens, and f (orange blocks) signifies the features, with subscripts indicating their positions in the sequence.  The red border indicates the predictions of the draft model. For simplicity, the n in the n-gram for Lookahead, as shown in the figure, has been set to 2.
    </div>
</div>
<p>This link is a Feishu drawboard to show the detailed process of speculative decoding with EAGLE in vLLM:</p>
<ul>
<li><a href="https://ncnqdau83tum.feishu.cn/docx/PliBdWWPWohaClxAagjcZqcZnMe?from=from_copylink" target="_blank" rel="noopener noreferrer">
    Speculative Decoding with EAGLE in vLLM
</a>
</li>
</ul>
<h2 id="6-deepseekmtp">6. DeepseekMTP</h2>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/deepseekmtp-structure.png" 
        alt="" 
        class="image" 
        width="90%"/>
    <div class="image-caption">
        Structure of DeepseekMTP. This figure also demonstrates the training process of draft models, which are fed with continuous tokens and corresponding masks to predict the next tokens for each position. This process is similar to the pre-training process of the larger scorer model.
    </div>
</div>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/deepseekmtp-compute-graph.png" 
        alt="" 
        class="image" 
        width="100%"/>
    <div class="image-caption">
        Compute graph of DeepseekMTP.
    </div>
</div>
<h2 id="7-discussion">7. Discussion</h2>
<h3 id="71-performance-insights-speedups-and-trade-offs">7.1. Performance Insights, Speedups, and Trade-offs</h3>
<blockquote>
<p>Ref: <a href="https://blog.vllm.ai/2024/10/17/spec-decode.html#speculative-decoding-performance-insights-speedups-and-trade-offs" target="_blank" rel="noopener noreferrer">
    [vllm] | How Speculative Decoding Boosts vLLM Performance by up to 2.8x
</a>
</p></blockquote>
<p>Speculative decoding offers significant performance benefits in <strong>low-QPS (queries per second)</strong> environments. For example, in testing on the ShareGPT dataset, vLLM demonstrated up to a 1.5x speedup in token generation when using draft model-based speculative decoding. Similarly, prompt lookup decoding has shown speedups of up to 2.8x when applied to summarization datasets, such as CNN/DailyMail.</p>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/sd-performance-low-qps.png" 
        alt="" 
        class="image" 
        width="75%"/>
    <div class="image-caption">
        Performance comparison showing spec decode delivering up to 1.5x Speedup at QPS=1 Llama3-70B on ShareGPT with 4xH100 using draft model (turboderp/Qwama-0.5B-Instruct) and up to 2.8x Speedup at QPS=1 Llama3-70B on CNN Dailymail with 4xH100 using n-grams.
    </div>
</div>
<p>However, in <strong>high-QPS environments</strong>, speculative decoding may introduce performance trade-offs. The extra compute required to propose and verify tokens can sometimes slow down the system when it is already compute-bound, as seen when the number of requests per second increases. In such cases, the overhead of speculative decoding can outweigh its benefits, leading to reduced performance.</p>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/sd-performance-high-qps.png" 
        alt="" 
        class="image" 
        width=""/>
    <div class="image-caption">
        As high QPS, we see 1.4x slowdown Llama3-70B on ShareGPT with 4xH100, 1.8x slowdown Llama3-70B on CNN Dailymail with 4xH100
    </div>
</div>
<h3 id="72-why-exactly-is-batch-expansion-inefficient">7.2. Why exactly is batch expansion inefficient?</h3>
<blockquote>
<p>Ref: <a href="https://docs.google.com/document/d/1T-JaS2T1NRfdP51qzqpyakoCXxSXTtORppiwaj5asxA/edit?tab=t.0#heading=h.71imqkdaug8g" target="_blank" rel="noopener noreferrer">
    Optimizing attention for spec decode can reduce latency / increase throughput
</a>
</p></blockquote>
<p>Looking at Llama2 architecture, each component has the following algorithmic complexity wrt speculative tokens and sequence length. The baseline is non-speculative decoding, so factors such as d_model are ignored as they are the same in either case.</p>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/llama2-sd-complexity.png" 
        alt="" 
        class="image" 
        width=""/>
    <div class="image-caption">
        
    </div>
</div>
<p>Each of these scales linearly with number of speculative tokens, except for attention, which scales by <code>num_spec_tokens * seq_len</code>. This means that for large batch sizes and/or large speculative trees and/or large sequence lengths, attention will be the computational bottleneck.</p>
<p>To optimize the attention operation, the key is that components of the attention operation are duplicated when scoring different speculative tokens given the same prefix sequence:</p>
<div class="image-container">
    <img src="/imgs/blogs/a-brief-talk-on-speculative-decoding/sd-attn-opt.png" 
        alt="" 
        class="image" 
        width=""/>
    <div class="image-caption">
        
    </div>
</div>
<p>Speaking theoretically, we can optimize attention for speculative scoring by reducing redundant <code>QK^T</code> computations + loads and <code>Softmax(...)V</code> loads:</p>
<ul>
<li>Share K loads for common tokens</li>
<li>Share K*Q compute for common tokens</li>
<li>Share V loads for common tokens</li>
</ul>
<p>We should experimentally verify this analysis: one weakness is that <code>Softmax(...)V</code> computation is still <code>O(num_spec_tokens * seq_len)</code>.</p>
<h2 id="references">References</h2>
<ol>
<li><a href="https://docs.vllm.ai/en/latest/features/spec_decode.html" target="_blank" rel="noopener noreferrer">
    [vllm] | Speculative Decoding
</a>
</li>
<li><a href="https://blog.vllm.ai/2024/10/17/spec-decode.html" target="_blank" rel="noopener noreferrer">
    [vllm] | How Speculative Decoding Boosts vLLM Performance by up to 2.8x
</a>
</li>
<li><a href="https://blog.vllm.ai/2024/10/17/spec-decode.html#how-to-use-speculative-decoding-in-vllm" target="_blank" rel="noopener noreferrer">
    [vllm] | How to Use Speculative Decoding in vLLM
</a>
.</li>
<li><a href="https://github.com/vllm-project/vllm/pull/4978" target="_blank" rel="noopener noreferrer">
    [vllm][PR] | [Speculative Decoding] Medusa Implementation with Top-1 proposer #4978
</a>
</li>
<li><a href="https://pytorch.org/blog/hitchhikers-guide-speculative-decoding" target="_blank" rel="noopener noreferrer">
    A Hitchhiker&#39;s Guide to Speculative Decoding
</a>
</li>
<li><a href="https://docs.google.com/document/d/1Z9TvqzzBPnh5WHcRwjvK2UEeFeq5zMZb5mFE8jR0HCs/edit?tab=t.0#heading=h.1fjfb0donq5a" target="_blank" rel="noopener noreferrer">
    [vllm] | What is lookahead scheduling in vLLM?
</a>
</li>
<li><a href="https://docs.google.com/document/d/1T-JaS2T1NRfdP51qzqpyakoCXxSXTtORppiwaj5asxA/edit?tab=t.0#heading=h.kk7dq05lc6q8" target="_blank" rel="noopener noreferrer">
    Optimizing attention for spec decode can reduce latency / increase throughput
</a>
</li>
<li><a href="https://github.com/vllm-project/vllm/issues/4565" target="_blank" rel="noopener noreferrer">
    [vllm][ISSUE] | [RFC]: Automate Speculative Decoding #4565
</a>
</li>
<li><a href="https://huggingface.co/blog/dynamic_speculation_lookahead" target="_blank" rel="noopener noreferrer">
    [HF] | Faster Assisted Generation with Dynamic Speculation
</a>
</li>
</ol>
]]></content:encoded></item><item><title>Dive into Paged Attention</title><link>https://jamesnulliu.github.io/blogs/dive-into-paged-attention/</link><pubDate>Mon, 07 Oct 2024 12:00:00 +0800</pubDate><guid>https://jamesnulliu.github.io/blogs/dive-into-paged-attention/</guid><description>Dive into the paged attention mechanism of vLLM.</description><content:encoded><![CDATA[<h2 id="1-why-attentions--only-depends-on">1. Why Attention&rsquo;s $O_i$ only depends on $Q_i$</h2>
<p>The Attention formula is:</p>
$$
O=Attention(Q,K,V)=softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$<p>Assume $Q=\begin{bmatrix}Q_0\\Q_1\end{bmatrix}$, $K=\begin{bmatrix}K_0\\K_1\end{bmatrix}$</p>
<p>Then:</p>
$$
O=softmax(\frac{\begin{bmatrix}Q_0K_0^T&Q_0K_1^T\\Q_1K_0^T&Q_1K_1^T\end{bmatrix}}{\sqrt{d_k}})V
$$<p>Let:</p>
$$
A=\begin{bmatrix}A_0\\A_1\end{bmatrix}=\begin{bmatrix}Q_0K_0^T&Q_0K_1^T\\Q_1K_0^T&Q_1K_1^T\end{bmatrix},f(x)=\frac{softmax(x)}{\sqrt{d_k}}
$$<p>At this point, $A_1$ only depends on $Q_1$ and is independent of $Q_0$, so:</p>
$$
\begin{bmatrix}O_0\\O_1\end{bmatrix}=O=\begin{bmatrix}f(A_0)\\f(A_1)\end{bmatrix}V=\begin{bmatrix}f(A_0)V\\f(A_1)V\end{bmatrix}
$$<p>Therefore, $O_i$ only depends on $A_i$, and according to the definition of $A$, $A_i$ only depends on $Q_i$, meaning:</p>
<p>The $i$-th output of the Attention matrix only depends on the $i$-th $Q$ and is independent of previous $Q$s.</p>
<p><strong>Summary</strong>:</p>
<ul>
<li>When predicting the next token, we only need to calculate the corresponding <code>Q_new</code> for the new token and perform attention calculation with the previously cached <code>K_cache</code> and <code>V_cache</code>.</li>
<li>The new <code>K_new</code> and <code>V_new</code> will be added to the cache to provide the foundation for the next token generation.</li>
<li>This process avoids repeated calculations for all historical tokens, greatly improving efficiency.</li>
</ul>
<h2 id="2-kv-cache-incremental-process">2. KV Cache Incremental Process</h2>
<p>Example code:</p>
<p>&ldquo;<a href="https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/cf690614d004aa647aefccb8db3eac83255cb99e/src/pmpp/models/attention.py">Learning-Programming-Massively-Parallel-Processors/src/pmpp/models/attention.py</a>&rdquo;</p>
<h3 id="21-prefilling-initial-input-complete-sequence-calculation">2.1. Prefilling: Initial Input (Complete Sequence) Calculation</h3>
<ul>
<li>For the initial input sequence <code>(seq_len, vocab_size)</code>, we obtain <code>Q</code>, <code>K</code>, and <code>V</code> through linear transformations, all with shape <code>(seq_len, embed_dim)</code> (<em>see <a href="">this</a></em>).</li>
<li>Using <code>Q</code> and <code>K</code> to calculate attention scores through dot product, then combining with <code>V</code> to compute the output <code>(seq_len, embed_dim)</code> (<em>see <a href="">this</a></em>), this is the first complete calculation for the initial sequence.</li>
</ul>
<h3 id="22-decoding-incremental-calculation-when-predicting-next-token">2.2. Decoding: Incremental Calculation When Predicting Next Token:</h3>
<p>When predicting the next token, there&rsquo;s no need to perform complete <code>Q</code>, <code>K</code>, <code>V</code> calculations for the entire sequence. Instead, only an incremental calculation for the newly generated token is required. The process is as follows:</p>
<ol>
<li><strong>Input New Token</strong>: Take the generated token from last round as input sequence, obtain <code>Q_new</code>, <code>K_new</code> and <code>V_new</code> through linear transformation.</li>
<li><strong>Update KV Cache</strong>: <code>K_new</code> and <code>V_new</code> are added to the end of <code>K_cache</code> and <code>V_cache</code>, making them a pair of <code>(kv_len + 1, embed_dim)</code> vectors.</li>
<li><strong>Attention Calculation with Updated <code>K_cache</code> and <code>V_cache</code></strong>: Use <code>Q_new</code> to perform attention calculation with updated <code>K_cache</code> and <code>V_cache</code>. <code>Q_new</code> can directly perform dot product with <code>K_cache</code> to get attention scores, then combine with <code>V_cache</code> to get new output.</li>
<li><strong>Output</strong>: The output after attention calculation has shape <code>(1, embed_dim)</code>, which is the newly generated token.</li>
</ol>
<h2 id="3-paged-attention-in-vllm">3. Paged Attention in vllm</h2>
<h3 id="31-motivation-memory-wastes">3.1. Motivation: Memory Wastes</h3>
<p><img alt="memory-wastes.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/memory-wastes.png"></p>
<p>The above figure shows possible memory waste scenarios. The main issue is that we don&rsquo;t know where the EOS (end of sequence) token is. Random memory allocation may lead to significant memory fragmentation, resulting in reduced throughput.</p>
<h3 id="32-solution-managing-caches-with-pages">3.2. Solution: Managing Caches with Pages</h3>
<p><img alt="paged-attention-animation.webp" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/paged-attention-animation.webp"></p>
<p>The above figure demonstrates how vLLM manages memory using Paged Attention.</p>
<p>In simple terms, before inference begins, vLLM allocates two long Tensors (<code>k_cache</code> and <code>v_cache</code>) for each Decoder Layer, dividing these Tensors into continuous equal-length PA blocks (each row in the figure represents one PA Block). Each PA Block can store K or V cache for <code>BLOCK_SIZE</code> tokens (each token&rsquo;s shape can be recognized as <code>(num_heads, head_size)</code>).</p>
<p>Therefore, the shapes of <code>k_cache</code> and <code>v_cache</code> can be recognized as <code>(num_blocks, block_size, num_heads, head_size)</code>.</p>
<p>For a continuous sequence, PA blocks are allocated before the prefilling stage, and during inference:</p>
<ul>
<li>When computing prompt attention, the input K and V are first stored in <code>k_cache</code> and <code>v_cache</code> according to PA blocks; then attention is calculated using the entire QKV.</li>
<li>When computing new tokens, Q and the block table are used to calculate attention during the decode phase; at this point, the memory access is to the PA blocks in <code>k_cache</code> and <code>v_cache</code>.</li>
</ul>
<h2 id="5-paged-attention-kernel-in-details">5. Paged Attention Kernel in Details</h2>
<blockquote>
<p>References:</p>
<ul>
<li><a href="https://docs.vllm.ai/en/latest/dev/kernel/paged_attention.html">vLLM Paged Attention</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/673284781">vLLMçš‡å† ä¸Šçš„æ˜ç ï¼šæ·±å…¥æµ…å‡ºç†è§£PagedAttention CUDAå®ç°</a></li>
</ul></blockquote>
<p>The general structure of the Paged Attention kernel is as follows:</p>
<p><img alt="pa-cal.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-cal.png"></p>
<h3 id="51-è¾“å…¥è¾“å‡ºè¾“å‡ºåˆ†æå’Œå‚æ•°è¯´æ˜">5.1. è¾“å…¥è¾“å‡ºè¾“å‡ºåˆ†æå’Œå‚æ•°è¯´æ˜</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Grid: (num_heads, num_seqs, 1).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">template</span><span class="o">&lt;</span>
</span></span><span class="line"><span class="cl"><span class="k">typename</span> <span class="n">scalar_t</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">HEAD_SIZE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">NUM_THREADS</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">PARTITION_SIZE</span> <span class="o">=</span> <span class="mi">0</span><span class="o">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="n">__device__</span> <span class="kt">void</span> <span class="n">paged_attention_kernel</span><span class="p">(</span>
</span></span><span class="line"><span class="cl"><span class="p">...</span> <span class="c1">// Other side args.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="n">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">out</span><span class="p">,</span>       <span class="c1">// [num_seqs, num_heads, max_num_partitions, head_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="n">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">q</span><span class="p">,</span>         <span class="c1">// [num_seqs, num_heads, head_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="n">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">k_cache</span><span class="p">,</span>   <span class="c1">// [num_blocks, num_kv_heads, head_size/x, block_size, x]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="n">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">v_cache</span><span class="p">,</span>   <span class="c1">// [num_blocks, num_kv_heads, head_size, block_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">...</span> <span class="c1">// Other side args.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">)</span>
</span></span></code></pre></div><p>æ¨¡æ¿å‚æ•°è¯´æ˜:</p>
<ul>
<li><code>scalar_t</code> å…ƒç´ ç±»å‹ (å®é™…ä»£ç ä¸­è¿˜æœ‰ <code>cache_t</code> è¡¨ç¤º KV cache çš„å…ƒç´ ç±»å‹).</li>
<li><code>HEAD_SIZE</code> æ¯ä¸ª head ä¸­å…ƒç´ æ•°é‡.</li>
<li><code>BLOCK_SIZE</code> æ¯ä¸ª PA block ä¸­çš„ token æ•°é‡.
<blockquote>
<ol>
<li>KV cache è¢«å­˜å‚¨åœ¨ä¸åŒ PA blocks. æ¯ä¸ª PA block å­˜å‚¨ä¸€ä¸ª head ä¸­ <code>BLOCK_SIZE</code> ä¸ª token.<br>
ä¾‹å¦‚, è‹¥ <code>BLOCK_SIZE=16</code>, <code>HEAD_SIZE=128</code>, åˆ™ä¸€ä¸ª  PA block èƒ½å­˜å‚¨ä¸€ä¸ª head çš„ <code>16 * 128 = 2048</code> ä¸ªå…ƒç´ .</li>
<li>æ¯ä¸ª PA block å¯èƒ½åªåŒ…å«ä¸€éƒ¨åˆ†çš„ context tokens.</li>
<li>ä» page è§’åº¦çœ‹, KV cache æ˜¯è‹¥å¹²ä¸ª page çš„é›†åˆ;</li>
</ol></blockquote>
</li>
<li><code>NUM_THREADS</code> æ¯ä¸ª CUDA thread block ä¸­ thread çš„æ•°é‡.</li>
<li><code>PARTITION_SIZE</code> å‚ä¸ TP çš„ GPU æ•°é‡, é»˜è®¤ 0 è¡¨ç¤ºå•å¡. (ä»¥ä¸‹éƒ½ä»¥å•å¡ä¸ºä¾‹è¯´æ˜)</li>
</ul>
<p>é¢å¤–çš„ä¸€äº›å‚æ•°:</p>
<ul>
<li><code>num_seqs</code>: æœ¬æ¬¡æ¨ç†è¯·æ±‚ sequence æ•°ç›®.
<blockquote>
<p>ç”±äºè¿™ä¸ª kernel åªå¤„ç† decode é˜¶æ®µå• query attention, æ‰€ä»¥å®é™…ä¸Šæ¯ä¸ª sequence åªæœ‰ä¸€ä¸ª query token.</p></blockquote>
</li>
<li><code>num_heads</code>: Q çš„ head æ•°ç›®</li>
<li><code>num_kv_heads</code>: KV çš„ head æ•°ç›®, å¯¹äº MHA å…¶å€¼å’Œ <code>num_heads</code> ç›¸åŒ; å¦‚æœæ˜¯ GQA, MQA åˆ™ <code>num_kv_heads</code> å°äº <code>num_head</code>.</li>
<li><code>head_size</code>: å³ <code>HEAD_SIZE</code></li>
<li><code>k_cache: (num_blocks, num_kv_heads, head_size/x, block_size, x)</code>, å…¶ä¸­ <code>x</code> è¡¨ç¤º <code>THREAD_GROUP_SIZE * VEC_SIZE</code> çš„å¤§å° (åé¢ä¼šç»†è¯´).</li>
</ul>
<p>ä¸‹é¢ç»“åˆ GPU architecture åˆæ­¥åˆ†æä¸€ä¸‹å‚æ•°.</p>
<p><img alt="gpu-archi.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/gpu-archi.png"></p>
<p>ğŸ§ <strong>ä¸ºä»€ä¹ˆè¦åˆ† thread group?</strong></p>
<ul>
<li>å› ä¸ºå½“ä¸€ä¸ª cuda block è¦å–çš„æ•°æ®æ¯”è¾ƒå°‘çš„æ—¶å€™ (è®¡ç®— QK), ä¸€ä¸ª thread group åˆ†åˆ«ä¸€æ¬¡å– Q å’Œ K ä¸­ 16B; å½“ä¸€ä¸ª cuda block è¦å–çš„æ•°æ®æ¯”è¾ƒå¤šçš„æ—¶å€™ (è®¡ç®— LV), ä¸€ä¸ª thread å– 16B.</li>
</ul>
<h3 id="52shared-memory-q_vecs-çš„å†™å…¥">5.2.Shared Memory: <code>q_vecs</code> çš„å†™å…¥</h3>
<p>ä» kernel ä¸­çš„ç¬¬ä¸€ä¸ªç”³è¯·çš„ shared memory å¼€å§‹è¯´.</p>
<blockquote>
<p>å…³äº shared memeory:</p>
<ol>
<li>åœ¨ kernel ä¸­ç”³è¯·çš„ shared memory è¢«å½“å‰ cuda block ä¸­çš„æ‰€æœ‰ thread å…±äº«.</li>
<li>shared memory çš„ä½œç”¨æ˜¯ä¸ºäº†å‡å°‘ global memory çš„è®¿é—®æ¬¡æ•°ï¼Œæé«˜è®¿å­˜æ•ˆç‡.</li>
</ol></blockquote>
<p>ä»¥ä¸‹ä»£ç ç”³è¯·äº†ä¸€å— shared memroy è¢«æ•´ä¸ª CUDA Block ä¸­æ‰€æœ‰ kernel å…±äº«:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__shared__</span> <span class="n">Q_vec</span> <span class="n">q_vecs</span><span class="p">[</span><span class="n">THREAD_GROUP_SIZE</span><span class="p">][</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span></code></pre></div><p>é¦–å…ˆ, <code>q_vecs</code> è¦†ç›–äº† Q ä¸­ <code>head_size</code> ä¸ªå…ƒç´  - è¿™ä¹Ÿæ˜¯ä¸€ä¸ª cuda block éœ€è¦å¤„ç†çš„æ•°æ®é‡.</p>
<p>æ¥ç€å†è¯´ä¸¤ä¸ªç»´åº¦çš„å‚æ•°çš„æ„æ€:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_ELEMS_PER_THREAD</span> <span class="o">=</span> <span class="n">HEAD_SIZE</span> <span class="o">/</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_VECS_PER_THREAD</span> <span class="o">=</span> <span class="n">NUM_ELEMS_PER_THREAD</span> <span class="o">/</span> <span class="n">VEC_SIZE</span><span class="p">;</span>
</span></span></code></pre></div><ul>
<li><code>THREAD_GROUP_SIZE</code>: æ¯ä¸ª thread group ä¸­çš„ thread æ•°é‡. æ³¨æ„, ä¸€ä¸ª cuda block ä¸­æœ‰ <code>NUM_THREADS</code> ä¸ª thread, <code>NUM_THREAD_GROUPS</code> ä¸ª thread group. <code>THREAD_GROUP_SIZE = MAX(WARP_SIZE/BLOCK_SIZE, 1)</code>.</li>
<li><code>NUM_VECS_PER_THREAD</code>: <code>HEAD_SIZE</code> èƒ½è¢«åˆ†æˆå¤šå°‘ä¸ª 16B. (è¿™ä¸ªå˜é‡è¿™ä¹ˆå‘½åçš„ç†ç”±æ˜¯åé¢è¯»å– K çš„æ—¶å€™æ¯ä¸ª thread ä¼šå¾€è‡ªå·±çš„å¯„å­˜å™¨å†…è¯» <code>NUM_VECS_PER_THREAD</code> ä¸ª k_vec.)</li>
</ul>
<blockquote>
<p>è¯æ˜: <code>q_vecs</code> è¦†ç›– Q çš„ä¸€ä¸ª head, å¹¶ä¸” <code>NUM_VECS_PER_THREAD</code> è¡¨ç¤º Q çš„ä¸€ä¸ª head è¢«åˆ†æˆå¤šå°‘ä¸ª 16B.<br>
=&gt; <code>THREAD_GROUP_SIZE</code> * <code>VEC_SIZE</code> = 16B / <code>sizeof(scalar_t)</code>;<br>
=&gt; <code>NUM_VECS_PER_THREAD</code> * 16B / <code>sizeof(scalar_t)</code> = <code>HEAD_SIZE</code>;</p></blockquote>
<p>ç„¶åçœ‹ load Q çš„ä»£ç , å»ºè®®ç»“åˆä¸‹é¢çš„å›¾ä¸€èµ·çœ‹:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl">  <span class="c1">// Load Q to shmem
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_group_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREAD_GROUPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">vec_idx</span> <span class="o">=</span> <span class="n">thread_group_offset</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">q_vecs</span><span class="p">[</span><span class="n">thread_group_offset</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">        <span class="o">*</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="n">Q_vec</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">q_ptr</span> <span class="o">+</span> <span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><ul>
<li><code>thread_group_idx</code> è¡¨ç¤ºå½“å‰ thread å±äºå½“å‰ cuda block ä¸­ç¬¬å‡ ä¸ª thread group.</li>
<li><code>thread_group_offset</code> è¡¨ç¤ºå½“å‰ thread åœ¨å½“å‰ thread group ä¸­æ˜¯ç¬¬å‡ ä¸ª thread.</li>
</ul>
<p><img alt="pa-load-q.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-load-q.png"></p>
<p>ä¸Šå›¾å±•ç¤ºäº†å¾ªç¯å…·ä½“æ˜¯æ€ä¹ˆè·‘çš„.</p>
<ul>
<li>ä¸€ä¸ªç´«è‰²ç®­å¤´è¡¨ç¤ºä¸€ä¸ª thread group.</li>
<li><code>NUM_VECS_PER_THREAD</code> è¡¨ç¤º <code>HEAD_SIZE</code> èƒ½è¢«åˆ†æˆå¤šå°‘ä¸ª 16B.</li>
<li>å®é™…è¯»å– Q çš„å†…å­˜æ—¶, æ‰€æœ‰ thread group ä» Q çš„èµ·å§‹ä½ç½®ç´§å¯†æ’åˆ—, æ ¹æ®å›¾ä¸Šçœ‹çš„è¯ä¸€å…±æœ‰ <code>NUM_THREAD_GROUPS</code> ä¸ªç´«è‰²ç®­å¤´.</li>
<li>æ‰€æœ‰ thread group è¯»å–ä¸€æ¬¡ Q å¹¶å­˜å…¥ <code>q_vecs</code> å¯¹åº”å¾ªç¯ä¸­çš„ä¸€æ¬¡è¿­ä»£; å› æ­¤ä¸‹æ¬¡è¿­ä»£ thread group éœ€è¦å‘ååç§» <code>NUM_THREAD_GROUPS</code> ä¸ªä½ç½® (ä¾‹å¦‚ <code>i</code> ä» 1 å˜ä¸º 7).</li>
<li>æ­¤å¤–, è¯»ä¸€æ¬¡ 16B å¯¹åº”ä¸€ä¸ª thread æ¥è¯´è‡ªç„¶ä¹Ÿæ˜¯å–ä¸€ä¸ª VEC.</li>
<li>å¯¹åº”åˆ° kernel ç¼–å†™, è¿˜éœ€è¦è®¡ç®—å½“å‰ thread å…·ä½“è¯»å–å“ªä¸ª vec; å› æ­¤å¾—åˆ° <code>vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE</code>.</li>
</ul>
<blockquote>
<p>ğŸ¤” è¿™é‡Œä¼šä¸ä¼šæœ‰ bank conflict?</p></blockquote>
<p>æ€»ä¹‹ç°åœ¨æˆ‘ä»¬æŠŠ <code>(1, head_size)</code> å¤§å°çš„å…ƒç´ è¯»åˆ°äº† cuda block å…±äº«çš„ shared memory <code>q_vecs</code> ä¸­.</p>
<h3 id="53-è¯»å–-k-cache-å¹¶è®¡ç®—-qk">5.3. è¯»å– K Cache å¹¶è®¡ç®— QK</h3>
<p>ç°åœ¨ä» cuda block çš„è§’åº¦çœ‹, å½“å‰ block å·²ç»è·å¾—äº†è‡ªå·±è¦ç®—çš„ Q ä¸­çš„ä¸€ä¸ª head (å½¢çŠ¶ä¸º <code>(1, head_size)</code>), æ¥ä¸‹æ¥å°±æ˜¯è®¡ç®— Q å’Œ K çš„ç‚¹ç§¯.</p>
<p>ç‚¹ç§¯è¿‡ç¨‹æ˜¯æŠŠå½“å‰ block æ‹¥æœ‰çš„ Q head å’Œæ•´ä¸ª K Cache (è¿­ä»£åœ°) è¿›è¡Œç‚¹ç§¯è¿ç®—. å‚è€ƒä¸‹å›¾:</p>
<p><img alt="pa-cal-kq-01.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-cal-kq-01.png"></p>
<p>QK ä¹˜ç§¯å®é™…ä¸Šè¢«æš‚å­˜åœ¨ <code>logits</code> (ä¹Ÿæ˜¯ä¸€å— shared memory) ä¸­, ä¹‹åä¼šè¢«ç”¨æ¥è®¡ç®— softmax.</p>
<p>ğŸ˜‡ çœ‹ä¸‹å¾ªç¯çš„å…·ä½“ä»£ç å§:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Loop 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Physical block calculation ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// Loop 2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Offset calculation ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">K_vec</span> <span class="n">k_vecs</span><span class="p">[</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Loop 3
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Load K to `k_vecs` ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="kt">float</span> <span class="n">qk</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">Qk_dot</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span> <span class="n">THREAD_GROUP_SIZE</span><span class="o">&gt;::</span><span class="n">dot</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                        <span class="n">q_vecs</span><span class="p">[</span><span class="n">thread_group_offset</span><span class="p">],</span> <span class="n">k_vecs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Add the ALiBi bias if slopes are given.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">qk</span> <span class="o">+=</span> <span class="p">(</span><span class="n">alibi_slope</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">?</span> <span class="n">alibi_slope</span> <span class="o">*</span> <span class="p">(</span><span class="n">token_idx</span> <span class="o">-</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">thread_group_offset</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Store the partial reductions to shared memory.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// Mask
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// Update the max value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>å…ˆè¯´ç¬¬ä¸€ä¸ªå¾ªç¯, å…¶ä¸­æ¯”è¾ƒé‡è¦çš„å‡ ä¸ªå‚æ•°å®šä¹‰å¦‚ä¸‹:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// [start_block_idx, end_block_idx) is the range of blocks to process.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">int</span> <span class="n">start_block_idx</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">    <span class="n">USE_PARTITIONING</span> <span class="o">?</span> <span class="n">partition_idx</span> <span class="o">*</span> <span class="nl">num_blocks_per_partition</span> <span class="p">:</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="c1">// If not using partitioning, `end_block_idx` should be equal to `num_seq_blocks`.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">int</span> <span class="n">end_block_idx</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">    <span class="n">MIN</span><span class="p">(</span><span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">num_blocks_per_partition</span><span class="p">,</span> <span class="n">num_seq_blocks</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="c1">// Number of blocks to process.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">int</span> <span class="n">num_blocks</span> <span class="o">=</span> <span class="n">end_block_idx</span> <span class="o">-</span> <span class="n">start_block_idx</span><span class="p">;</span>
</span></span></code></pre></div><p>ç”¨æ–‡å­—æè¿°å°±æ˜¯:</p>
<ul>
<li><code>blk_idx</code> è¡¨ç¤ºå½“å‰ thread æ‰€åœ¨ warp éœ€è¦å¤„ç†çš„ PA block çš„åœ¨ <code>block_table</code> ä¸­ç´¢å¼• (é€»è¾‘ä¸Šçš„ç´¢å¼•).</li>
<li><code>start_block_idx</code> å’Œ <code>end_block_idx</code> è¡¨ç¤ºå½“å‰ cuda block éœ€è¦å¤„ç†çš„ block èŒƒå›´.</li>
<li><code>num_blocks</code> è¡¨ç¤ºå½“å‰ cuda block éœ€è¦å¤„ç†çš„ block æ•°é‡.</li>
<li><code>NUM_WARPS</code> è¡¨ç¤ºå½“å‰ cuda block ä¸­ warp çš„æ•°é‡. ä¸€ä¸ª warp åŒ…å« 32 ä¸ª thread.</li>
<li><code>warp_idx</code> è¡¨ç¤ºå½“å‰ warp åœ¨å½“å‰ cuda block ä¸­çš„ç´¢å¼•.</li>
</ul>
<p>è¯´äººè¯å°±æ˜¯æ¯ä¸ª warp å¤„ç†ä¸€ä¸ª PA block, ä¸€å¼€å§‹ cuda block ä¸­çš„æ‰€æœ‰ warp ç´§å¯†åœ°æŒ‡å‘æœ€å‰é¢çš„ <code>NUM_WARPS</code> ä¸ª PA block, æ¯æ¬¡å¾ªç¯æ‰€æœ‰ warp å‘ååç§» <code>NUM_WARPS</code> ä¸ª PA block çš„é•¿åº¦. å‚è€ƒä¸‹å›¾:</p>
<p><img alt="pa-cal-kq-02.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-cal-kq-02.png"></p>
<blockquote>
<p>ğŸ”” è¿™é‡Œå†å›é¡¾ä¸€ä¸‹, ä¸€ä¸ª PA block é‡Œå­˜æ”¾äº† <code>BLOCK_SIZE</code> ä¸ª token çš„ K æˆ– V cache.</p></blockquote>
<p>æ‰€ä»¥è¯´è¿™ä¸ªå¾ªç¯å’Œä¸Šé¢è¯»å– Q çš„å¾ªç¯ä¸€ä¸ªå°¿æ€§ğŸ¤®, ä¸è¿‡æ˜¯ä»¥ warp çš„ç²’åº¦å¤„ç†æ•°æ®;</p>
<p>è¿›å…¥äº†ç¬¬ä¸€ä¸ªå¾ªç¯å†…éƒ¨, ç¬¬ä¸€æ­¥å½“ç„¶æ˜¯è®¡ç®—å½“å‰ thread å¯¹åº”çš„ warp åº”è¯¥è®¡ç®—å“ªä¸ª PA block (ç‰©ç†ä¸Šçš„ç´¢å¼•), å› æ­¤å¾—åˆ°äº† <code>physical_block_number</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">const</span> <span class="kt">int64_t</span> <span class="n">physical_block_number</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">    <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">block_table</span><span class="p">[</span><span class="n">block_idx</span><span class="p">]);</span>
</span></span></code></pre></div><hr>
<p>ç„¶åè§£é‡Šç¬¬äºŒä¸ªå¾ªç¯, ç¬¬äºŒä¸ªå¾ªç¯çš„æ•´ä½“ç›®æ ‡å°±æ˜¯è®©å½“å‰ warp è®¡ç®—å¥½è‡ªå·±è´Ÿè´£çš„ PA block ä¸­ <code>BLOCK_SIZE</code> ä¸ª token çš„ QK ä¹˜ç§¯.</p>
<p>å…ˆçœ‹ä¸€ä¸‹ <code>i</code> çš„ä¸Šç•Œ:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">    <span class="n">DIVIDE_ROUND_UP</span><span class="p">(</span><span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">WARP_SIZE</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="c1">// Loop 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Loop 2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></div><p>ä» kernel è§’åº¦çœ‹, æ¯ä¸ª thread éœ€è¦è¾…åŠ©å½“å‰ warp è®¡ç®—è‡ªå·±è´Ÿè´£çš„ä¸€æ•´ä¸ª PA block (åŒ…å« <code>BLOCK_SIZE</code> ä¸ª token), è€Œæˆ‘ä»¬æŠŠè¿™ä¸ªè¿‡ç¨‹æ‹†åˆ†ä¸º Loop 2 ä¸­çš„ <code>NUM_TOKEN_PER_THREAD_GROUP</code> (ä¹Ÿå°±æ˜¯ <code>ceil(BLOCK_SIZE / 32)</code>) æ¬¡å¾ªç¯;</p>
<p>è¯´äººè¯å°±æ˜¯<strong>ä¸€ä¸ª thread group å¯¹åº”ä¸€ä¸ª token ä¸­çš„ä¸€ä¸ª head</strong>, å¦‚æœ BLOCK SIZE å¤ªå¤§äº†åé¢æ¯ä¸ª thread å‘ååç§» <code>i * WARP_SIZE</code> ä¸ª token ç»§ç»­ç‹ ç‹ ç®—ğŸ¤£.</p>
<p>ä¹Ÿå› æ­¤ç¬¬äºŒä¸ªå¾ªç¯å†…éƒ¨ä¸€ä¸Šæ¥å…ˆè®¡ç®—äº†å‡ ä¸ªåç§»é‡, å¹¶ä¸”ç”³è¯·äº† thread å†…éƒ¨ç§æœ‰çš„ <code>k_vecs</code> æ•°ç»„:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">const</span> <span class="kt">int</span> <span class="n">physical_block_offset</span> <span class="o">=</span> <span class="p">(</span><span class="n">thread_group_idx</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">WARP_SIZE</span><span class="p">)</span> <span class="o">%</span> <span class="n">BLOCK_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">const</span> <span class="kt">int</span> <span class="n">token_idx</span> <span class="o">=</span> <span class="n">block_idx</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">physical_block_offset</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">K_vec</span> <span class="n">k_vecs</span><span class="p">[</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span></code></pre></div><ul>
<li><code>thread_group_idx</code> è¡¨ç¤ºå½“å‰ thread group åœ¨æ•´ä¸ª cuda block ä¸­çš„ç´¢å¼•.</li>
<li>â˜¢ï¸ ä¸€ä¸ª thread group åœ¨ä¸€æ¬¡å¾ªç¯ä¸­è´Ÿè´£ fetch ä¸€ä¸ª PA block ä¸­ K cache çš„ä¸€ä¸ª token ä¸­<strong>è‡ªå·±è´Ÿè´£çš„ head</strong>.</li>
<li>â˜¢ï¸ ä¸€ä¸ª thread group è´Ÿè´£è®¡ç®—ä¸€ä¸ª qk å€¼; è¿™ä¸ªå€¼æ˜¾ç„¶æ˜¯ç”±ä¸€ä¸ª Q head å’Œä¸€ä¸ª K head ç‚¹ç§¯å¾—åˆ°çš„.</li>
<li><code>physical_block_offset</code> è¡¨ç¤ºå½“å‰è¦ç®—çš„ token åœ¨å½“å‰ PA block ä¸­çš„åç§»é‡ (æ³¨æ„å’Œå‰é¢çš„ <code>physical_block_number</code> åŒºåˆ†).</li>
<li>åŠ  <code>i * WARP_SIZE</code> çš„åŸå› æ˜¯å¦‚æœ <code>BLOCK_SIZE</code> å¤§äº 32, é‚£ä¹ˆä¸€ä¸ª warp è¦å¤šæ¬¡å¾ªç¯æ‰èƒ½å¤„ç†å®Œä¸€ä¸ª PA block ä¸­çš„æ‰€æœ‰ token, å¯¹åº” <code>thread_group_idx</code> éœ€è¦åšåç§».</li>
<li><code>token_idx</code> è¡¨ç¤ºå½“å‰è¦ç®—çš„ token åœ¨æ•´ä¸ª seq çš„ KV cache ä¸­çš„ç´¢å¼•.</li>
<li><code>k_vecs</code> ä¸­èƒ½å­˜æ”¾ <code>NUM_VECS_PER_THREAD</code> ä¸ª VEC, è€Œä¸€æ•´ä¸ª thread group ä¸­æ‰€æœ‰çš„ thread çš„ <code>k_vecs</code> åˆèµ·æ¥æ‰èƒ½ç»„æˆä¸€ä¸ª K çš„ head (æ¨å¯¼å‚è€ƒä¸Šé¢ Q çš„ ğŸ˜‡). è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåé¢ç®— QK çš„æ—¶å€™è¦ reduce.</li>
</ul>
<p>ğŸ¤” <strong>çœ‹åˆ°è¿™é‡Œè¯»è€…å¯èƒ½æœ‰ä¸€ä¸ªé—®é¢˜: ä¸€ä¸ª token çš„ K cache åº”è¯¥å¯¹åº”å¤šä¸ª head, ä¸ºä»€ä¹ˆä¸Šé¢è¯´ä¸€ä¸ª thread group åªè´Ÿè´£ä¸€ä¸ª head?</strong><br>
ç­”: å› ä¸ºå®é™…è®¡ç®—çš„æ—¶å€™, ä¸€ä¸ª cuda block åªè´Ÿè´£è®¡ç®—ä¸€ä¸ª head, å¯¹åº”åˆ° K Cache ä¹ƒè‡³åé¢ V Cache çš„ä½ç½®ä¹Ÿæ˜¯ä¸€æ ·çš„.</p>
<blockquote>
<p>è¿™é‡Œé¢å¤–è¯´ä¸€ä¸‹, è¯» K çš„ head çš„ä¸€ä¸ªç›®æ ‡åº”è¯¥æ˜¯åœ¨å°½é‡å°‘çš„ register ä¸­è£…ä¸‹ä¸€ä¸ª head çš„æ‰€æœ‰å…ƒç´ , è¿™æ ·åç»­å’Œ shared memory ä¸­çš„ Q åšç‚¹ä¹˜å¹¶è§„çº¦çš„é€Ÿåº¦æ›´å¿«. å‡è®¾ä¸€ä¸ª head æœ‰ 128 ä¸ª float16, åˆ™å ç”¨ 256B, è€Œ A100 ä¸­ä¸€ä¸ª thread æœ€å¤šèƒ½æœ‰ 255 ä¸ª 32-bit register (ä¹Ÿå°±æ˜¯ 1020B), æ­¤æ—¶å¯ä»¥è®¤ä¸ºä¸€ä¸ª thread èƒ½è£…ä¸‹ä¸€ä¸ª head çš„æ‰€æœ‰å…ƒç´ .<br>
ä½†æ˜¯ç”±äºç›®å‰ PA kernel åœ¨ <code>BLOCK_SIZE</code> ä¸º 16 çš„æƒ…å†µä¸‹ <code>THREAD_GROUP_SIZE</code> ç­‰äº 2, å› æ­¤ä¸€ä¸ª thread åªä¼šè£…ä¸€ä¸ª head çš„ä¸€åŠå…ƒç´ , è¿™æ ·å¯èƒ½ä¼šå¯¼è‡´ register çš„ä½¿ç”¨ç‡ä¸é«˜.</p></blockquote>
<hr>
<p>æ¥ç€è¿›å…¥ç¬¬ä¸‰ä¸ªå¾ªç¯, ç›®çš„æ˜¯è®© thread group ä» K cache ä¸­è¯»ä¸€ä¸ª head, å¹¶å­˜å…¥ <code>k_vecs</code> ä¸­:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// x == THREAD_GROUP_SIZE * VEC_SIZE
</span></span></span><span class="line"><span class="cl"><span class="c1">// Each thread group fetches x elements from the key at a time.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">16</span> <span class="o">/</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">cache_t</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="c1">//...
</span></span></span><span class="line"><span class="cl"><span class="c1">// Loop 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Loop 2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">K_vec</span> <span class="n">k_vecs</span><span class="p">[</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Loop 3
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">const</span> <span class="n">cache_t</span><span class="o">*</span> <span class="n">k_ptr</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">                <span class="n">k_cache</span> <span class="o">+</span> <span class="n">physical_block_number</span> <span class="o">*</span> <span class="n">kv_block_stride</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">                <span class="n">kv_head_idx</span> <span class="o">*</span> <span class="n">kv_head_stride</span> <span class="o">+</span> <span class="n">physical_block_offset</span> <span class="o">*</span> <span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="k">const</span> <span class="kt">int</span> <span class="n">vec_idx</span> <span class="o">=</span> <span class="n">thread_group_offset</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="k">const</span> <span class="kt">int</span> <span class="n">offset1</span> <span class="o">=</span> <span class="p">(</span><span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">)</span> <span class="o">/</span> <span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="k">const</span> <span class="kt">int</span> <span class="n">offset2</span> <span class="o">=</span> <span class="p">(</span><span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">)</span> <span class="o">%</span> <span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// if Fp8KVCacheDataType::kAuto
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">k_vecs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="o">*</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="n">K_vec</span><span class="o">*&gt;</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">              <span class="n">k_ptr</span> <span class="o">+</span> <span class="n">offset1</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">offset2</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></div><p>è€è§„çŸ©, å…ˆçœ‹ <code>j</code>, æœ¬è´¨å°±æ˜¯ä» 0 è¿­ä»£åˆ° <code>NUM_VECS_PER_THREAD</code>, æ¯æ¬¡è¿­ä»£å½“å‰ thread è¯»å–ä¸€ä¸ª VEC å­˜å…¥ <code>k_vecs</code> ä¸­.</p>
<blockquote>
<p>ğŸ”” å›é¡¾:</p>
<ol>
<li><code>NUM_VECS_PER_THREAD</code> è¡¨ç¤ºä¸€ä¸ª head è¢«åˆ†æˆå¤šå°‘ä¸ª 16B.</li>
<li><code>k_cache</code> çš„ shape ä¸º <code>(num_blocks, num_kv_heads, head_size/x, block_size, x)</code>.</li>
</ol></blockquote>
<p>å…¶ä¸­çš„ <code>x</code> è¡¨ç¤ºä¸€ä¸ª thread group éœ€è¦è¯»å–çš„å…ƒç´ æ•°é‡ (<code>VEC_SIZE</code> * <code>THREAD_GROUP_SIZE</code>); å› æ­¤ä½œè€…å°† K Cache çš„ layout çš„æœ€åä¸€ç»´è®¾ç½®ä¸º <code>x</code> å…¶å®ä¹Ÿæ˜¯æ–¹ä¾¿åç»­ thread group å¯¹ K cache çš„è¯»å–.</p>
<p>ä¸‹å›¾å…·ä½“å±•ç¤ºäº†å¯»å€çš„è¿‡ç¨‹:</p>
<p><img alt="pa-cal-kq-03.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-cal-kq-03.png"></p>
<p>å…¶ä¸­:</p>
<ul>
<li>åœ¨ MHSA ä¸­, <code>num_kv_heads</code> ç­‰äº <code>num_heads</code>; è€Œåœ¨ GQA, MQA ä¸­, <code>num_kv_heads</code> å°äº <code>num_heads</code>.</li>
<li>(1) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread å±äºçš„ warp è¦å¤„ç†å“ªä¸ª PA block.</li>
<li>(2) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread è¦è®¡ç®—çš„ head åœ¨ K cache ä¸­çš„ä½ç½®. è¿™ä¸ª head çš„ç´¢å¼•å’Œ Q ä¸­ head çš„ç´¢å¼•åœ¨ MHSA ä¸­ç›¸åŒ.</li>
<li>(3) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread group è¦è®¡ç®—çš„ token åœ¨å½“å‰ PA block ä¸­çš„ä½ç½®.</li>
<li>(5) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread åœ¨éœ€è¦è¯»å–çš„ head (è“è‰²é•¿æ–¹ä½“) ä¸­ x çš„åç§», é€šè¿‡ <code>j</code> è¿›è¡Œè¿­ä»£è¯»å–. <strong>æ¯æ¬¡å¾ªç¯ thread group ä¸­çš„æ‰€æœ‰ thread å–ä¸€ä¸ª x.</strong></li>
<li>(6) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread åœ¨ thread gruop ä¸­è¯»å–çš„ x ä¸­ VEC çš„åç§»; thread ä¸€æ¬¡è¯»å–ä¸€ä¸ª VEC.</li>
</ul>
<p>ğŸ¤” <strong>ä¸ºä»€ä¹ˆ (5) åœ¨å®é™…å¯»å€æ—¶éœ€è¦ <code>* BLOCK_SIZE * x</code> ?</strong><br>
ç­”: è¿™æ˜¯æ ¹æ® <code>k_cache</code> çš„ layout å¾—åˆ°çš„ stride. åŒç† (3) <code>* x</code> ä¹Ÿæ˜¯ stride.</p>
<p>ç¬¬ 3 ä¸ªå¾ªç¯ç»“æŸæ—¶å½“å‰ warp è´Ÿè´£çš„æ¯ä¸ª token ä¸­éœ€è¦çš„ K cache head å·²ç»å…¨è¢«åŠ è½½å…¥ thread æœ¬åœ°çš„ <code>k_vecs</code> ä¸­äº†.</p>
<p>ç”±äºä¸€ä¸ª thread group çš„ <code>k_vecs</code> æ‰èƒ½çœŸæ­£ç»„æˆä¸€ä¸ª head, åœ¨é€€å›ç¬¬äºŒä¸ªå¾ªç¯è¿›è¡Œ QK dot çš„æ—¶å€™, éœ€è¦åšä¸ª reduction, å…·ä½“çš„èŒƒå›´å°±æ˜¯ <code>THREAD_GROUP_SIZE</code> ä¸ª thread:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Loop 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Loop 2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">K_vec</span> <span class="n">k_vecs</span><span class="p">[</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Loop 3
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="kt">float</span> <span class="n">qk</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">Qk_dot</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span> <span class="n">THREAD_GROUP_SIZE</span><span class="o">&gt;::</span><span class="n">dot</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                             <span class="n">q_vecs</span><span class="p">[</span><span class="n">thread_group_offset</span><span class="p">],</span> <span class="n">k_vecs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></div><p>è®¡ç®—å®Œ <code>qk</code> å, ç”±å½“å‰ thread group ä¸­ç¬¬ä¸€ä¸ª (offset ä¸º 0) çš„ thread å¯¹è‡ªå·±åˆšæ‰ç®—å‡ºæ¥çš„ <code>qk</code> è¿›è¡Œ mask, é¡ºä¾¿çœ‹çœ‹å¦‚æœæ²¡æœ‰ mask æ‰, æŠŠ <code>qk_max</code> èµ‹å€¼ä¸º <code>qk</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">thread_group_offset</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Store the partial reductions to shared memory.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// NOTE(woosuk): It is required to zero out the masked logits.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">bool</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">token_idx</span> <span class="o">&gt;=</span> <span class="n">seq_len</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">[</span><span class="n">token_idx</span> <span class="o">-</span> <span class="n">start_token_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">?</span> <span class="mf">0.f</span> <span class="o">:</span> <span class="n">qk</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Update the max value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">qk_max</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">?</span> <span class="nl">qk_max</span> <span class="p">:</span> <span class="n">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">qk</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>ğŸ§ <strong>ä¸ºä»€ä¹ˆè¦åš mask?</strong></p>
<ul>
<li>å› ä¸ºä¸€ä¸ª seq çš„æœ€åä¸€ä¸ª PA block å¯èƒ½è¦†ç›–ä¸æ»¡ <code>BLOCK_SIZE</code> ä¸ª token. è¿™é‡Œçš„ mask å°±æ˜¯æŠŠé‚£éƒ¨åˆ† qk ç½®é›¶.</li>
</ul>
<h3 id="54-softmax">5.4. Softmax</h3>
<p>æˆ‘å‹’ä¸ª QK å•Š, æ€»ç®—ç®—å®Œäº†, é”å…‹ five éƒ½è¦è¢«æŠ½æ¸…ä»“äº†. é¡µæ„ä¸çœŸ, é‰´å®šä¸ºå¼€ç®— softmax.</p>
<p>ä¸»è¦æ­¥éª¤å°±æ˜¯å¹¿æ’­ç„¶åç®—, ç®— softmax éœ€è¦çŸ¥é“æ¯ä¸ª head å¯¹åº”çš„ qk çš„æœ€å¤§å€¼. ç”±äºä¸€ä¸ª cuda block è´Ÿè´£çš„å°±æ˜¯ä¸€ä¸ª head, å¯¹äºè¿™ä¸ª head ä¸Šé¢çš„è®¡ç®—æ­¥éª¤ä¸€å…±ç®—äº† <code>cache_len</code>ä¸ª token çš„ qk, å› æ­¤éœ€è¦åšä¸€ä¸ª cuda block èŒƒå›´çš„è§„çº¦, æ‰¾åˆ°å…¶ä¸­æœ€å¤§çš„ qk å€¼.</p>
<p>å…ˆåœ¨ warp å±‚é¢è§„çº¦.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__shared__</span> <span class="kt">float</span> <span class="n">red_smem</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">NUM_WARPS</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="c1">// Perform reduction across the threads in the same warp to get the
</span></span></span><span class="line"><span class="cl"><span class="c1">// max qk value for each &#34;warp&#34; (not across the thread block yet).
</span></span></span><span class="line"><span class="cl"><span class="c1">// The 0-th thread of each thread group already has its max qk value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">WARP_SIZE</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">mask</span> <span class="o">&gt;=</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span> <span class="n">mask</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">qk_max</span> <span class="o">=</span> <span class="n">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">VLLM_SHFL_XOR_SYNC</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">mask</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">lane</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">red_smem</span><span class="p">[</span><span class="n">warp_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">qk_max</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">__syncthreads</span><span class="p">();</span>
</span></span></code></pre></div><ul>
<li><code>red_smem</code> æ˜¯ä¹‹å‰ç”³è¯·çš„ shared memory.</li>
<li><code>VLLM_SHFL_XOR_SYNC</code> æ˜¯ä¸€ä¸ª warp å†…çš„ shuffle æ“ä½œ, å…·ä½“æ¥è¯´, åœ¨æ¯æ¬¡å¾ªç¯æ—¶, æ¯ä¸ª thread å’Œè‡ªå·±ç›¸è· <code>mask</code> ä½ç½®çš„çº¿ç¨‹äº¤æ¢æ•°æ® (äº¤æ¢æ¥çš„æ•°æ®é€šè¿‡ <code>fmaxf</code> æ¯”è¾ƒ), å¹¶ä¸” <code>mask</code> ä¼šé€æ¸å‡åŠ, ç›´åˆ° <code>THREAD_GROUP_SIZE</code> ä¸ºæ­¢.</li>
<li><code>lane</code> è¡¨ç¤ºå½“å‰ warp ä¸­çš„çº¿ç¨‹ç´¢å¼•.</li>
</ul>
<p>æ¥ç€å†å¯¹æ¯ä¸ª warp çš„æœ€å¤§å€¼è¿›è¡Œè§„çº¦, ç”±äºæ¯ä¸ª warp çš„æœ€å¤§å€¼éƒ½è¢«å­˜å…¥äº† <code>red_smem</code> ä¸­, æ‰€ä»¥åªéœ€è¦å†æ¬¡è¿›è¡Œ shuffle æ“ä½œå³å¯.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// TODO(woosuk): Refactor this part.
</span></span></span><span class="line"><span class="cl"><span class="c1">// Get the max qk value for the sequence.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">qk_max</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">&lt;</span> <span class="n">NUM_WARPS</span> <span class="o">?</span> <span class="n">red_smem</span><span class="p">[</span><span class="n">lane</span><span class="p">]</span> <span class="o">:</span> <span class="o">-</span><span class="n">FLT_MAX</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">NUM_WARPS</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">mask</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">mask</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">qk_max</span> <span class="o">=</span> <span class="n">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">VLLM_SHFL_XOR_SYNC</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">mask</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>æ­¤æ—¶, ç¬¬ 1 ä¸ªçº¿ç¨‹çš„ <code>qk_max</code> å°±æ˜¯å½“å‰ cuda block ä¸­æ‰€æœ‰ warp ä¸­æœ€å¤§çš„ qk å€¼. å°†å…¶å¹¿æ’­ç»™æ‰€æœ‰çº¿ç¨‹:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Broadcast the max qk value to all threads.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">qk_max</span> <span class="o">=</span> <span class="n">VLLM_SHFL_SYNC</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
</span></span></code></pre></div><p>åœ¨è·å¾—äº† <code>qk_max</code> å, å°±å¯ä»¥è®¡ç®— softmax äº†:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Get the sum of the exp values.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">float</span> <span class="n">exp_sum</span> <span class="o">=</span> <span class="mf">0.f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_tokens</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREADS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">__expf</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">qk_max</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">exp_sum</span> <span class="o">+=</span> <span class="n">val</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">exp_sum</span> <span class="o">=</span> <span class="n">block_sum</span><span class="o">&lt;</span><span class="n">NUM_WARPS</span><span class="o">&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="n">red_smem</span><span class="p">[</span><span class="n">NUM_WARPS</span><span class="p">],</span> <span class="n">exp_sum</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Compute softmax.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">float</span> <span class="n">inv_sum</span> <span class="o">=</span> <span class="n">__fdividef</span><span class="p">(</span><span class="mf">1.f</span><span class="p">,</span> <span class="n">exp_sum</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="n">f</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_tokens</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREADS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*=</span> <span class="n">inv_sum</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">__syncthreads</span><span class="p">();</span>
</span></span></code></pre></div><h3 id="55-lv-logits--value">5.5. LV (Logits * Value)</h3>
<p><img alt="pa-cal.png" loading="lazy" src="/imgs/blogs/dive-into-paged-attention/pa-cal.png"></p>
<p>ä¸Šå›¾å±•ç¤ºäº† LV çš„è®¡ç®—è¿‡ç¨‹, ä¸»è¦åŒºåˆ«æ˜¯ç”±äºè¦è®¡ç®— Logits çš„ shape å¯ä»¥è¡¨ç¤ºä¸º <code>(num_heads, num_seqs, cache_len)</code>, è€Œ V çš„ shape å¯ä»¥è¡¨ç¤ºä¸º <code>(num_heads, cache_len, head_size)</code>, å› æ­¤ LV çš„çŸ©é˜µä¹˜æ³•ä¸­, æ¯è®¡ç®—ä¸€ä¸ªå…ƒç´ éœ€è¦è¯»å– logits çš„ä¸€è¡Œå’Œ V çš„ä¸€åˆ—è¿›è¡Œè®¡ç®—.</p>
<p>æ­¤æ—¶, ä¸€ä¸ª cuda block çš„èŒè´£ä» &ldquo;è‡ª Q ä¸­è¯»å–ä¸€ä¸ª head&rdquo; è½¬å˜ä¸º &ldquo;è®¡ç®— output ä¸­çš„ä¸€ä¸ª head&rdquo;.</p>
<p>ğŸ§ <strong>ä¸ºä»€ä¹ˆåœ¨è®¡ç®— LV æ—¶, å»æ‰äº† thread group çš„æ¦‚å¿µ, æ¯ä¸ª thread éƒ½è¢«è®¾å®šä¸ºæ¯æ¬¡è¯»å– 16B?</strong></p>
<ul>
<li>å› ä¸ºç°åœ¨æ¯è®¡ç®—ä¸€ä¸ªå…ƒç´ , éœ€è¦çš„è®¿å­˜é‡æ›´å¤§, å› æ­¤ç»™æ¯ä¸ª thread åˆ†é…äº†æ›´å¤šçš„æ•°æ®è¯»å–é‡. ä¹Ÿå°±æ˜¯è¯´, <code>V_VEC_SIZE</code> æ¯” <code>VEC_SIZE</code> æ›´å¤§.</li>
</ul>
<p>ç”±äº cuda è®¿å­˜æ¨¡å¼æŒ‰è¡Œè¯»å–æ›´å¿«, æ‰€ä»¥å®é™…çš„è®¡ç®—ç»“æœåœ¨éå† PA block æ—¶çº¿ç¨‹å†…éƒ¨åˆ©ç”¨ <code>accs</code> è¿›è¡Œç´¯è®¡ (ä»¥å®ç°ä¸ V çš„ä¸€åˆ—è¿›è¡Œè®¡ç®—çš„è¡Œä¸º):</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">=</span> <span class="n">BLOCK_SIZE</span> <span class="o">/</span> <span class="n">V_VEC_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_ROWS_PER_ITER</span> <span class="o">=</span> <span class="n">WARP_SIZE</span> <span class="o">/</span> <span class="n">NUM_V_VECS_PER_ROW</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">constexpr</span> <span class="kt">int</span> <span class="n">NUM_ROWS_PER_THREAD</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">    <span class="n">DIVIDE_ROUND_UP</span><span class="p">(</span><span class="n">HEAD_SIZE</span><span class="p">,</span> <span class="n">NUM_ROWS_PER_ITER</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// NOTE(woosuk): We use FP32 for the accumulator for better accuracy.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">float</span> <span class="n">accs</span><span class="p">[</span><span class="n">NUM_ROWS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">V_vec</span> <span class="n">v_vec</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">V_VEC_SIZE</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Load V to `v_vec` ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">v_vec_ptr</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">token_idx</span> <span class="o">+</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">seq_len</span> <span class="o">?</span> <span class="n">v_vec_ptr</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">:</span> <span class="n">zero_value</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Accumulate the dot product.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">dot</span><span class="p">(</span><span class="n">logits_vec</span><span class="p">,</span> <span class="n">v_vec</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>ç”±äºæ¯ä¸ªçº¿ç¨‹è´Ÿè´£çš„ç´¯è®¡éƒ¨åˆ†ä¸æ»¡ä¸€æ•´è¡Œ/åˆ—, æ‰€ä»¥è¿›è¡Œè§„çº¦:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Perform reduction within each warp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">mask</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">mask</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">acc</span> <span class="o">+=</span> <span class="n">VLLM_SHFL_XOR_SYNC</span><span class="p">(</span><span class="n">acc</span><span class="p">,</span> <span class="n">mask</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">acc</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// NOTE(woosuk): A barrier is required because the shared memory space for
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// logits is reused for the output.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Perform reduction across warps.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">float</span><span class="o">*</span> <span class="n">out_smem</span> <span class="o">=</span> <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">shared_mem</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">NUM_WARPS</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">mid</span> <span class="o">=</span> <span class="n">i</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Upper warps write to shared memory.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">warp_idx</span> <span class="o">&gt;=</span> <span class="n">mid</span> <span class="o">&amp;&amp;</span> <span class="n">warp_idx</span> <span class="o">&lt;</span> <span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="kt">float</span><span class="o">*</span> <span class="n">dst</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">out_smem</span><span class="p">[(</span><span class="n">warp_idx</span> <span class="o">-</span> <span class="n">mid</span><span class="p">)</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">const</span> <span class="kt">int</span> <span class="n">row_idx</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">/</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">NUM_ROWS_PER_ITER</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">HEAD_SIZE</span> <span class="o">&amp;&amp;</span> <span class="n">lane</span> <span class="o">%</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="n">dst</span><span class="p">[</span><span class="n">row_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Lower warps update the output.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">warp_idx</span> <span class="o">&lt;</span> <span class="n">mid</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">src</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">out_smem</span><span class="p">[</span><span class="n">warp_idx</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">const</span> <span class="kt">int</span> <span class="n">row_idx</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">/</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">NUM_ROWS_PER_ITER</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">HEAD_SIZE</span> <span class="o">&amp;&amp;</span> <span class="n">lane</span> <span class="o">%</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">src</span><span class="p">[</span><span class="n">row_idx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span></code></pre></div><p>æœ€åå†™å…¥åˆ°è¾“å‡ºä¸­:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl">  <span class="c1">// Write the final output.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="n">warp_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">scalar_t</span><span class="o">*</span> <span class="n">out_ptr</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">+</span> <span class="n">seq_idx</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">max_num_partitions</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">        <span class="n">head_idx</span> <span class="o">*</span> <span class="n">max_num_partitions</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span> <span class="o">+</span> <span class="n">partition_idx</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_ROWS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">const</span> <span class="kt">int</span> <span class="n">row_idx</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">/</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">NUM_ROWS_PER_ITER</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="p">(</span><span class="n">row_idx</span> <span class="o">&lt;</span> <span class="n">HEAD_SIZE</span> <span class="o">&amp;&amp;</span> <span class="n">lane</span> <span class="o">%</span> <span class="n">NUM_V_VECS_PER_ROW</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">from_float</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">out_ptr</span> <span class="o">+</span> <span class="n">row_idx</span><span class="p">),</span> <span class="n">accs</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span></code></pre></div>]]></content:encoded></item></channel></rss>