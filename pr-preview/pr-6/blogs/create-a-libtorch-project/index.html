<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Create A LibTorch Project | ÁßãÊ∞¥¬∑JamesNULLiu</title><meta name=keywords content="pytorch,gtest"><meta name=description content="How to create a LibTorch project."><meta name=author content="jamesnulliu"><link rel=canonical href=https://jamesnulliu.github.io/blogs/create-a-libtorch-project/><link crossorigin=anonymous href=/assets/css/stylesheet.62cb9c488bb33c0e9a9d3c29b7f4259cbb0db25aaa19ba672188203d3d5bcaf9.css integrity="sha256-YsucSIuzPA6anTwpt/QlnLsNslqqGbpnIYggPT1byvk=" rel="preload stylesheet" as=style><link rel=icon href=https://jamesnulliu.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://jamesnulliu.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://jamesnulliu.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://jamesnulliu.github.io/apple-touch-icon.png><link rel=mask-icon href=https://jamesnulliu.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://jamesnulliu.github.io/blogs/create-a-libtorch-project/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]]}}</script><meta property="og:url" content="https://jamesnulliu.github.io/blogs/create-a-libtorch-project/"><meta property="og:site_name" content="ÁßãÊ∞¥¬∑JamesNULLiu"><meta property="og:title" content="Create A LibTorch Project"><meta property="og:description" content="How to create a LibTorch project."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2024-12-23T01:00:00+08:00"><meta property="article:modified_time" content="2025-09-12T22:50:04+00:00"><meta property="article:tag" content="C++"><meta property="article:tag" content="Python"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="Gtest"><meta name=twitter:card content="summary"><meta name=twitter:title content="Create A LibTorch Project"><meta name=twitter:description content="How to create a LibTorch project."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://jamesnulliu.github.io/blogs/"},{"@type":"ListItem","position":2,"name":"Create A LibTorch Project","item":"https://jamesnulliu.github.io/blogs/create-a-libtorch-project/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Create A LibTorch Project","name":"Create A LibTorch Project","description":"How to create a LibTorch project.","keywords":["pytorch","gtest"],"articleBody":"0. Introduction These days I am reading Programming Massively Parallel Processors: A Hands-on Approach, 4th Edition , and created a project to store my notes as I learn.\nOne of the most important parts in the book is writing cuda kernels, so I decided to build all kernels into shared libraries and test those implementations both in C++ and Python.\nI generated my project using this template specifically tailored for the similar scenario, but still met some problems such as conflicts when linking libtorch and gtest ü§Ø.\nSo the purpose of this blog is to provide a concise guide to:\nBuild a C++, CUDA and LibTorch library, test it with gtest. Load the library into torch, call the operaters in Python. Resolve problems when linking all the libraries. ‚ö†Ô∏èWARNING\nFind some tutorials on how to use cmake and vcpkg before reading this blog.\n1. Environment and Quick Start Check README.md of the project repository.\n2. Create a C++, CUDA and LibTorch Project I put all C++ codes in ‚Äú ./csrc/ ‚Äù and build them with cmake. The intermediate files should be generated in ‚Äú./build/‚Äù and that is just about using some command-line arguments, see this line .\nVcpkg is used to manage the dependencies of the project. I am not going to teach you how to use vcpkg in this blog, but I will mention some pitfalls I met when using it.\nüòçÔ∏è I really enjoy building C++ projects with cmake and vcpkg. Have a try if you haven‚Äôt used them before.\n2.1. How to Link against LibTorch Since you have installed pytorch in 1. Environment , now you already have libtorch installed in your conda environment. Run this command, and you will get the cmake prefix path of libtorch:\n1 python -c \"import torch;print(torch.utils.cmake_prefix_path)\" To integrate libtorch into cmake, I create this file and this file to find libtorch in the current project and use them here .\nNow you can link your targets against libtorch simply like what I do here .\nüìùNOTE\nWhen you link your target against ${TORCH_LIBRARIES}, cuda libraries are being linked automatically, which means you don‚Äôt have to find and link cuda using something like I write here 2.2. CMake and VCPKG Configuration Currently, I am planning to use the C/C++ packages listed in this file . I load the packages with these lines in \"./csrc/CMakeLists.txt\" . Then I link those packages to my targets here and here .\nüìùNOTE\nlibtorch \u003c 2.6 is compiled with _GLIBCXX_USE_CXX11_ABI=0 to use legacy ABI before C++11, which conflicts with the packages managed by vcpkg in default. Consequentially, you have to create a custom vcpkg triplet to control the behaviors when vcpkg actually build the packages. The triplet file is here and is enabled by these lines when building the C++ part.\nI also set CMAKE_CXX_SCAN_FOR_MODULES to OFF on this line because some compile errors occurs. This is a temporary solution but I am not planning to use modules from C++20 in this project, so just ignoring it.\n2.3. Write and Register Custom Torch Operators In order to register a custom torch operator, basically what you need to do next is to write a function that usually takes several torch::Tensor as input and returns a torch::Tensor as output, and then register this function to torch.\nFor example, I implement pmpp::ops::cpu::launchVecAdd in this cpp file and pmpp::ops::cuda::launchVecAdd in this cu file and provide the corresponding torch implentations pmpp::ops::cpu::vectorAddImpl and pmpp::ops::cuda::vectorAddImpl in this file .\nü§î I didn‚Äôt add any of those function declarations in hpp files under ‚Äú./include‚Äù because I don‚Äôt think they should be exposed to the users of the library. For the testing part, I will get and test the functions using torch::Dispatcher which aligns with the operaters invoked in python.\nTo register these implementations as an operater into pytorch, see this line , this line , and this line , where I:\nDefine a python function vector_add with signature: vector_add(Tensor a, Tensor b) -\u003e Tensor. Register the CPU implementation of the function. Register the CUDA implementation of the function. Now vector_add is a custom torch operator which can be called in both C++ and Python. All you need to do is to build these codes into a shared library like what I did here in cmake .\n2.4. Test the Custom Torch Operators in C++ As long as a custom torch operator is registered, normally one or multiple shared libraries will be generated. For C++ users, you should link your executable target against libtorch and the generated shared libraries so that those registered operators can be called.\nSince I have linked libPmppTorchOps against libtorch as PUBLIC in this line , the test target will link against libtorch automatically as long as it links against libPmppTorchOps, see this line .\nüìùNOTE\nYou may be confused about why -Wl,--no-as-needed is added before ${PROJECT_NAMESPACE}pmpp-torch-ops. This is because the shared libraries are not directly used in the test target (an operator is register in the library but not called directly in the executable), and the linker will not link against them by default. This flag will force the linker to link against the shared libraries even if they are not directly used.\nThe registered operators can be dispatched in a not-so-intuitional way ü§£ based on the official documentation, see here .\nNow the only thing is to test the operators in C++ using gtest, but this is not the focus of this blog. So let‚Äôs move on to the next part.\n3. Create and Package a Python Project 3.1. pyproject.toml and setup.py In modern python, pyproject.toml is a de-facto standard configuration file for packaging, and in this project, setuptools is used as the build backend because I believe it is the most popular one and is easy to cooperate with cmake.\nParticularly, ‚Äú ./pyproject.toml ‚Äù and ‚Äú ./setup.py ‚Äù defines what will happen when you run pip install . in the root directory of the project. I created CMakeExtention and CMakeBuild ( here ) and pass them to setup function ( here ) so that the C++ library libPmppTorchOps (under ‚Äú./csrc/‚Äù) will be built and installed before installing the python package.\nYou can easily understand what I did by reading the source code of these two files, and there is one more thing I want to mention.\nBased on 2. Create a C++, CUDA and LibTorch Project, you should find that the generated shared library is under ./build/lib ending with .so on linux or .dll on windows. Additionally, I added an install procedure here which will copy the shared libraries to ‚Äú./src/pmpp/_torch_ops‚Äù.\nNote that ‚Äú ./src/pmpp ‚Äù is already an existing directory being the root of the actual python package, and ‚Äú./src/pmpp/_torch_ops‚Äù will be created automatically while installing the shared libraries.\nThe problem is, when packaging the python project, only the directory containing ‚Äú__init__.py‚Äù will be considered as a package (or module), and I don‚Äôt want to add this file to ‚Äú./src/pmpp/_torch_ops‚Äù due to my mysophobia üò∑. Therefore, I used find_namespace_packages instead of find_packages and specified package_data to include the shared libraries here .\n3.2. Install the Python Package If you are planning to build your libraries with dependencies listed here while installing the python project, I don‚Äôt really suggest installing it in an isolated python environment (which is the default behavior of setuptools). All packages listed here have to be re-installed and in our case you need to at least append torch to that list.\nAlternatively, try this command, which will directly use the torch installed in current conda environment:\npip install --no-build-isolation -v . 3.3. Test the Custom Torch Operators in Python As long as you have the shared libraries built in 2. Create a C++, CUDA and LibTorch Project , all you need to do is to use torch.ops.load_library to load the shared libraries and call the registered operators.\nI write this process into ‚Äú src/pmpp/__init__.py ‚Äù, so the time you import pmpp in python, your custom torch operators will be ready to use. See this file for an example of testing the operators.\n","wordCount":"1337","inLanguage":"en","datePublished":"2024-12-23T01:00:00+08:00","dateModified":"2025-09-12T22:50:04Z","author":[{"@type":"Person","name":"jamesnulliu"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://jamesnulliu.github.io/blogs/create-a-libtorch-project/"},"publisher":{"@type":"Organization","name":"ÁßãÊ∞¥¬∑JamesNULLiu","logo":{"@type":"ImageObject","url":"https://jamesnulliu.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://jamesnulliu.github.io/ accesskey=h title="ÁßãÊ∞¥¬∑JamesNULLiu (Alt + H)">ÁßãÊ∞¥¬∑JamesNULLiu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://jamesnulliu.github.io/zh/ title=ÁÆÄ‰Ωì‰∏≠Êñá aria-label=ÁÆÄ‰Ωì‰∏≠Êñá>ÁÆÄ‰Ωì‰∏≠Êñá</a></li></ul></div></div><ul id=menu><li><a href=https://jamesnulliu.github.io/ title=Home><span>Home</span></a></li><li><a href=https://jamesnulliu.github.io/about_me/ title="About Me"><span>About Me</span></a></li><li><a href=https://jamesnulliu.github.io/blogs/ title=Blogs><span>Blogs</span></a></li><li><a href=https://jamesnulliu.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://jamesnulliu.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://jamesnulliu.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://jamesnulliu.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://jamesnulliu.github.io/friends/ title=Friends><span>Friends</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://jamesnulliu.github.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://jamesnulliu.github.io/blogs/>Blogs</a></div><h1 class="post-title entry-hint-parent">Create A LibTorch Project</h1><div class=post-description>How to create a LibTorch project.</div><div class=post-meta><span title='2024-12-23 01:00:00 +0800 +0800'>Dec-23-2024</span>&nbsp;¬∑&nbsp;7 min&nbsp;¬∑&nbsp;1337 words&nbsp;¬∑&nbsp;jamesnulliu</div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#0-introduction aria-label="0. Introduction">0. Introduction</a></li><li><a href=#1-environment-and-quick-start aria-label="1. Environment and Quick Start">1. Environment and Quick Start</a></li><li><a href=#2-create-a-c-cuda-and-libtorch-project aria-label="2. Create a C++, CUDA and LibTorch Project">2. Create a C++, CUDA and LibTorch Project</a><ul><li><a href=#21-how-to-link-against-libtorch aria-label="2.1. How to Link against LibTorch">2.1. How to Link against LibTorch</a></li><li><a href=#22-cmake-and-vcpkg-configuration aria-label="2.2. CMake and VCPKG Configuration">2.2. CMake and VCPKG Configuration</a></li><li><a href=#23-write-and-register-custom-torch-operators aria-label="2.3. Write and Register Custom Torch Operators">2.3. Write and Register Custom Torch Operators</a></li><li><a href=#24-test-the-custom-torch-operators-in-c aria-label="2.4. Test the Custom Torch Operators in C++">2.4. Test the Custom Torch Operators in C++</a></li></ul></li><li><a href=#3-create-and-package-a-python-project aria-label="3. Create and Package a Python Project">3. Create and Package a Python Project</a><ul><li><a href=#31-pyprojecttoml-and-setuppy aria-label="3.1. pyproject.toml and setup.py">3.1. <code>pyproject.toml</code> and <code>setup.py</code></a></li><li><a href=#32-install-the-python-package aria-label="3.2. Install the Python Package">3.2. Install the Python Package</a></li><li><a href=#33-test-the-custom-torch-operators-in-python aria-label="3.3. Test the Custom Torch Operators in Python">3.3. Test the Custom Torch Operators in Python</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{let e=null;const t=window.innerHeight+window.pageYOffset>=document.body.offsetHeight-100;if(t)e=elements[elements.length-1];else{let t=null,n=1/0;elements.forEach(e=>{const s=getOffsetTop(e)-window.pageYOffset;if(s<=window.innerHeight*.3){const o=Math.abs(s);o<n&&(n=o,t=e)}}),e=t||elements[0]}if(e&&e!==activeElement){if(activeElement){const t=encodeURI(activeElement.getAttribute("id")).toLowerCase(),e=document.querySelector(`.inner ul li a[href="#${t}"]`);e&&e.classList.remove("active")}activeElement=e;const n=encodeURI(activeElement.getAttribute("id")).toLowerCase(),t=document.querySelector(`.inner ul li a[href="#${n}"]`);t&&(t.classList.add("active"),document.getElementById("toc-container").classList.contains("wide")&&scrollTocToActiveItem(t))}},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}function scrollTocToActiveItem(e){const t=document.querySelector(".toc .inner");if(!t||!e)return;const n=t.getBoundingClientRect(),s=e.getBoundingClientRect(),o=n.height/2,i=s.top-n.top+t.scrollTop,a=i-o;t.scrollTo({top:Math.max(0,a),behavior:"smooth"})}</script><div class=post-content><h2 id=0-introduction>0. Introduction<a hidden class=anchor aria-hidden=true href=#0-introduction>#</a></h2><p>These days I am reading <a href=https://www.elsevier.com/books/programming-massively-parallel-processors/kirk/978-0-12-811986-0 target=_blank rel="noopener noreferrer">Programming Massively Parallel Processors: A Hands-on Approach, 4th Edition
</a>, and created a <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors target=_blank rel="noopener noreferrer">project
</a>to store my notes as I learn.</p><p>One of the most important parts in the book is writing <strong>cuda kernels</strong>, so I decided to build all kernels into shared libraries and test those implementations both in C++ and Python.</p><p>I generated my project using <a href=https://github.com/jamesnulliu/VSC-Python-Project-Template target=_blank rel="noopener noreferrer">this template
</a>specifically tailored for the similar scenario, but still met some problems such as conflicts when linking libtorch and gtest ü§Ø.</p><p><strong>So the purpose of this blog is to provide a concise guide to:</strong></p><ol><li>Build a C++, CUDA and LibTorch library, test it with gtest.</li><li>Load the library into torch, call the operaters in Python.</li><li>Resolve problems when linking all the libraries.</li></ol><blockquote><p>‚ö†Ô∏è<strong>WARNING</strong><br>Find some tutorials on how to use cmake and vcpkg before reading this blog.</p></blockquote><h2 id=1-environment-and-quick-start>1. Environment and Quick Start<a hidden class=anchor aria-hidden=true href=#1-environment-and-quick-start>#</a></h2><p>Check <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/tree/c648649/README.md target=_blank rel="noopener noreferrer">README.md
</a>of the project repository.</p><h2 id=2-create-a-c-cuda-and-libtorch-project>2. Create a C++, CUDA and LibTorch Project<a hidden class=anchor aria-hidden=true href=#2-create-a-c-cuda-and-libtorch-project>#</a></h2><p>I put all C++ codes in &ldquo;<a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/tree/96685ab/csrc target=_blank rel="noopener noreferrer">
./csrc/
</a>&rdquo; and build them with cmake. The intermediate files should be generated in &ldquo;./build/&rdquo; and that is just about using some command-line arguments, see <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/scripts/build.sh#L42 target=_blank rel="noopener noreferrer">this line
</a>.</p><p>Vcpkg is used to manage the dependencies of the project. I am not going to teach you how to use vcpkg in this blog, but I will mention some pitfalls I met when using it.</p><blockquote><p>üòçÔ∏è I really enjoy building C++ projects with cmake and vcpkg. Have a try if you haven&rsquo;t used them before.</p></blockquote><h3 id=21-how-to-link-against-libtorch>2.1. How to Link against LibTorch<a hidden class=anchor aria-hidden=true href=#21-how-to-link-against-libtorch>#</a></h3><p>Since you have installed pytorch in <a href=#1-environment>1. Environment
</a>, now you already have libtorch installed in your conda environment. Run this command, and you will get the cmake prefix path of libtorch:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python -c <span class=s2>&#34;import torch;print(torch.utils.cmake_prefix_path)&#34;</span>
</span></span></code></pre></td></tr></table></div></div><p>To integrate libtorch into cmake, I create <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/cmake/utils/run-python.cmake target=_blank rel="noopener noreferrer">this file
</a>and <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/cmake/libraries/libtorch.cmake target=_blank rel="noopener noreferrer">this file
</a>to find libtorch in the current project and use them <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/CMakeLists.txt#L27 target=_blank rel="noopener noreferrer">here
</a>.</p><p>Now you can link your targets against libtorch simply like what I do <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/CMakeLists.txt#L19 target=_blank rel="noopener noreferrer">here
</a>.</p><blockquote><p>üìù<strong>NOTE</strong><br>When you link your target against <code>${TORCH_LIBRARIES}</code>, cuda libraries are being linked automatically, which means you don&rsquo;t have to find and link cuda using something like I write <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab6/csrc/cmake/libraries/libcuda.cmake target=_blank rel="noopener noreferrer">here</a></p></blockquote><h3 id=22-cmake-and-vcpkg-configuration>2.2. CMake and VCPKG Configuration<a hidden class=anchor aria-hidden=true href=#22-cmake-and-vcpkg-configuration>#</a></h3><p>Currently, I am planning to use the C/C++ packages listed in <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/vcpkg.json target=_blank rel="noopener noreferrer">this file
</a>. I load the packages with <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/CMakeLists.txt#L30-L36 target=_blank rel="noopener noreferrer">these lines in "./csrc/CMakeLists.txt"
</a>. Then I link those packages to my targets <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/CMakeLists.txt#L20-L21 target=_blank rel="noopener noreferrer">here
</a>and <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/test/CMakeLists.txt#L11-L13 target=_blank rel="noopener noreferrer">here
</a>.</p><blockquote><p>üìù<strong>NOTE</strong><br><code>libtorch &lt; 2.6</code> is compiled with <code>_GLIBCXX_USE_CXX11_ABI=0</code> to use legacy ABI before C++11, which conflicts with the packages managed by vcpkg in default. Consequentially, you have to create a custom vcpkg triplet to control the behaviors when vcpkg actually build the packages. The triplet file is <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/cmake/vcpkg-triplets/x64-linux.cmake target=_blank rel="noopener noreferrer">here
</a>and is enabled by <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/scripts/build.sh#L47-L48 target=_blank rel="noopener noreferrer">these lines
</a>when building the C++ part.</p></blockquote><p>I also set <code>CMAKE_CXX_SCAN_FOR_MODULES</code> to <code>OFF</code> on <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/cmake/compilers/cxx-compiler-configs.cmake#L15 target=_blank rel="noopener noreferrer">this line
</a>because some compile errors occurs. This is a temporary solution but I am not planning to use modules from C++20 in this project, so just ignoring it.</p><h3 id=23-write-and-register-custom-torch-operators>2.3. Write and Register Custom Torch Operators<a hidden class=anchor aria-hidden=true href=#23-write-and-register-custom-torch-operators>#</a></h3><p>In order to register a custom torch <strong>operator</strong>, basically what you need to do next is to write a <strong>function</strong> that usually takes several <code>torch::Tensor</code> as input and returns a <code>torch::Tensor</code> as output, and then register this function to torch.</p><p>For example, I implement <code>pmpp::ops::cpu::launchVecAdd</code> in <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/ops/vecAdd/op.cpp target=_blank rel="noopener noreferrer">this cpp file
</a>and <code>pmpp::ops::cuda::launchVecAdd</code> in <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/ops/vecAdd/op.cu target=_blank rel="noopener noreferrer">this cu file
</a>and provide the corresponding torch implentations <code>pmpp::ops::cpu::vectorAddImpl</code> and <code>pmpp::ops::cuda::vectorAddImpl</code> in <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/ops/vecAdd/torch_impl.cpp target=_blank rel="noopener noreferrer">this file
</a>.</p><blockquote><p>ü§î I didn&rsquo;t add any of those function declarations in hpp files under &ldquo;./include&rdquo; because I don&rsquo;t think they should be exposed to the users of the library. For the testing part, I will get and test the functions using <code>torch::Dispatcher</code> which aligns with the operaters invoked in python.</p></blockquote><p>To register these implementations as an operater into pytorch, see <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/ops/torch_bind.cpp#L10 target=_blank rel="noopener noreferrer">this line
</a>, <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/ops/torch_bind.cpp#L22 target=_blank rel="noopener noreferrer">this line
</a>, and <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/ops/torch_bind.cpp#L32 target=_blank rel="noopener noreferrer">this line
</a>, where I:</p><ol><li>Define a python function <code>vector_add</code> with signature: <code>vector_add(Tensor a, Tensor b) -> Tensor</code>.</li><li>Register the CPU implementation of the function.</li><li>Register the CUDA implementation of the function.</li></ol><p>Now <code>vector_add</code> is a custom torch operator which can be called in both C++ and Python. All you need to do is to build these codes into a shared library like what I did <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/CMakeLists.txt#L7 target=_blank rel="noopener noreferrer">here in cmake
</a>.</p><h3 id=24-test-the-custom-torch-operators-in-c>2.4. Test the Custom Torch Operators in C++<a hidden class=anchor aria-hidden=true href=#24-test-the-custom-torch-operators-in-c>#</a></h3><p>As long as a custom torch operator is registered, normally one or multiple shared libraries will be generated. For C++ users, you should link your executable target against libtorch and the generated shared libraries so that those registered operators can be called.</p><p>Since I have linked <code>libPmppTorchOps</code> against libtorch as <code>PUBLIC</code> in <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/lib/CMakeLists.txt#L18 target=_blank rel="noopener noreferrer">this line
</a>, the test target will link against libtorch automatically as long as it links against <code>libPmppTorchOps</code>, see <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/test/CMakeLists.txt#L10 target=_blank rel="noopener noreferrer">this line
</a>.</p><blockquote><p>üìù<strong>NOTE</strong><br>You may be confused about why <code>-Wl,--no-as-needed</code> is added before <code>${PROJECT_NAMESPACE}pmpp-torch-ops</code>. This is because the shared libraries are not directly used in the test target (an operator is register in the library but not called directly in the executable), and the linker will not link against them by default. This flag will force the linker to link against the shared libraries even if they are not directly used.</p></blockquote><p>The registered operators can be dispatched in a not-so-intuitional way ü§£ based on the official documentation, see <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/csrc/test/OpTest/vecAdd.cpp#L14-L17 target=_blank rel="noopener noreferrer">here
</a>.</p><p>Now the only thing is to test the operators in C++ using gtest, but this is not the focus of this blog. So let&rsquo;s move on to the next part.</p><h2 id=3-create-and-package-a-python-project>3. Create and Package a Python Project<a hidden class=anchor aria-hidden=true href=#3-create-and-package-a-python-project>#</a></h2><h3 id=31-pyprojecttoml-and-setuppy>3.1. <code>pyproject.toml</code> and <code>setup.py</code><a hidden class=anchor aria-hidden=true href=#31-pyprojecttoml-and-setuppy>#</a></h3><p>In modern python, pyproject.toml is a de-facto standard configuration file for packaging, and in this project, setuptools is used as the build backend because I believe it is the most popular one and is easy to cooperate with cmake.</p><p>Particularly, &ldquo;<a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/pyproject.toml target=_blank rel="noopener noreferrer">
./pyproject.toml
</a>&rdquo; and &ldquo;<a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/setup.py target=_blank rel="noopener noreferrer">
./setup.py
</a>&rdquo; defines what will happen when you run <code>pip install .</code> in the root directory of the project. I created <code>CMakeExtention</code> and <code>CMakeBuild</code> (<a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/setup.py#L23-L68 target=_blank rel="noopener noreferrer">
here
</a>) and pass them to <code>setup</code> function (<a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/setup.py#L92-L105 target=_blank rel="noopener noreferrer">
here
</a>) so that the C++ library <code>libPmppTorchOps</code> (under &ldquo;./csrc/&rdquo;) will be built and installed before installing the python package.</p><p>You can easily understand what I did by reading the source code of these two files, and there is one more thing I want to mention.</p><p>Based on <a href=/blogs/create-a-libtorch-project/#2-create-a-c-cuda-and-libtorch-project>2. Create a C++, CUDA and LibTorch Project</a>, you should find that the generated shared library is under <code>./build/lib</code> ending with <code>.so</code> on linux or <code>.dll</code> on windows. Additionally, I added an install procedure <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/setup.py#L62-L68 target=_blank rel="noopener noreferrer">here
</a>which will copy the shared libraries to &ldquo;./src/pmpp/_torch_ops&rdquo;.</p><blockquote><p>Note that &ldquo;<a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/tree/96685ab/src/pmpp target=_blank rel="noopener noreferrer">
./src/pmpp
</a>&rdquo; is already an existing directory being the root of the actual python package, and &ldquo;./src/pmpp/_torch_ops&rdquo; will be created automatically while installing the shared libraries.</p></blockquote><p>The problem is, when packaging the python project, only the directory containing &ldquo;__init__.py&rdquo; will be considered as a package (or module), and I don&rsquo;t want to add this file to &ldquo;./src/pmpp/_torch_ops&rdquo; due to my mysophobia üò∑. Therefore, I used <code>find_namespace_packages</code> instead of <code>find_packages</code> and specified <code>package_data</code> to include the shared libraries <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/setup.py#L106-L108 target=_blank rel="noopener noreferrer">here
</a>.</p><h3 id=32-install-the-python-package>3.2. Install the Python Package<a hidden class=anchor aria-hidden=true href=#32-install-the-python-package>#</a></h3><p>If you are planning to build your libraries with dependencies listed <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/pyproject.toml#L26-L31 target=_blank rel="noopener noreferrer">here
</a>while installing the python project, I don&rsquo;t really suggest installing it in an isolated python environment (which is the default behavior of setuptools). All packages listed <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/pyproject.toml#L2 target=_blank rel="noopener noreferrer">here
</a>have to be re-installed and in our case you need to at least append <code>torch</code> to that list.</p><p>Alternatively, try this command, which will directly use the torch installed in current conda environment:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install --no-build-isolation -v .
</span></span></code></pre></div><h3 id=33-test-the-custom-torch-operators-in-python>3.3. Test the Custom Torch Operators in Python<a hidden class=anchor aria-hidden=true href=#33-test-the-custom-torch-operators-in-python>#</a></h3><p>As long as you have the shared libraries built in <a href=#2-create-a-c-cuda-and-libtorch-project>2. Create a C++, CUDA and LibTorch Project
</a>, all you need to do is to use <code>torch.ops.load_library</code> to load the shared libraries and call the registered operators.</p><p>I write this process into &ldquo;<a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/src/pmpp/__init__.py target=_blank rel="noopener noreferrer">
src/pmpp/__init__.py
</a>&rdquo;, so the time you import <code>pmpp</code> in python, your custom torch operators will be ready to use. See <a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/96685ab/test/test.py target=_blank rel="noopener noreferrer">this file
</a>for an example of testing the operators.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://jamesnulliu.github.io/tags/c++/>C++</a></li><li><a href=https://jamesnulliu.github.io/tags/python/>Python</a></li><li><a href=https://jamesnulliu.github.io/tags/pytorch/>Pytorch</a></li><li><a href=https://jamesnulliu.github.io/tags/gtest/>Gtest</a></li></ul><nav class=paginav><a class=prev href=https://jamesnulliu.github.io/blogs/wsl-is-all-you-need/><span class=title>¬´ Prev</span><br><span>WSL is All You Need</span>
</a><a class=next href=https://jamesnulliu.github.io/blogs/my-vimrc/><span class=title>Next ¬ª</span><br><span>My vimrc</span></a></nav></footer><script src=https://giscus.app/client.js data-repo=jamesnulliu/jamesnulliu.github.io data-repo-id=R_kgDOMPCQIw data-category=Announcements data-category-id=DIC_kwDOMPCQI84Cgb2t data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async></script></article></main><footer class=footer><span>¬© 2024-2025 JamesNULLiu</span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>