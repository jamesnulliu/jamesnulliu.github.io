<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>A Brief Talk on Speculative Decoding | 秋水·JamesNULLiu</title><meta name=keywords content="speculative decoding"><meta name=description content="A brief talk on speculative decoding in large language models."><meta name=author content="jamesnulliu"><link rel=canonical href=https://jamesnulliu.github.io/blogs/a-brief-talk-on-speculative-decoding/><link crossorigin=anonymous href=/assets/css/stylesheet.62cb9c488bb33c0e9a9d3c29b7f4259cbb0db25aaa19ba672188203d3d5bcaf9.css integrity="sha256-YsucSIuzPA6anTwpt/QlnLsNslqqGbpnIYggPT1byvk=" rel="preload stylesheet" as=style><link rel=icon href=https://jamesnulliu.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://jamesnulliu.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://jamesnulliu.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://jamesnulliu.github.io/apple-touch-icon.png><link rel=mask-icon href=https://jamesnulliu.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://jamesnulliu.github.io/blogs/a-brief-talk-on-speculative-decoding/><link rel=alternate hreflang=zh href=https://jamesnulliu.github.io/zh/blogs/a-brief-talk-on-speculative-decoding/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]]}}</script><meta property="og:url" content="https://jamesnulliu.github.io/blogs/a-brief-talk-on-speculative-decoding/"><meta property="og:site_name" content="秋水·JamesNULLiu"><meta property="og:title" content="A Brief Talk on Speculative Decoding"><meta property="og:description" content="A brief talk on speculative decoding in large language models."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2025-02-21T01:14:06+08:00"><meta property="article:modified_time" content="2025-09-12T22:46:13+00:00"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="Llm"><meta property="article:tag" content="Vllm"><meta name=twitter:card content="summary"><meta name=twitter:title content="A Brief Talk on Speculative Decoding"><meta name=twitter:description content="A brief talk on speculative decoding in large language models."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://jamesnulliu.github.io/blogs/"},{"@type":"ListItem","position":2,"name":"A Brief Talk on Speculative Decoding","item":"https://jamesnulliu.github.io/blogs/a-brief-talk-on-speculative-decoding/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A Brief Talk on Speculative Decoding","name":"A Brief Talk on Speculative Decoding","description":"A brief talk on speculative decoding in large language models.","keywords":["speculative decoding"],"articleBody":" 1. Introduction to Speculative Decoding Given a score model S (for example, LLAMA-3-70B) and a draft model D (for example, LLAMA-3-7B), the process of speculative decoding can be described as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 input_ids = Tensor(...) # (seq_len,) while True: # D generates tokens[seq_len, ..., seq_len + k] draft_outputs = D(input_ids) # (k,) # Given tokens[seq_len - 1, ..., seq_len + k], S generates real # prediction for tokens[seq_len, ..., seq_len + k, seq_len + k + 1] with # one forward pass. score_outputs = S(cat(input_ids, draft_outputs)) # (k + 1,) i = 0 for i in range(k): if not verify(draft_outputs[i], score_outputs[i]): break input_ids.append(draft_outputs[i]) input_ids.append(score_outputs[i]) Speculative decoding workflow in vLLM (k=3, top-p=1). k=3 indicates that the draft model generates 3 tokens per forward pass, and top-p=1 means that for each token, only 1 candidate is proposed. As shown in the picture, at prefill statge, input sequence would first be fed into both draft and score models to acquire kv caches. The output of draft model at this stage is omitted. Then, T5 is fed into draft model to generate proposed T6', T7', and T8'. To verify these tokens, T5, T6', T7' and T8' are fed into the score model to get T6, T7*, T8* and T9* in one forward pass. Note that here T6 must be correct because it is generated by T5 through the score model; However, T7*, T8* and T9* are not guaranteed to be correct. The final step is to verify T6', T7' and T8' to see if T7*, T8* and T9* are correct. For example, if T6' and T7' is correct, then the final accepted tokens would be T6', T7' and T8', which means the socore model generates 3 tokens in one forward pass. Workflow of spuculative decoing in vLLM (k=1, top-p=1). Like the previous picture, if T6' is correct, then the final accepted tokens would be T6' and T7*, one generated by the draft model and the other by the score model. The score model generates 2 tokens in one forward pass. 2. How Speculative Decoding Works in vLLM In vLLM, speculative decoding is integrated with the system’s continuous batching architecture, where different requests are processed together in a single batch, enabling higher throughput. vLLM uses two key components to implement this:\nDraft Runner: This runner is responsible for executing the smaller proposer model to propose candidate tokens. Target Runner: The target runner verifies the tokens by running the larger scorer model. vLLM’s system is optimized to handle this process efficiently, allowing speculative decoding to work seamlessly with continuous batching, which increases the overall system performance.\nDiagram illustrating how the draft and target runners interact within the vLLM batching system. To implement speculative decoding in vLLM, two crucial components had to be modified:\nScheduler: The scheduler was adjusted to handle multiple token slots within a single forward pass, enabling the simultaneous generation and verification of several tokens. Memory Manager: The memory manager now handles the KV cache for both the draft and scorer models, ensuring smooth processing during speculative decoding. System architecture of speculative decoding in vLLM. 3. Types of Speculative Decoding Supported in vLLM 3.1. Draft Model-Based Speculative Decoding This is the most commonly used form of speculative decoding, where a smaller model predicts the next tokens, and a larger model verifies them. A common example would be using a Llama 68M model to predict tokens for a Llama 2 70B model. This approach requires careful selection of the draft model to balance accuracy and overhead.\nChoosing the correct draft model is essential for maximizing the efficiency of speculative decoding. The draft model needs to be small enough to avoid creating significant overhead but still accurate enough to provide a meaningful performance boost.\nHowever, selecting the right draft model can be challenging. For example, in models like Llama 3, finding a suitable draft model is difficult due to differences in vocabulary size. Speculative decoding requires that the draft and target models share the same vocabulary, and in some cases, this can limit the use of speculative decoding. Therefore, in the following sections, we introduce several draft-model free speculative decoding methods.\n3.2. Prompt Lookup Decoding An example of prompt lookup decoding. Given the prompt, we build all 2-grams as the lookup key. The values are the three tokens following the lookup key. During generation, we will check if the current 2-gram matches any key. If so, we will propose the following tokens with the value. Otherwise known as n-gram matching, this approach is effective for use cases like summarization and question-answering, where there is a significant overlap between the prompt and the answer. Instead of using a small model to propose tokens, the system speculates based on the information already available in the prompt. This works particularly well when the large model repeats parts of the prompt in its answers.\n4. MEDUSA 4.1. Roadmap [vllm][ISSUE] | Can vLLM support medusa head? #1023 [vllm][ISSUE] | [Discussion] Will vLLM consider using Speculative Sampling to accelerating LLM decoding? #1171 [vllm][PR] | [Speculative Decoding] Medusa Implementation with Top-1 proposer #4978 4.1. MEDUSA Heads MEDUSA heads are additional decoding heads appended to the last hidden states of the original model.\nThree heads are used to propose tokens for the following three positions. Head 1 is proposing [\"is\", \"\\'\", \"the\"] for the first position. Head 2 is proposing [\"difficult\", \"is\", \"\\'\"] for the second position. Head 3 is proposing [\"not\", \"difficult\", \"a\"] for the third position. NOTE: All heads take the output of the last transformer block as the input. Specifically, given the original model’s last hidden states $h_t$ at position $t$, we add $K$ decoding heads to $h_t$. The $k$-th head is used to predict the token in the $(t + k + 1)$-th position of the next tokens (the original language model head is used to predict the $(t + 1)$-th position).\n$$ \\begin{aligned} p_{t}^{(k)} \u0026 =\\mathrm{softmax}\\left(W_{2}^{(k)}\\cdot\\left(\\mathrm{SiLU}(W_{1}^{(k)}\\cdot h_{t})+h_{t}\\right)\\right), \\\\ \u0026 \\mathrm{where~}W_{2}^{(k)}\\in\\mathbb{R}^{d\\times V},W_{1}^{(k)}\\in\\mathbb{R}^{d\\times d}. \\end{aligned} $$Unlike a draft model, MEDUSA heads are trained in conjunction with the original backbone model, which can remain frozen during training (MEDUSA-1) or be trained together (MEDUSA-2).\n4.2. Tree Attention The top-2 predictions from the first MEDUSA head and the top-3 from the second result in a total of $2 \\times 3 = 6$ candidates. Each of these candidates corresponds to a distinct branch within the tree structure.\nTo guarantee that each token only accesses its predecessors, an attention mask is devised that exclusively permits attention flow from the current token back to its antecedent tokens.\n5. EAGLE 5.1. Roadmap [vllm][PR] | [Speculative Decoding] EAGLE Implementation with Top-1 proposer #6830 5.2. Detailed Process A comparison of the methods for drafting the fourth and fifth tokens, t4 and t5. t (represented by blue blocks) denotes tokens, and f (orange blocks) signifies the features, with subscripts indicating their positions in the sequence. The red border indicates the predictions of the draft model. For simplicity, the n in the n-gram for Lookahead, as shown in the figure, has been set to 2. This link is a Feishu drawboard to show the detailed process of speculative decoding with EAGLE in vLLM:\nSpeculative Decoding with EAGLE in vLLM 6. DeepseekMTP Structure of DeepseekMTP. This figure also demonstrates the training process of draft models, which are fed with continuous tokens and corresponding masks to predict the next tokens for each position. This process is similar to the pre-training process of the larger scorer model. Compute graph of DeepseekMTP. 7. Discussion 7.1. Performance Insights, Speedups, and Trade-offs Ref: [vllm] | How Speculative Decoding Boosts vLLM Performance by up to 2.8x Speculative decoding offers significant performance benefits in low-QPS (queries per second) environments. For example, in testing on the ShareGPT dataset, vLLM demonstrated up to a 1.5x speedup in token generation when using draft model-based speculative decoding. Similarly, prompt lookup decoding has shown speedups of up to 2.8x when applied to summarization datasets, such as CNN/DailyMail.\nPerformance comparison showing spec decode delivering up to 1.5x Speedup at QPS=1 Llama3-70B on ShareGPT with 4xH100 using draft model (turboderp/Qwama-0.5B-Instruct) and up to 2.8x Speedup at QPS=1 Llama3-70B on CNN Dailymail with 4xH100 using n-grams. However, in high-QPS environments, speculative decoding may introduce performance trade-offs. The extra compute required to propose and verify tokens can sometimes slow down the system when it is already compute-bound, as seen when the number of requests per second increases. In such cases, the overhead of speculative decoding can outweigh its benefits, leading to reduced performance.\nAs high QPS, we see 1.4x slowdown Llama3-70B on ShareGPT with 4xH100, 1.8x slowdown Llama3-70B on CNN Dailymail with 4xH100 7.2. Why exactly is batch expansion inefficient? Ref: Optimizing attention for spec decode can reduce latency / increase throughput Looking at Llama2 architecture, each component has the following algorithmic complexity wrt speculative tokens and sequence length. The baseline is non-speculative decoding, so factors such as d_model are ignored as they are the same in either case.\nEach of these scales linearly with number of speculative tokens, except for attention, which scales by num_spec_tokens * seq_len. This means that for large batch sizes and/or large speculative trees and/or large sequence lengths, attention will be the computational bottleneck.\nTo optimize the attention operation, the key is that components of the attention operation are duplicated when scoring different speculative tokens given the same prefix sequence:\nSpeaking theoretically, we can optimize attention for speculative scoring by reducing redundant QK^T computations + loads and Softmax(...)V loads:\nShare K loads for common tokens Share K*Q compute for common tokens Share V loads for common tokens We should experimentally verify this analysis: one weakness is that Softmax(...)V computation is still O(num_spec_tokens * seq_len).\nReferences [vllm] | Speculative Decoding [vllm] | How Speculative Decoding Boosts vLLM Performance by up to 2.8x [vllm] | How to Use Speculative Decoding in vLLM . [vllm][PR] | [Speculative Decoding] Medusa Implementation with Top-1 proposer #4978 A Hitchhiker's Guide to Speculative Decoding [vllm] | What is lookahead scheduling in vLLM? Optimizing attention for spec decode can reduce latency / increase throughput [vllm][ISSUE] | [RFC]: Automate Speculative Decoding #4565 [HF] | Faster Assisted Generation with Dynamic Speculation ","wordCount":"1691","inLanguage":"en","datePublished":"2025-02-21T01:14:06+08:00","dateModified":"2025-09-12T22:46:13Z","author":[{"@type":"Person","name":"jamesnulliu"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://jamesnulliu.github.io/blogs/a-brief-talk-on-speculative-decoding/"},"publisher":{"@type":"Organization","name":"秋水·JamesNULLiu","logo":{"@type":"ImageObject","url":"https://jamesnulliu.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://jamesnulliu.github.io/ accesskey=h title="秋水·JamesNULLiu (Alt + H)">秋水·JamesNULLiu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://jamesnulliu.github.io/zh/ title=简体中文 aria-label=简体中文>简体中文</a></li></ul></div></div><ul id=menu><li><a href=https://jamesnulliu.github.io/ title=Home><span>Home</span></a></li><li><a href=https://jamesnulliu.github.io/about_me/ title="About Me"><span>About Me</span></a></li><li><a href=https://jamesnulliu.github.io/blogs/ title=Blogs><span>Blogs</span></a></li><li><a href=https://jamesnulliu.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://jamesnulliu.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://jamesnulliu.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://jamesnulliu.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://jamesnulliu.github.io/friends/ title=Friends><span>Friends</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://jamesnulliu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://jamesnulliu.github.io/blogs/>Blogs</a></div><h1 class="post-title entry-hint-parent">A Brief Talk on Speculative Decoding</h1><div class=post-description>A brief talk on speculative decoding in large language models.</div><div class=post-meta><span title='2025-02-21 01:14:06 +0800 +0800'>Feb-21-2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1691 words&nbsp;·&nbsp;jamesnulliu&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://jamesnulliu.github.io/zh/blogs/a-brief-talk-on-speculative-decoding/>简体中文</a></li></ul></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-introduction-to-speculative-decoding aria-label="1. Introduction to Speculative Decoding">1. Introduction to Speculative Decoding</a></li><li><a href=#2-how-speculative-decoding-works-in-vllm aria-label="2. How Speculative Decoding Works in vLLM">2. How Speculative Decoding Works in vLLM</a></li><li><a href=#3-types-of-speculative-decoding-supported-in-vllm aria-label="3. Types of Speculative Decoding Supported in vLLM">3. Types of Speculative Decoding Supported in vLLM</a><ul><li><a href=#31-draft-model-based-speculative-decoding aria-label="3.1. Draft Model-Based Speculative Decoding">3.1. Draft Model-Based Speculative Decoding</a></li><li><a href=#32-prompt-lookup-decoding aria-label="3.2. Prompt Lookup Decoding">3.2. Prompt Lookup Decoding</a></li></ul></li><li><a href=#4-medusa aria-label="4. MEDUSA">4. MEDUSA</a><ul><li><a href=#41-roadmap aria-label="4.1. Roadmap">4.1. Roadmap</a></li><li><a href=#41-medusa-heads aria-label="4.1. MEDUSA Heads">4.1. <strong>MEDUSA Heads</strong></a></li><li><a href=#42-tree-attention aria-label="4.2. Tree Attention">4.2. <strong>Tree Attention</strong></a></li></ul></li><li><a href=#5-eagle aria-label="5. EAGLE">5. EAGLE</a><ul><li><a href=#51-roadmap aria-label="5.1. Roadmap">5.1. Roadmap</a></li><li><a href=#52-detailed-process aria-label="5.2. Detailed Process">5.2. Detailed Process</a></li></ul></li><li><a href=#6-deepseekmtp aria-label="6. DeepseekMTP">6. DeepseekMTP</a></li><li><a href=#7-discussion aria-label="7. Discussion">7. Discussion</a><ul><li><a href=#71-performance-insights-speedups-and-trade-offs aria-label="7.1. Performance Insights, Speedups, and Trade-offs">7.1. Performance Insights, Speedups, and Trade-offs</a></li><li><a href=#72-why-exactly-is-batch-expansion-inefficient aria-label="7.2. Why exactly is batch expansion inefficient?">7.2. Why exactly is batch expansion inefficient?</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{let e=null;const t=window.innerHeight+window.pageYOffset>=document.body.offsetHeight-100;if(t)e=elements[elements.length-1];else{let t=null,n=1/0;elements.forEach(e=>{const s=getOffsetTop(e)-window.pageYOffset;if(s<=window.innerHeight*.3){const o=Math.abs(s);o<n&&(n=o,t=e)}}),e=t||elements[0]}if(e&&e!==activeElement){if(activeElement){const t=encodeURI(activeElement.getAttribute("id")).toLowerCase(),e=document.querySelector(`.inner ul li a[href="#${t}"]`);e&&e.classList.remove("active")}activeElement=e;const n=encodeURI(activeElement.getAttribute("id")).toLowerCase(),t=document.querySelector(`.inner ul li a[href="#${n}"]`);t&&(t.classList.add("active"),document.getElementById("toc-container").classList.contains("wide")&&scrollTocToActiveItem(t))}},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}function scrollTocToActiveItem(e){const t=document.querySelector(".toc .inner");if(!t||!e)return;const n=t.getBoundingClientRect(),s=e.getBoundingClientRect(),o=n.height/2,i=s.top-n.top+t.scrollTop,a=i-o;t.scrollTo({top:Math.max(0,a),behavior:"smooth"})}</script><div class=post-content><iframe src=https://docs.google.com/spreadsheets/d/e/2PACX-1vS8hnf4AA8xVJCNKTACvm4H_Lnu6kXtfB7tdL4Iv90OcsuXBnMs87XaVll4Dz0XhmXbjvbjKIeu8k3r/pubhtml frameborder=0 width=100% height=400></iframe><h2 id=1-introduction-to-speculative-decoding>1. Introduction to Speculative Decoding<a hidden class=anchor aria-hidden=true href=#1-introduction-to-speculative-decoding>#</a></h2><p>Given a score model <code>S</code> (for example, LLAMA-3-70B) and a draft model <code>D</code> (for example, LLAMA-3-7B), the process of <strong>speculative decoding</strong> can be described as follows:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>input_ids</span> <span class=o>=</span> <span class=n>Tensor</span><span class=p>(</span><span class=o>...</span><span class=p>)</span>  <span class=c1># (seq_len,)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>while</span> <span class=kc>True</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># D generates tokens[seq_len, ..., seq_len + k]</span>
</span></span><span class=line><span class=cl>    <span class=n>draft_outputs</span> <span class=o>=</span> <span class=n>D</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>  <span class=c1># (k,)</span>
</span></span><span class=line><span class=cl>    <span class=c1># Given tokens[seq_len - 1, ..., seq_len + k], S generates real </span>
</span></span><span class=line><span class=cl>    <span class=c1># prediction for tokens[seq_len, ..., seq_len + k, seq_len + k + 1] with</span>
</span></span><span class=line><span class=cl>    <span class=c1># one forward pass.</span>
</span></span><span class=line><span class=cl>    <span class=n>score_outputs</span> <span class=o>=</span> <span class=n>S</span><span class=p>(</span><span class=n>cat</span><span class=p>(</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>draft_outputs</span><span class=p>))</span>  <span class=c1># (k + 1,)</span>
</span></span><span class=line><span class=cl>    <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=ow>not</span> <span class=n>verify</span><span class=p>(</span><span class=n>draft_outputs</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>score_outputs</span><span class=p>[</span><span class=n>i</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>            <span class=k>break</span>
</span></span><span class=line><span class=cl>        <span class=n>input_ids</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>draft_outputs</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>input_ids</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>score_outputs</span><span class=p>[</span><span class=n>i</span><span class=p>])</span> 
</span></span></code></pre></td></tr></table></div></div><div class=image-container><img src=/imgs/blogs/a-brief-talk-on-speculative-decoding/sd-workflow-in-vllm-k-3-p-1.png alt class=image width=85%><div class=image-caption>Speculative decoding workflow in vLLM (k=3, top-p=1). k=3 indicates that the draft model generates 3 tokens per forward pass, and top-p=1 means that for each token, only 1 candidate is proposed. As shown in the picture, at prefill statge, input sequence would first be fed into both draft and score models to acquire kv caches. The output of draft model at this stage is omitted. Then, T5 is fed into draft model to generate proposed T6', T7', and T8'. To verify these tokens, T5, T6', T7' and T8' are fed into the score model to get T6, T7*, T8* and T9* in one forward pass. Note that here T6 must be correct because it is generated by T5 through the score model; However, T7*, T8* and T9* are not guaranteed to be correct. The final step is to verify T6', T7' and T8' to see if T7*, T8* and T9* are correct. For example, if T6' and T7' is correct, then the final accepted tokens would be T6', T7' and T8', which means the socore model generates 3 tokens in one forward pass.</div></div><div class=image-container><img src=/imgs/blogs/a-brief-talk-on-speculative-decoding/sd-workflow-in-vllm-k-1-p-1.png alt class=image width=80%><div class=image-caption>Workflow of spuculative decoing in vLLM (k=1, top-p=1). Like the previous picture, if T6' is correct, then the final accepted tokens would be T6' and T7*, one generated by the draft model and the other by the score model. The score model generates 2 tokens in one forward pass.</div></div><h2 id=2-how-speculative-decoding-works-in-vllm>2. How Speculative Decoding Works in vLLM<a hidden class=anchor aria-hidden=true href=#2-how-speculative-decoding-works-in-vllm>#</a></h2><p>In vLLM, speculative decoding is integrated with the system&rsquo;s continuous batching architecture, where different requests are processed together in a single batch, enabling higher throughput. vLLM uses two key components to implement this:</p><ul><li><strong>Draft Runner</strong>: This runner is responsible for executing <strong>the smaller proposer model</strong> to propose candidate tokens.</li><li><strong>Target Runner</strong>: The target runner verifies the tokens by running <strong>the larger scorer model</strong>.</li></ul><p>vLLM&rsquo;s system is optimized to handle this process efficiently, allowing speculative decoding to work seamlessly with continuous batching, which increases the overall system performance.</p><div class=image-container><img src=/imgs/blogs/a-brief-talk-on-speculative-decoding/sd-in-vllm.png alt class=image width><div class=image-caption>Diagram illustrating how the draft and target runners interact within the vLLM batching system.</div></div><p>To implement speculative decoding in vLLM, two crucial components had to be modified:</p><ul><li><strong>Scheduler</strong>: The scheduler was adjusted to handle multiple token slots within a single forward pass, enabling the simultaneous generation and verification of several tokens.</li><li><strong>Memory Manager</strong>: The memory manager now handles the KV cache for both the draft and scorer models, ensuring smooth processing during speculative decoding.</li></ul><div class=image-container><img src=/imgs/blogs/a-brief-talk-on-speculative-decoding/vllm-sd-system-archi.png alt class=image width=80%><div class=image-caption>System architecture of speculative decoding in vLLM.</div></div><h2 id=3-types-of-speculative-decoding-supported-in-vllm>3. Types of Speculative Decoding Supported in vLLM<a hidden class=anchor aria-hidden=true href=#3-types-of-speculative-decoding-supported-in-vllm>#</a></h2><h3 id=31-draft-model-based-speculative-decoding>3.1. Draft Model-Based Speculative Decoding<a hidden class=anchor aria-hidden=true href=#31-draft-model-based-speculative-decoding>#</a></h3><p>This is the most commonly used form of speculative decoding, where a smaller model predicts the next tokens, and a larger model verifies them. A common example would be using a Llama 68M model to predict tokens for a Llama 2 70B model. This approach requires careful selection of the draft model to balance accuracy and overhead.</p><p>Choosing the correct draft model is essential for maximizing the efficiency of speculative decoding. The draft model needs to be small enough to avoid creating significant overhead but still accurate enough to provide a meaningful performance boost.</p><p>However, <strong>selecting the right draft model</strong> can be challenging. For example, in models like Llama 3, finding a suitable draft model is difficult due to differences in vocabulary size. Speculative decoding requires that the draft and target models <strong>share the same vocabulary</strong>, and in some cases, this can limit the use of speculative decoding. Therefore, in the following sections, we introduce several draft-model free speculative decoding methods.</p><h3 id=32-prompt-lookup-decoding>3.2. Prompt Lookup Decoding<a hidden class=anchor aria-hidden=true href=#32-prompt-lookup-decoding>#</a></h3><div class=image-container><img src=/imgs/blogs/a-brief-talk-on-speculative-decoding/prompt-lookup-decoding.png alt class=image width=80%><div class=image-caption>An example of prompt lookup decoding. Given the prompt, we build all 2-grams as the lookup key. The values are the three tokens following the lookup key. During generation, we will check if the current 2-gram matches any key. If so, we will propose the following tokens with the value.</div></div><p>Otherwise known as n-gram matching, this approach is effective for use cases like summarization and question-answering, where there is a significant overlap between the prompt and the answer. Instead of using a small model to propose tokens, the system speculates based on the information already available in the prompt. This works particularly well when the large model repeats parts of the prompt in its answers.</p><h2 id=4-medusa>4. MEDUSA<a hidden class=anchor aria-hidden=true href=#4-medusa>#</a></h2><h3 id=41-roadmap>4.1. Roadmap<a hidden class=anchor aria-hidden=true href=#41-roadmap>#</a></h3><ol><li><a href=https://github.com/vllm-project/vllm/issues/1023 target=_blank rel="noopener noreferrer">[vllm][ISSUE] | Can vLLM support medusa head? #1023</a></li><li><a href=https://github.com/vllm-project/vllm/issues/1171 target=_blank rel="noopener noreferrer">[vllm][ISSUE] | [Discussion] Will vLLM consider using Speculative Sampling to accelerating LLM decoding? #1171</a></li><li><a href=https://github.com/vllm-project/vllm/pull/4978 target=_blank rel="noopener noreferrer">[vllm][PR] | [Speculative Decoding] Medusa Implementation with Top-1 proposer #4978</a></li></ol><h3 id=41-medusa-heads>4.1. <strong>MEDUSA Heads</strong><a hidden class=anchor aria-hidden=true href=#41-medusa-heads>#</a></h3><p>MEDUSA heads are additional decoding heads appended to the last hidden states of the original model.</p><div class=image-container><img src=/imgs/blogs/a-brief-talk-on-speculative-decoding/medusa.png alt class=image width=70%><div class=image-caption>Three heads are used to propose tokens for the following three positions. Head 1 is proposing ["is", "\'", "the"] for the first position. Head 2 is proposing ["difficult", "is", "\'"] for the second position. Head 3 is proposing ["not", "difficult", "a"] for the third position. NOTE: All heads take the output of the last transformer block as the input.</div></div><p>Specifically, given the original model’s last hidden states $h_t$ at position $t$, we add $K$ decoding heads to $h_t$. The $k$-th head is used to predict the token in the $(t + k + 1)$-th position of the next tokens (the original language model head is used to predict the $(t + 1)$-th position).</p>$$
\begin{aligned}
p_{t}^{(k)} & =\mathrm{softmax}\left(W_{2}^{(k)}\cdot\left(\mathrm{SiLU}(W_{1}^{(k)}\cdot h_{t})+h_{t}\right)\right), \\
& \mathrm{where~}W_{2}^{(k)}\in\mathbb{R}^{d\times V},W_{1}^{(k)}\in\mathbb{R}^{d\times d}.
\end{aligned}
$$<p>Unlike a draft model, MEDUSA heads are trained in conjunction with the original backbone model, which can remain frozen during training (MEDUSA-1) or be trained together (MEDUSA-2).</p><h3 id=42-tree-attention>4.2. <strong>Tree Attention</strong><a hidden class=anchor aria-hidden=true href=#42-tree-attention>#</a></h3><div class=image-container><img src=/imgs/blogs/a-brief-talk-on-speculative-decoding/tree-attn.png alt class=image width=70%><div class=image-caption></div></div><p>The top-2 predictions from the first MEDUSA head and the top-3 from the second result in a total of $2 \times 3 = 6$ candidates. Each of these candidates corresponds to a distinct branch within the tree structure.</p><p>To guarantee that each token only accesses its predecessors, an attention mask is devised that exclusively permits attention flow from the current token back to its antecedent tokens.</p><h2 id=5-eagle>5. EAGLE<a hidden class=anchor aria-hidden=true href=#5-eagle>#</a></h2><h3 id=51-roadmap>5.1. Roadmap<a hidden class=anchor aria-hidden=true href=#51-roadmap>#</a></h3><ol><li><a href=https://github.com/vllm-project/vllm/pull/6830 target=_blank rel="noopener noreferrer">[vllm][PR] | [Speculative Decoding] EAGLE Implementation with Top-1 proposer #6830</a></li></ol><h3 id=52-detailed-process>5.2. Detailed Process<a hidden class=anchor aria-hidden=true href=#52-detailed-process>#</a></h3><div class=image-container><img src=/imgs/blogs/a-brief-talk-on-speculative-decoding/eagle-compare.png alt class=image width=80%><div class=image-caption>A comparison of the methods for drafting the fourth and fifth tokens, t4 and t5. t (represented by blue blocks) denotes tokens, and f (orange blocks) signifies the features, with subscripts indicating their positions in the sequence. The red border indicates the predictions of the draft model. For simplicity, the n in the n-gram for Lookahead, as shown in the figure, has been set to 2.</div></div><p>This link is a Feishu drawboard to show the detailed process of speculative decoding with EAGLE in vLLM:</p><ul><li><a href="https://ncnqdau83tum.feishu.cn/docx/PliBdWWPWohaClxAagjcZqcZnMe?from=from_copylink" target=_blank rel="noopener noreferrer">Speculative Decoding with EAGLE in vLLM</a></li></ul><h2 id=6-deepseekmtp>6. DeepseekMTP<a hidden class=anchor aria-hidden=true href=#6-deepseekmtp>#</a></h2><div class=image-container><img src=/imgs/blogs/a-brief-talk-on-speculative-decoding/deepseekmtp-structure.png alt class=image width=90%><div class=image-caption>Structure of DeepseekMTP. This figure also demonstrates the training process of draft models, which are fed with continuous tokens and corresponding masks to predict the next tokens for each position. This process is similar to the pre-training process of the larger scorer model.</div></div><div class=image-container><img src=/imgs/blogs/a-brief-talk-on-speculative-decoding/deepseekmtp-compute-graph.png alt class=image width=100%><div class=image-caption>Compute graph of DeepseekMTP.</div></div><h2 id=7-discussion>7. Discussion<a hidden class=anchor aria-hidden=true href=#7-discussion>#</a></h2><h3 id=71-performance-insights-speedups-and-trade-offs>7.1. Performance Insights, Speedups, and Trade-offs<a hidden class=anchor aria-hidden=true href=#71-performance-insights-speedups-and-trade-offs>#</a></h3><blockquote><p>Ref: <a href=https://blog.vllm.ai/2024/10/17/spec-decode.html#speculative-decoding-performance-insights-speedups-and-trade-offs target=_blank rel="noopener noreferrer">[vllm] | How Speculative Decoding Boosts vLLM Performance by up to 2.8x</a></p></blockquote><p>Speculative decoding offers significant performance benefits in <strong>low-QPS (queries per second)</strong> environments. For example, in testing on the ShareGPT dataset, vLLM demonstrated up to a 1.5x speedup in token generation when using draft model-based speculative decoding. Similarly, prompt lookup decoding has shown speedups of up to 2.8x when applied to summarization datasets, such as CNN/DailyMail.</p><div class=image-container><img src=/imgs/blogs/a-brief-talk-on-speculative-decoding/sd-performance-low-qps.png alt class=image width=75%><div class=image-caption>Performance comparison showing spec decode delivering up to 1.5x Speedup at QPS=1 Llama3-70B on ShareGPT with 4xH100 using draft model (turboderp/Qwama-0.5B-Instruct) and up to 2.8x Speedup at QPS=1 Llama3-70B on CNN Dailymail with 4xH100 using n-grams.</div></div><p>However, in <strong>high-QPS environments</strong>, speculative decoding may introduce performance trade-offs. The extra compute required to propose and verify tokens can sometimes slow down the system when it is already compute-bound, as seen when the number of requests per second increases. In such cases, the overhead of speculative decoding can outweigh its benefits, leading to reduced performance.</p><div class=image-container><img src=/imgs/blogs/a-brief-talk-on-speculative-decoding/sd-performance-high-qps.png alt class=image width><div class=image-caption>As high QPS, we see 1.4x slowdown Llama3-70B on ShareGPT with 4xH100, 1.8x slowdown Llama3-70B on CNN Dailymail with 4xH100</div></div><h3 id=72-why-exactly-is-batch-expansion-inefficient>7.2. Why exactly is batch expansion inefficient?<a hidden class=anchor aria-hidden=true href=#72-why-exactly-is-batch-expansion-inefficient>#</a></h3><blockquote><p>Ref: <a href="https://docs.google.com/document/d/1T-JaS2T1NRfdP51qzqpyakoCXxSXTtORppiwaj5asxA/edit?tab=t.0#heading=h.71imqkdaug8g" target=_blank rel="noopener noreferrer">Optimizing attention for spec decode can reduce latency / increase throughput</a></p></blockquote><p>Looking at Llama2 architecture, each component has the following algorithmic complexity wrt speculative tokens and sequence length. The baseline is non-speculative decoding, so factors such as d_model are ignored as they are the same in either case.</p><div class=image-container><img src=/imgs/blogs/a-brief-talk-on-speculative-decoding/llama2-sd-complexity.png alt class=image width><div class=image-caption></div></div><p>Each of these scales linearly with number of speculative tokens, except for attention, which scales by <code>num_spec_tokens * seq_len</code>. This means that for large batch sizes and/or large speculative trees and/or large sequence lengths, attention will be the computational bottleneck.</p><p>To optimize the attention operation, the key is that components of the attention operation are duplicated when scoring different speculative tokens given the same prefix sequence:</p><div class=image-container><img src=/imgs/blogs/a-brief-talk-on-speculative-decoding/sd-attn-opt.png alt class=image width><div class=image-caption></div></div><p>Speaking theoretically, we can optimize attention for speculative scoring by reducing redundant <code>QK^T</code> computations + loads and <code>Softmax(...)V</code> loads:</p><ul><li>Share K loads for common tokens</li><li>Share K*Q compute for common tokens</li><li>Share V loads for common tokens</li></ul><p>We should experimentally verify this analysis: one weakness is that <code>Softmax(...)V</code> computation is still <code>O(num_spec_tokens * seq_len)</code>.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><a href=https://docs.vllm.ai/en/latest/features/spec_decode.html target=_blank rel="noopener noreferrer">[vllm] | Speculative Decoding</a></li><li><a href=https://blog.vllm.ai/2024/10/17/spec-decode.html target=_blank rel="noopener noreferrer">[vllm] | How Speculative Decoding Boosts vLLM Performance by up to 2.8x</a></li><li><a href=https://blog.vllm.ai/2024/10/17/spec-decode.html#how-to-use-speculative-decoding-in-vllm target=_blank rel="noopener noreferrer">[vllm] | How to Use Speculative Decoding in vLLM
</a>.</li><li><a href=https://github.com/vllm-project/vllm/pull/4978 target=_blank rel="noopener noreferrer">[vllm][PR] | [Speculative Decoding] Medusa Implementation with Top-1 proposer #4978</a></li><li><a href=https://pytorch.org/blog/hitchhikers-guide-speculative-decoding target=_blank rel="noopener noreferrer">A Hitchhiker's Guide to Speculative Decoding</a></li><li><a href="https://docs.google.com/document/d/1Z9TvqzzBPnh5WHcRwjvK2UEeFeq5zMZb5mFE8jR0HCs/edit?tab=t.0#heading=h.1fjfb0donq5a" target=_blank rel="noopener noreferrer">[vllm] | What is lookahead scheduling in vLLM?</a></li><li><a href="https://docs.google.com/document/d/1T-JaS2T1NRfdP51qzqpyakoCXxSXTtORppiwaj5asxA/edit?tab=t.0#heading=h.kk7dq05lc6q8" target=_blank rel="noopener noreferrer">Optimizing attention for spec decode can reduce latency / increase throughput</a></li><li><a href=https://github.com/vllm-project/vllm/issues/4565 target=_blank rel="noopener noreferrer">[vllm][ISSUE] | [RFC]: Automate Speculative Decoding #4565</a></li><li><a href=https://huggingface.co/blog/dynamic_speculation_lookahead target=_blank rel="noopener noreferrer">[HF] | Faster Assisted Generation with Dynamic Speculation</a></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://jamesnulliu.github.io/tags/transformer/>Transformer</a></li><li><a href=https://jamesnulliu.github.io/tags/llm/>Llm</a></li><li><a href=https://jamesnulliu.github.io/tags/vllm/>Vllm</a></li></ul><nav class=paginav><a class=prev href=https://jamesnulliu.github.io/blogs/arithmetic-intensity-estimation-of-large-language-models/><span class=title>« Prev</span><br><span>Arithmetic Intensity Estimation of Large Language Models</span>
</a><a class=next href=https://jamesnulliu.github.io/blogs/wsl-is-all-you-need/><span class=title>Next »</span><br><span>WSL is All You Need</span></a></nav></footer><script src=https://giscus.app/client.js data-repo=jamesnulliu/jamesnulliu.github.io data-repo-id=R_kgDOMPCQIw data-category=Announcements data-category-id=DIC_kwDOMPCQI84Cgb2t data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async></script></article></main><footer class=footer><span>© 2024-2025 JamesNULLiu</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>