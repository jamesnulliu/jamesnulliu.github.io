<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Dive into Paged Attention | 秋水·JamesNULLiu</title><meta name=keywords content="vllm,continuous-batching,paged-attention"><meta name=description content="Dive into the paged attention mechanism of vLLM."><meta name=author content="jamesnulliu"><link rel=canonical href=https://jamesnulliu.github.io/blogs/dive-into-paged-attention/><link crossorigin=anonymous href=/assets/css/stylesheet.62cb9c488bb33c0e9a9d3c29b7f4259cbb0db25aaa19ba672188203d3d5bcaf9.css integrity="sha256-YsucSIuzPA6anTwpt/QlnLsNslqqGbpnIYggPT1byvk=" rel="preload stylesheet" as=style><link rel=icon href=https://jamesnulliu.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://jamesnulliu.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://jamesnulliu.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://jamesnulliu.github.io/apple-touch-icon.png><link rel=mask-icon href=https://jamesnulliu.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://jamesnulliu.github.io/blogs/dive-into-paged-attention/><link rel=alternate hreflang=zh href=https://jamesnulliu.github.io/zh/blogs/dive-into-paged-attention/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]]}}</script><meta property="og:url" content="https://jamesnulliu.github.io/blogs/dive-into-paged-attention/"><meta property="og:site_name" content="秋水·JamesNULLiu"><meta property="og:title" content="Dive into Paged Attention"><meta property="og:description" content="Dive into the paged attention mechanism of vLLM."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2024-10-07T12:00:00+08:00"><meta property="article:modified_time" content="2025-09-12T22:46:13+00:00"><meta property="article:tag" content="Python"><meta property="article:tag" content="Vllm"><meta property="article:tag" content="Attention"><meta name=twitter:card content="summary"><meta name=twitter:title content="Dive into Paged Attention"><meta name=twitter:description content="Dive into the paged attention mechanism of vLLM."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://jamesnulliu.github.io/blogs/"},{"@type":"ListItem","position":2,"name":"Dive into Paged Attention","item":"https://jamesnulliu.github.io/blogs/dive-into-paged-attention/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Dive into Paged Attention","name":"Dive into Paged Attention","description":"Dive into the paged attention mechanism of vLLM.","keywords":["vllm","continuous-batching","paged-attention"],"articleBody":"1. Why Attention’s $O_i$ only depends on $Q_i$ The Attention formula is:\n$$ O=Attention(Q,K,V)=softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$Assume $Q=\\begin{bmatrix}Q_0\\\\Q_1\\end{bmatrix}$, $K=\\begin{bmatrix}K_0\\\\K_1\\end{bmatrix}$\nThen:\n$$ O=softmax(\\frac{\\begin{bmatrix}Q_0K_0^T\u0026Q_0K_1^T\\\\Q_1K_0^T\u0026Q_1K_1^T\\end{bmatrix}}{\\sqrt{d_k}})V $$Let:\n$$ A=\\begin{bmatrix}A_0\\\\A_1\\end{bmatrix}=\\begin{bmatrix}Q_0K_0^T\u0026Q_0K_1^T\\\\Q_1K_0^T\u0026Q_1K_1^T\\end{bmatrix},f(x)=\\frac{softmax(x)}{\\sqrt{d_k}} $$At this point, $A_1$ only depends on $Q_1$ and is independent of $Q_0$, so:\n$$ \\begin{bmatrix}O_0\\\\O_1\\end{bmatrix}=O=\\begin{bmatrix}f(A_0)\\\\f(A_1)\\end{bmatrix}V=\\begin{bmatrix}f(A_0)V\\\\f(A_1)V\\end{bmatrix} $$Therefore, $O_i$ only depends on $A_i$, and according to the definition of $A$, $A_i$ only depends on $Q_i$, meaning:\nThe $i$-th output of the Attention matrix only depends on the $i$-th $Q$ and is independent of previous $Q$s.\nSummary:\nWhen predicting the next token, we only need to calculate the corresponding Q_new for the new token and perform attention calculation with the previously cached K_cache and V_cache. The new K_new and V_new will be added to the cache to provide the foundation for the next token generation. This process avoids repeated calculations for all historical tokens, greatly improving efficiency. 2. KV Cache Incremental Process Example code:\n“Learning-Programming-Massively-Parallel-Processors/src/pmpp/models/attention.py”\n2.1. Prefilling: Initial Input (Complete Sequence) Calculation For the initial input sequence (seq_len, vocab_size), we obtain Q, K, and V through linear transformations, all with shape (seq_len, embed_dim) (see this). Using Q and K to calculate attention scores through dot product, then combining with V to compute the output (seq_len, embed_dim) (see this), this is the first complete calculation for the initial sequence. 2.2. Decoding: Incremental Calculation When Predicting Next Token: When predicting the next token, there’s no need to perform complete Q, K, V calculations for the entire sequence. Instead, only an incremental calculation for the newly generated token is required. The process is as follows:\nInput New Token: Take the generated token from last round as input sequence, obtain Q_new, K_new and V_new through linear transformation. Update KV Cache: K_new and V_new are added to the end of K_cache and V_cache, making them a pair of (kv_len + 1, embed_dim) vectors. Attention Calculation with Updated K_cache and V_cache: Use Q_new to perform attention calculation with updated K_cache and V_cache. Q_new can directly perform dot product with K_cache to get attention scores, then combine with V_cache to get new output. Output: The output after attention calculation has shape (1, embed_dim), which is the newly generated token. 3. Paged Attention in vllm 3.1. Motivation: Memory Wastes The above figure shows possible memory waste scenarios. The main issue is that we don’t know where the EOS (end of sequence) token is. Random memory allocation may lead to significant memory fragmentation, resulting in reduced throughput.\n3.2. Solution: Managing Caches with Pages The above figure demonstrates how vLLM manages memory using Paged Attention.\nIn simple terms, before inference begins, vLLM allocates two long Tensors (k_cache and v_cache) for each Decoder Layer, dividing these Tensors into continuous equal-length PA blocks (each row in the figure represents one PA Block). Each PA Block can store K or V cache for BLOCK_SIZE tokens (each token’s shape can be recognized as (num_heads, head_size)).\nTherefore, the shapes of k_cache and v_cache can be recognized as (num_blocks, block_size, num_heads, head_size).\nFor a continuous sequence, PA blocks are allocated before the prefilling stage, and during inference:\nWhen computing prompt attention, the input K and V are first stored in k_cache and v_cache according to PA blocks; then attention is calculated using the entire QKV. When computing new tokens, Q and the block table are used to calculate attention during the decode phase; at this point, the memory access is to the PA blocks in k_cache and v_cache. 5. Paged Attention Kernel in Details References:\nvLLM Paged Attention vLLM皇冠上的明珠：深入浅出理解PagedAttention CUDA实现 The general structure of the Paged Attention kernel is as follows:\n5.1. 输入输出输出分析和参数说明 // Grid: (num_heads, num_seqs, 1). template\u003c typename scalar_t, int HEAD_SIZE, int BLOCK_SIZE, int NUM_THREADS, int PARTITION_SIZE = 0\u003e __device__ void paged_attention_kernel( ... // Other side args. const scalar_t* __restrict__ out, // [num_seqs, num_heads, max_num_partitions, head_size] const scalar_t* __restrict__ q, // [num_seqs, num_heads, head_size] const scalar_t* __restrict__ k_cache, // [num_blocks, num_kv_heads, head_size/x, block_size, x] const scalar_t* __restrict__ v_cache, // [num_blocks, num_kv_heads, head_size, block_size] ... // Other side args. ) 模板参数说明:\nscalar_t 元素类型 (实际代码中还有 cache_t 表示 KV cache 的元素类型). HEAD_SIZE 每个 head 中元素数量. BLOCK_SIZE 每个 PA block 中的 token 数量. KV cache 被存储在不同 PA blocks. 每个 PA block 存储一个 head 中 BLOCK_SIZE 个 token.\n例如, 若 BLOCK_SIZE=16, HEAD_SIZE=128, 则一个 PA block 能存储一个 head 的 16 * 128 = 2048 个元素. 每个 PA block 可能只包含一部分的 context tokens. 从 page 角度看, KV cache 是若干个 page 的集合; NUM_THREADS 每个 CUDA thread block 中 thread 的数量. PARTITION_SIZE 参与 TP 的 GPU 数量, 默认 0 表示单卡. (以下都以单卡为例说明) 额外的一些参数:\nnum_seqs: 本次推理请求 sequence 数目. 由于这个 kernel 只处理 decode 阶段单 query attention, 所以实际上每个 sequence 只有一个 query token.\nnum_heads: Q 的 head 数目 num_kv_heads: KV 的 head 数目, 对于 MHA 其值和 num_heads 相同; 如果是 GQA, MQA 则 num_kv_heads 小于 num_head. head_size: 即 HEAD_SIZE k_cache: (num_blocks, num_kv_heads, head_size/x, block_size, x), 其中 x 表示 THREAD_GROUP_SIZE * VEC_SIZE 的大小 (后面会细说). 下面结合 GPU architecture 初步分析一下参数.\n🧐 为什么要分 thread group?\n因为当一个 cuda block 要取的数据比较少的时候 (计算 QK), 一个 thread group 分别一次取 Q 和 K 中 16B; 当一个 cuda block 要取的数据比较多的时候 (计算 LV), 一个 thread 取 16B. 5.2.Shared Memory: q_vecs 的写入 从 kernel 中的第一个申请的 shared memory 开始说.\n关于 shared memeory:\n在 kernel 中申请的 shared memory 被当前 cuda block 中的所有 thread 共享. shared memory 的作用是为了减少 global memory 的访问次数，提高访存效率. 以下代码申请了一块 shared memroy 被整个 CUDA Block 中所有 kernel 共享:\n__shared__ Q_vec q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD]; 首先, q_vecs 覆盖了 Q 中 head_size 个元素 - 这也是一个 cuda block 需要处理的数据量.\n接着再说两个维度的参数的意思:\nconstexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE; constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE; THREAD_GROUP_SIZE: 每个 thread group 中的 thread 数量. 注意, 一个 cuda block 中有 NUM_THREADS 个 thread, NUM_THREAD_GROUPS 个 thread group. THREAD_GROUP_SIZE = MAX(WARP_SIZE/BLOCK_SIZE, 1). NUM_VECS_PER_THREAD: HEAD_SIZE 能被分成多少个 16B. (这个变量这么命名的理由是后面读取 K 的时候每个 thread 会往自己的寄存器内读 NUM_VECS_PER_THREAD 个 k_vec.) 证明: q_vecs 覆盖 Q 的一个 head, 并且 NUM_VECS_PER_THREAD 表示 Q 的一个 head 被分成多少个 16B.\n=\u003e THREAD_GROUP_SIZE * VEC_SIZE = 16B / sizeof(scalar_t);\n=\u003e NUM_VECS_PER_THREAD * 16B / sizeof(scalar_t) = HEAD_SIZE;\n然后看 load Q 的代码, 建议结合下面的图一起看:\n// Load Q to shmem #pragma unroll for (int i = thread_group_idx; i \u003c NUM_VECS_PER_THREAD; i += NUM_THREAD_GROUPS) { const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE; q_vecs[thread_group_offset][i] = *reinterpret_cast\u003cconst Q_vec*\u003e(q_ptr + vec_idx * VEC_SIZE); } thread_group_idx 表示当前 thread 属于当前 cuda block 中第几个 thread group. thread_group_offset 表示当前 thread 在当前 thread group 中是第几个 thread. 上图展示了循环具体是怎么跑的.\n一个紫色箭头表示一个 thread group. NUM_VECS_PER_THREAD 表示 HEAD_SIZE 能被分成多少个 16B. 实际读取 Q 的内存时, 所有 thread group 从 Q 的起始位置紧密排列, 根据图上看的话一共有 NUM_THREAD_GROUPS 个紫色箭头. 所有 thread group 读取一次 Q 并存入 q_vecs 对应循环中的一次迭代; 因此下次迭代 thread group 需要向后偏移 NUM_THREAD_GROUPS 个位置 (例如 i 从 1 变为 7). 此外, 读一次 16B 对应一个 thread 来说自然也是取一个 VEC. 对应到 kernel 编写, 还需要计算当前 thread 具体读取哪个 vec; 因此得到 vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE. 🤔 这里会不会有 bank conflict?\n总之现在我们把 (1, head_size) 大小的元素读到了 cuda block 共享的 shared memory q_vecs 中.\n5.3. 读取 K Cache 并计算 QK 现在从 cuda block 的角度看, 当前 block 已经获得了自己要算的 Q 中的一个 head (形状为 (1, head_size)), 接下来就是计算 Q 和 K 的点积.\n点积过程是把当前 block 拥有的 Q head 和整个 K Cache (迭代地) 进行点积运算. 参考下图:\nQK 乘积实际上被暂存在 logits (也是一块 shared memory) 中, 之后会被用来计算 softmax.\n😇 看下循环的具体代码吧:\n// Loop 1 for (int block_idx = start_block_idx + warp_idx; block_idx \u003c end_block_idx; block_idx += NUM_WARPS) { // Physical block calculation ... // Loop 2 for (int i = 0; i \u003c NUM_TOKENS_PER_THREAD_GROUP; i++) { // Offset calculation ... K_vec k_vecs[NUM_VECS_PER_THREAD]; // Loop 3 #pragma unroll for (int j = 0; j \u003c NUM_VECS_PER_THREAD; j++) { // Load K to `k_vecs` ... } float qk = scale * Qk_dot\u003cscalar_t, THREAD_GROUP_SIZE\u003e::dot( q_vecs[thread_group_offset], k_vecs); // Add the ALiBi bias if slopes are given. qk += (alibi_slope != 0) ? alibi_slope * (token_idx - seq_len + 1) : 0; if (thread_group_offset == 0) { // Store the partial reductions to shared memory. // Mask // Update the max value. } } } 先说第一个循环, 其中比较重要的几个参数定义如下:\n// [start_block_idx, end_block_idx) is the range of blocks to process. const int start_block_idx = USE_PARTITIONING ? partition_idx * num_blocks_per_partition : 0; // If not using partitioning, `end_block_idx` should be equal to `num_seq_blocks`. const int end_block_idx = MIN(start_block_idx + num_blocks_per_partition, num_seq_blocks); // Number of blocks to process. const int num_blocks = end_block_idx - start_block_idx; 用文字描述就是:\nblk_idx 表示当前 thread 所在 warp 需要处理的 PA block 的在 block_table 中索引 (逻辑上的索引). start_block_idx 和 end_block_idx 表示当前 cuda block 需要处理的 block 范围. num_blocks 表示当前 cuda block 需要处理的 block 数量. NUM_WARPS 表示当前 cuda block 中 warp 的数量. 一个 warp 包含 32 个 thread. warp_idx 表示当前 warp 在当前 cuda block 中的索引. 说人话就是每个 warp 处理一个 PA block, 一开始 cuda block 中的所有 warp 紧密地指向最前面的 NUM_WARPS 个 PA block, 每次循环所有 warp 向后偏移 NUM_WARPS 个 PA block 的长度. 参考下图:\n🔔 这里再回顾一下, 一个 PA block 里存放了 BLOCK_SIZE 个 token 的 K 或 V cache.\n所以说这个循环和上面读取 Q 的循环一个尿性🤮, 不过是以 warp 的粒度处理数据;\n进入了第一个循环内部, 第一步当然是计算当前 thread 对应的 warp 应该计算哪个 PA block (物理上的索引), 因此得到了 physical_block_number:\nconst int64_t physical_block_number = static_cast\u003cint64_t\u003e(block_table[block_idx]); 然后解释第二个循环, 第二个循环的整体目标就是让当前 warp 计算好自己负责的 PA block 中 BLOCK_SIZE 个 token 的 QK 乘积.\n先看一下 i 的上界:\nconstexpr int NUM_TOKENS_PER_THREAD_GROUP = DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE); // ... // Loop 1 for (int block_idx = start_block_idx + warp_idx; block_idx \u003c end_block_idx; block_idx += NUM_WARPS) { // Loop 2 for (int i = 0; i \u003c NUM_TOKENS_PER_THREAD_GROUP; i++) { // ... } // ... } 从 kernel 角度看, 每个 thread 需要辅助当前 warp 计算自己负责的一整个 PA block (包含 BLOCK_SIZE 个 token), 而我们把这个过程拆分为 Loop 2 中的 NUM_TOKEN_PER_THREAD_GROUP (也就是 ceil(BLOCK_SIZE / 32)) 次循环;\n说人话就是一个 thread group 对应一个 token 中的一个 head, 如果 BLOCK SIZE 太大了后面每个 thread 向后偏移 i * WARP_SIZE 个 token 继续狠狠算🤣.\n也因此第二个循环内部一上来先计算了几个偏移量, 并且申请了 thread 内部私有的 k_vecs 数组:\nconst int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE; const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset; K_vec k_vecs[NUM_VECS_PER_THREAD]; thread_group_idx 表示当前 thread group 在整个 cuda block 中的索引. ☢️ 一个 thread group 在一次循环中负责 fetch 一个 PA block 中 K cache 的一个 token 中自己负责的 head. ☢️ 一个 thread group 负责计算一个 qk 值; 这个值显然是由一个 Q head 和一个 K head 点积得到的. physical_block_offset 表示当前要算的 token 在当前 PA block 中的偏移量 (注意和前面的 physical_block_number 区分). 加 i * WARP_SIZE 的原因是如果 BLOCK_SIZE 大于 32, 那么一个 warp 要多次循环才能处理完一个 PA block 中的所有 token, 对应 thread_group_idx 需要做偏移. token_idx 表示当前要算的 token 在整个 seq 的 KV cache 中的索引. k_vecs 中能存放 NUM_VECS_PER_THREAD 个 VEC, 而一整个 thread group 中所有的 thread 的 k_vecs 合起来才能组成一个 K 的 head (推导参考上面 Q 的 😇). 这就是为什么后面算 QK 的时候要 reduce. 🤔 看到这里读者可能有一个问题: 一个 token 的 K cache 应该对应多个 head, 为什么上面说一个 thread group 只负责一个 head?\n答: 因为实际计算的时候, 一个 cuda block 只负责计算一个 head, 对应到 K Cache 乃至后面 V Cache 的位置也是一样的.\n这里额外说一下, 读 K 的 head 的一个目标应该是在尽量少的 register 中装下一个 head 的所有元素, 这样后续和 shared memory 中的 Q 做点乘并规约的速度更快. 假设一个 head 有 128 个 float16, 则占用 256B, 而 A100 中一个 thread 最多能有 255 个 32-bit register (也就是 1020B), 此时可以认为一个 thread 能装下一个 head 的所有元素.\n但是由于目前 PA kernel 在 BLOCK_SIZE 为 16 的情况下 THREAD_GROUP_SIZE 等于 2, 因此一个 thread 只会装一个 head 的一半元素, 这样可能会导致 register 的使用率不高.\n接着进入第三个循环, 目的是让 thread group 从 K cache 中读一个 head, 并存入 k_vecs 中:\n// x == THREAD_GROUP_SIZE * VEC_SIZE // Each thread group fetches x elements from the key at a time. constexpr int x = 16 / sizeof(cache_t); //... // Loop 1 for (int block_idx = start_block_idx + warp_idx; block_idx \u003c end_block_idx; block_idx += NUM_WARPS) { // Loop 2 for (int i = 0; i \u003c NUM_TOKENS_PER_THREAD_GROUP; i++) { K_vec k_vecs[NUM_VECS_PER_THREAD]; // Loop 3 for (int j = 0; j \u003c NUM_VECS_PER_THREAD; j++) { const cache_t* k_ptr = k_cache + physical_block_number * kv_block_stride + kv_head_idx * kv_head_stride + physical_block_offset * x; const int vec_idx = thread_group_offset + j * THREAD_GROUP_SIZE; const int offset1 = (vec_idx * VEC_SIZE) / x; const int offset2 = (vec_idx * VEC_SIZE) % x; // if Fp8KVCacheDataType::kAuto k_vecs[j] = *reinterpret_cast\u003cconst K_vec*\u003e( k_ptr + offset1 * BLOCK_SIZE * x + offset2); } // ... } // ... } 老规矩, 先看 j, 本质就是从 0 迭代到 NUM_VECS_PER_THREAD, 每次迭代当前 thread 读取一个 VEC 存入 k_vecs 中.\n🔔 回顾:\nNUM_VECS_PER_THREAD 表示一个 head 被分成多少个 16B. k_cache 的 shape 为 (num_blocks, num_kv_heads, head_size/x, block_size, x). 其中的 x 表示一个 thread group 需要读取的元素数量 (VEC_SIZE * THREAD_GROUP_SIZE); 因此作者将 K Cache 的 layout 的最后一维设置为 x 其实也是方便后续 thread group 对 K cache 的读取.\n下图具体展示了寻址的过程:\n其中:\n在 MHSA 中, num_kv_heads 等于 num_heads; 而在 GQA, MQA 中, num_kv_heads 小于 num_heads. (1) 负责找到当前 thread 属于的 warp 要处理哪个 PA block. (2) 负责找到当前 thread 要计算的 head 在 K cache 中的位置. 这个 head 的索引和 Q 中 head 的索引在 MHSA 中相同. (3) 负责找到当前 thread group 要计算的 token 在当前 PA block 中的位置. (5) 负责找到当前 thread 在需要读取的 head (蓝色长方体) 中 x 的偏移, 通过 j 进行迭代读取. 每次循环 thread group 中的所有 thread 取一个 x. (6) 负责找到当前 thread 在 thread gruop 中读取的 x 中 VEC 的偏移; thread 一次读取一个 VEC. 🤔 为什么 (5) 在实际寻址时需要 * BLOCK_SIZE * x ?\n答: 这是根据 k_cache 的 layout 得到的 stride. 同理 (3) * x 也是 stride.\n第 3 个循环结束时当前 warp 负责的每个 token 中需要的 K cache head 已经全被加载入 thread 本地的 k_vecs 中了.\n由于一个 thread group 的 k_vecs 才能真正组成一个 head, 在退回第二个循环进行 QK dot 的时候, 需要做个 reduction, 具体的范围就是 THREAD_GROUP_SIZE 个 thread:\n// Loop 1 for (int block_idx = start_block_idx + warp_idx; block_idx \u003c end_block_idx; block_idx += NUM_WARPS) { // Loop 2 for (int i = 0; i \u003c NUM_TOKENS_PER_THREAD_GROUP; i++) { K_vec k_vecs[NUM_VECS_PER_THREAD]; // Loop 3 for (int j = 0; j \u003c NUM_VECS_PER_THREAD; j++) { // ... } float qk = scale * Qk_dot\u003cscalar_t, THREAD_GROUP_SIZE\u003e::dot( q_vecs[thread_group_offset], k_vecs); } // ... } 计算完 qk 后, 由当前 thread group 中第一个 (offset 为 0) 的 thread 对自己刚才算出来的 qk 进行 mask, 顺便看看如果没有 mask 掉, 把 qk_max 赋值为 qk:\nif (thread_group_offset == 0) { // Store the partial reductions to shared memory. // NOTE(woosuk): It is required to zero out the masked logits. const bool mask = token_idx \u003e= seq_len; logits[token_idx - start_token_idx] = mask ? 0.f : qk; // Update the max value. qk_max = mask ? qk_max : fmaxf(qk_max, qk); } 🧐 为什么要做 mask?\n因为一个 seq 的最后一个 PA block 可能覆盖不满 BLOCK_SIZE 个 token. 这里的 mask 就是把那部分 qk 置零. 5.4. Softmax 我勒个 QK 啊, 总算算完了, 锐克 five 都要被抽清仓了. 页意丁真, 鉴定为开算 softmax.\n主要步骤就是广播然后算, 算 softmax 需要知道每个 head 对应的 qk 的最大值. 由于一个 cuda block 负责的就是一个 head, 对于这个 head 上面的计算步骤一共算了 cache_len个 token 的 qk, 因此需要做一个 cuda block 范围的规约, 找到其中最大的 qk 值.\n先在 warp 层面规约.\n__shared__ float red_smem[2 * NUM_WARPS]; // ... // Perform reduction across the threads in the same warp to get the // max qk value for each \"warp\" (not across the thread block yet). // The 0-th thread of each thread group already has its max qk value. #pragma unroll for (int mask = WARP_SIZE / 2; mask \u003e= THREAD_GROUP_SIZE; mask /= 2) { qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask)); } if (lane == 0) { red_smem[warp_idx] = qk_max; } __syncthreads(); red_smem 是之前申请的 shared memory. VLLM_SHFL_XOR_SYNC 是一个 warp 内的 shuffle 操作, 具体来说, 在每次循环时, 每个 thread 和自己相距 mask 位置的线程交换数据 (交换来的数据通过 fmaxf 比较), 并且 mask 会逐渐减半, 直到 THREAD_GROUP_SIZE 为止. lane 表示当前 warp 中的线程索引. 接着再对每个 warp 的最大值进行规约, 由于每个 warp 的最大值都被存入了 red_smem 中, 所以只需要再次进行 shuffle 操作即可.\n// TODO(woosuk): Refactor this part. // Get the max qk value for the sequence. qk_max = lane \u003c NUM_WARPS ? red_smem[lane] : -FLT_MAX; #pragma unroll for (int mask = NUM_WARPS / 2; mask \u003e= 1; mask /= 2) { qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask)); } 此时, 第 1 个线程的 qk_max 就是当前 cuda block 中所有 warp 中最大的 qk 值. 将其广播给所有线程:\n// Broadcast the max qk value to all threads. qk_max = VLLM_SHFL_SYNC(qk_max, 0); 在获得了 qk_max 后, 就可以计算 softmax 了:\n// Get the sum of the exp values. float exp_sum = 0.f; for (int i = thread_idx; i \u003c num_tokens; i += NUM_THREADS) { float val = __expf(logits[i] - qk_max); logits[i] = val; exp_sum += val; } exp_sum = block_sum\u003cNUM_WARPS\u003e(\u0026red_smem[NUM_WARPS], exp_sum); // Compute softmax. const float inv_sum = __fdividef(1.f, exp_sum + 1e-6f); for (int i = thread_idx; i \u003c num_tokens; i += NUM_THREADS) { logits[i] *= inv_sum; } __syncthreads(); 5.5. LV (Logits * Value) 上图展示了 LV 的计算过程, 主要区别是由于要计算 Logits 的 shape 可以表示为 (num_heads, num_seqs, cache_len), 而 V 的 shape 可以表示为 (num_heads, cache_len, head_size), 因此 LV 的矩阵乘法中, 每计算一个元素需要读取 logits 的一行和 V 的一列进行计算.\n此时, 一个 cuda block 的职责从 “自 Q 中读取一个 head” 转变为 “计算 output 中的一个 head”.\n🧐 为什么在计算 LV 时, 去掉了 thread group 的概念, 每个 thread 都被设定为每次读取 16B?\n因为现在每计算一个元素, 需要的访存量更大, 因此给每个 thread 分配了更多的数据读取量. 也就是说, V_VEC_SIZE 比 VEC_SIZE 更大. 由于 cuda 访存模式按行读取更快, 所以实际的计算结果在遍历 PA block 时线程内部利用 accs 进行累计 (以实现与 V 的一列进行计算的行为):\nconstexpr int NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE; constexpr int NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW; constexpr int NUM_ROWS_PER_THREAD = DIVIDE_ROUND_UP(HEAD_SIZE, NUM_ROWS_PER_ITER); // NOTE(woosuk): We use FP32 for the accumulator for better accuracy. float accs[NUM_ROWS_PER_THREAD]; for (int block_idx = start_block_idx + warp_idx; block_idx \u003c end_block_idx; block_idx += NUM_WARPS) { V_vec v_vec; // ... for (int i = 0; i \u003c NUM_ROWS_PER_THREAD; i++) { // ... for (int j = 0; j \u003c V_VEC_SIZE; j++) { // Load V to `v_vec` ... v_vec_ptr[j] = token_idx + j \u003c seq_len ? v_vec_ptr[j] : zero_value; } // Accumulate the dot product. accs[i] += dot(logits_vec, v_vec); } } 由于每个线程负责的累计部分不满一整行/列, 所以进行规约:\n// Perform reduction within each warp. #pragma unroll for (int i = 0; i \u003c NUM_ROWS_PER_THREAD; i++) { float acc = accs[i]; #pragma unroll for (int mask = NUM_V_VECS_PER_ROW / 2; mask \u003e= 1; mask /= 2) { acc += VLLM_SHFL_XOR_SYNC(acc, mask); } accs[i] = acc; } // NOTE(woosuk): A barrier is required because the shared memory space for // logits is reused for the output. __syncthreads(); // Perform reduction across warps. float* out_smem = reinterpret_cast\u003cfloat*\u003e(shared_mem); #pragma unroll for (int i = NUM_WARPS; i \u003e 1; i /= 2) { int mid = i / 2; // Upper warps write to shared memory. if (warp_idx \u003e= mid \u0026\u0026 warp_idx \u003c i) { float* dst = \u0026out_smem[(warp_idx - mid) * HEAD_SIZE]; #pragma unroll for (int i = 0; i \u003c NUM_ROWS_PER_THREAD; i++) { const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER; if (row_idx \u003c HEAD_SIZE \u0026\u0026 lane % NUM_V_VECS_PER_ROW == 0) { dst[row_idx] = accs[i]; } } } __syncthreads(); // Lower warps update the output. if (warp_idx \u003c mid) { const float* src = \u0026out_smem[warp_idx * HEAD_SIZE]; #pragma unroll for (int i = 0; i \u003c NUM_ROWS_PER_THREAD; i++) { const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER; if (row_idx \u003c HEAD_SIZE \u0026\u0026 lane % NUM_V_VECS_PER_ROW == 0) { accs[i] += src[row_idx]; } } } __syncthreads(); } 最后写入到输出中:\n// Write the final output. if (warp_idx == 0) { scalar_t* out_ptr = out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE + head_idx * max_num_partitions * HEAD_SIZE + partition_idx * HEAD_SIZE; #pragma unroll for (int i = 0; i \u003c NUM_ROWS_PER_THREAD; i++) { const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER; if (row_idx \u003c HEAD_SIZE \u0026\u0026 lane % NUM_V_VECS_PER_ROW == 0) { from_float(*(out_ptr + row_idx), accs[i]); } } } ","wordCount":"5109","inLanguage":"en","datePublished":"2024-10-07T12:00:00+08:00","dateModified":"2025-09-12T22:46:13Z","author":[{"@type":"Person","name":"jamesnulliu"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://jamesnulliu.github.io/blogs/dive-into-paged-attention/"},"publisher":{"@type":"Organization","name":"秋水·JamesNULLiu","logo":{"@type":"ImageObject","url":"https://jamesnulliu.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://jamesnulliu.github.io/ accesskey=h title="秋水·JamesNULLiu (Alt + H)">秋水·JamesNULLiu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://jamesnulliu.github.io/zh/ title=简体中文 aria-label=简体中文>简体中文</a></li></ul></div></div><ul id=menu><li><a href=https://jamesnulliu.github.io/ title=Home><span>Home</span></a></li><li><a href=https://jamesnulliu.github.io/about_me/ title="About Me"><span>About Me</span></a></li><li><a href=https://jamesnulliu.github.io/blogs/ title=Blogs><span>Blogs</span></a></li><li><a href=https://jamesnulliu.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://jamesnulliu.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://jamesnulliu.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://jamesnulliu.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://jamesnulliu.github.io/friends/ title=Friends><span>Friends</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://jamesnulliu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://jamesnulliu.github.io/blogs/>Blogs</a></div><h1 class="post-title entry-hint-parent">Dive into Paged Attention</h1><div class=post-description>Dive into the paged attention mechanism of vLLM.</div><div class=post-meta><span title='2024-10-07 12:00:00 +0800 +0800'>Oct-07-2024</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;5109 words&nbsp;·&nbsp;jamesnulliu&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://jamesnulliu.github.io/zh/blogs/dive-into-paged-attention/>简体中文</a></li></ul></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-why-attentions--only-depends-on aria-label="1. Why Attention&rsquo;s $O_i$ only depends on $Q_i$">1. Why Attention&rsquo;s $O_i$ only depends on $Q_i$</a></li><li><a href=#2-kv-cache-incremental-process aria-label="2. KV Cache Incremental Process">2. KV Cache Incremental Process</a><ul><li><a href=#21-prefilling-initial-input-complete-sequence-calculation aria-label="2.1. Prefilling: Initial Input (Complete Sequence) Calculation">2.1. Prefilling: Initial Input (Complete Sequence) Calculation</a></li><li><a href=#22-decoding-incremental-calculation-when-predicting-next-token aria-label="2.2. Decoding: Incremental Calculation When Predicting Next Token:">2.2. Decoding: Incremental Calculation When Predicting Next Token:</a></li></ul></li><li><a href=#3-paged-attention-in-vllm aria-label="3. Paged Attention in vllm">3. Paged Attention in vllm</a><ul><li><a href=#31-motivation-memory-wastes aria-label="3.1. Motivation: Memory Wastes">3.1. Motivation: Memory Wastes</a></li><li><a href=#32-solution-managing-caches-with-pages aria-label="3.2. Solution: Managing Caches with Pages">3.2. Solution: Managing Caches with Pages</a></li></ul></li><li><a href=#5-paged-attention-kernel-in-details aria-label="5. Paged Attention Kernel in Details">5. Paged Attention Kernel in Details</a><ul><li><a href=#51-%e8%be%93%e5%85%a5%e8%be%93%e5%87%ba%e8%be%93%e5%87%ba%e5%88%86%e6%9e%90%e5%92%8c%e5%8f%82%e6%95%b0%e8%af%b4%e6%98%8e aria-label="5.1. 输入输出输出分析和参数说明">5.1. 输入输出输出分析和参数说明</a></li><li><a href=#52shared-memory-q_vecs-%e7%9a%84%e5%86%99%e5%85%a5 aria-label="5.2.Shared Memory: q_vecs 的写入">5.2.Shared Memory: <code>q_vecs</code> 的写入</a></li><li><a href=#53-%e8%af%bb%e5%8f%96-k-cache-%e5%b9%b6%e8%ae%a1%e7%ae%97-qk aria-label="5.3. 读取 K Cache 并计算 QK">5.3. 读取 K Cache 并计算 QK</a></li><li><a href=#54-softmax aria-label="5.4. Softmax">5.4. Softmax</a></li><li><a href=#55-lv-logits--value aria-label="5.5. LV (Logits * Value)">5.5. LV (Logits * Value)</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{let e=null;const t=window.innerHeight+window.pageYOffset>=document.body.offsetHeight-100;if(t)e=elements[elements.length-1];else{let t=null,n=1/0;elements.forEach(e=>{const s=getOffsetTop(e)-window.pageYOffset;if(s<=window.innerHeight*.3){const o=Math.abs(s);o<n&&(n=o,t=e)}}),e=t||elements[0]}if(e&&e!==activeElement){if(activeElement){const t=encodeURI(activeElement.getAttribute("id")).toLowerCase(),e=document.querySelector(`.inner ul li a[href="#${t}"]`);e&&e.classList.remove("active")}activeElement=e;const n=encodeURI(activeElement.getAttribute("id")).toLowerCase(),t=document.querySelector(`.inner ul li a[href="#${n}"]`);t&&(t.classList.add("active"),document.getElementById("toc-container").classList.contains("wide")&&scrollTocToActiveItem(t))}},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}function scrollTocToActiveItem(e){const t=document.querySelector(".toc .inner");if(!t||!e)return;const n=t.getBoundingClientRect(),s=e.getBoundingClientRect(),o=n.height/2,i=s.top-n.top+t.scrollTop,a=i-o;t.scrollTo({top:Math.max(0,a),behavior:"smooth"})}</script><div class=post-content><h2 id=1-why-attentions--only-depends-on>1. Why Attention&rsquo;s $O_i$ only depends on $Q_i$<a hidden class=anchor aria-hidden=true href=#1-why-attentions--only-depends-on>#</a></h2><p>The Attention formula is:</p>$$
O=Attention(Q,K,V)=softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$<p>Assume $Q=\begin{bmatrix}Q_0\\Q_1\end{bmatrix}$, $K=\begin{bmatrix}K_0\\K_1\end{bmatrix}$</p><p>Then:</p>$$
O=softmax(\frac{\begin{bmatrix}Q_0K_0^T&Q_0K_1^T\\Q_1K_0^T&Q_1K_1^T\end{bmatrix}}{\sqrt{d_k}})V
$$<p>Let:</p>$$
A=\begin{bmatrix}A_0\\A_1\end{bmatrix}=\begin{bmatrix}Q_0K_0^T&Q_0K_1^T\\Q_1K_0^T&Q_1K_1^T\end{bmatrix},f(x)=\frac{softmax(x)}{\sqrt{d_k}}
$$<p>At this point, $A_1$ only depends on $Q_1$ and is independent of $Q_0$, so:</p>$$
\begin{bmatrix}O_0\\O_1\end{bmatrix}=O=\begin{bmatrix}f(A_0)\\f(A_1)\end{bmatrix}V=\begin{bmatrix}f(A_0)V\\f(A_1)V\end{bmatrix}
$$<p>Therefore, $O_i$ only depends on $A_i$, and according to the definition of $A$, $A_i$ only depends on $Q_i$, meaning:</p><p>The $i$-th output of the Attention matrix only depends on the $i$-th $Q$ and is independent of previous $Q$s.</p><p><strong>Summary</strong>:</p><ul><li>When predicting the next token, we only need to calculate the corresponding <code>Q_new</code> for the new token and perform attention calculation with the previously cached <code>K_cache</code> and <code>V_cache</code>.</li><li>The new <code>K_new</code> and <code>V_new</code> will be added to the cache to provide the foundation for the next token generation.</li><li>This process avoids repeated calculations for all historical tokens, greatly improving efficiency.</li></ul><h2 id=2-kv-cache-incremental-process>2. KV Cache Incremental Process<a hidden class=anchor aria-hidden=true href=#2-kv-cache-incremental-process>#</a></h2><p>Example code:</p><p>&ldquo;<a href=https://github.com/jamesnulliu/Learning-Programming-Massively-Parallel-Processors/blob/cf690614d004aa647aefccb8db3eac83255cb99e/src/pmpp/models/attention.py>Learning-Programming-Massively-Parallel-Processors/src/pmpp/models/attention.py</a>&rdquo;</p><h3 id=21-prefilling-initial-input-complete-sequence-calculation>2.1. Prefilling: Initial Input (Complete Sequence) Calculation<a hidden class=anchor aria-hidden=true href=#21-prefilling-initial-input-complete-sequence-calculation>#</a></h3><ul><li>For the initial input sequence <code>(seq_len, vocab_size)</code>, we obtain <code>Q</code>, <code>K</code>, and <code>V</code> through linear transformations, all with shape <code>(seq_len, embed_dim)</code> (<em>see <a href>this</a></em>).</li><li>Using <code>Q</code> and <code>K</code> to calculate attention scores through dot product, then combining with <code>V</code> to compute the output <code>(seq_len, embed_dim)</code> (<em>see <a href>this</a></em>), this is the first complete calculation for the initial sequence.</li></ul><h3 id=22-decoding-incremental-calculation-when-predicting-next-token>2.2. Decoding: Incremental Calculation When Predicting Next Token:<a hidden class=anchor aria-hidden=true href=#22-decoding-incremental-calculation-when-predicting-next-token>#</a></h3><p>When predicting the next token, there&rsquo;s no need to perform complete <code>Q</code>, <code>K</code>, <code>V</code> calculations for the entire sequence. Instead, only an incremental calculation for the newly generated token is required. The process is as follows:</p><ol><li><strong>Input New Token</strong>: Take the generated token from last round as input sequence, obtain <code>Q_new</code>, <code>K_new</code> and <code>V_new</code> through linear transformation.</li><li><strong>Update KV Cache</strong>: <code>K_new</code> and <code>V_new</code> are added to the end of <code>K_cache</code> and <code>V_cache</code>, making them a pair of <code>(kv_len + 1, embed_dim)</code> vectors.</li><li><strong>Attention Calculation with Updated <code>K_cache</code> and <code>V_cache</code></strong>: Use <code>Q_new</code> to perform attention calculation with updated <code>K_cache</code> and <code>V_cache</code>. <code>Q_new</code> can directly perform dot product with <code>K_cache</code> to get attention scores, then combine with <code>V_cache</code> to get new output.</li><li><strong>Output</strong>: The output after attention calculation has shape <code>(1, embed_dim)</code>, which is the newly generated token.</li></ol><h2 id=3-paged-attention-in-vllm>3. Paged Attention in vllm<a hidden class=anchor aria-hidden=true href=#3-paged-attention-in-vllm>#</a></h2><h3 id=31-motivation-memory-wastes>3.1. Motivation: Memory Wastes<a hidden class=anchor aria-hidden=true href=#31-motivation-memory-wastes>#</a></h3><p><img alt=memory-wastes.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/memory-wastes.png></p><p>The above figure shows possible memory waste scenarios. The main issue is that we don&rsquo;t know where the EOS (end of sequence) token is. Random memory allocation may lead to significant memory fragmentation, resulting in reduced throughput.</p><h3 id=32-solution-managing-caches-with-pages>3.2. Solution: Managing Caches with Pages<a hidden class=anchor aria-hidden=true href=#32-solution-managing-caches-with-pages>#</a></h3><p><img alt=paged-attention-animation.webp loading=lazy src=/imgs/blogs/dive-into-paged-attention/paged-attention-animation.webp></p><p>The above figure demonstrates how vLLM manages memory using Paged Attention.</p><p>In simple terms, before inference begins, vLLM allocates two long Tensors (<code>k_cache</code> and <code>v_cache</code>) for each Decoder Layer, dividing these Tensors into continuous equal-length PA blocks (each row in the figure represents one PA Block). Each PA Block can store K or V cache for <code>BLOCK_SIZE</code> tokens (each token&rsquo;s shape can be recognized as <code>(num_heads, head_size)</code>).</p><p>Therefore, the shapes of <code>k_cache</code> and <code>v_cache</code> can be recognized as <code>(num_blocks, block_size, num_heads, head_size)</code>.</p><p>For a continuous sequence, PA blocks are allocated before the prefilling stage, and during inference:</p><ul><li>When computing prompt attention, the input K and V are first stored in <code>k_cache</code> and <code>v_cache</code> according to PA blocks; then attention is calculated using the entire QKV.</li><li>When computing new tokens, Q and the block table are used to calculate attention during the decode phase; at this point, the memory access is to the PA blocks in <code>k_cache</code> and <code>v_cache</code>.</li></ul><h2 id=5-paged-attention-kernel-in-details>5. Paged Attention Kernel in Details<a hidden class=anchor aria-hidden=true href=#5-paged-attention-kernel-in-details>#</a></h2><blockquote><p>References:</p><ul><li><a href=https://docs.vllm.ai/en/latest/dev/kernel/paged_attention.html>vLLM Paged Attention</a></li><li><a href=https://zhuanlan.zhihu.com/p/673284781>vLLM皇冠上的明珠：深入浅出理解PagedAttention CUDA实现</a></li></ul></blockquote><p>The general structure of the Paged Attention kernel is as follows:</p><p><img alt=pa-cal.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/pa-cal.png></p><h3 id=51-输入输出输出分析和参数说明>5.1. 输入输出输出分析和参数说明<a hidden class=anchor aria-hidden=true href=#51-输入输出输出分析和参数说明>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Grid: (num_heads, num_seqs, 1).
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>template</span><span class=o>&lt;</span>
</span></span><span class=line><span class=cl><span class=k>typename</span> <span class=n>scalar_t</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>HEAD_SIZE</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>BLOCK_SIZE</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>NUM_THREADS</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>PARTITION_SIZE</span> <span class=o>=</span> <span class=mi>0</span><span class=o>&gt;</span>
</span></span><span class=line><span class=cl><span class=n>__device__</span> <span class=kt>void</span> <span class=n>paged_attention_kernel</span><span class=p>(</span>
</span></span><span class=line><span class=cl><span class=p>...</span> <span class=c1>// Other side args.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=n>scalar_t</span><span class=o>*</span> <span class=n>__restrict__</span> <span class=n>out</span><span class=p>,</span>       <span class=c1>// [num_seqs, num_heads, max_num_partitions, head_size]
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=n>scalar_t</span><span class=o>*</span> <span class=n>__restrict__</span> <span class=n>q</span><span class=p>,</span>         <span class=c1>// [num_seqs, num_heads, head_size]
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=n>scalar_t</span><span class=o>*</span> <span class=n>__restrict__</span> <span class=n>k_cache</span><span class=p>,</span>   <span class=c1>// [num_blocks, num_kv_heads, head_size/x, block_size, x]
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=n>scalar_t</span><span class=o>*</span> <span class=n>__restrict__</span> <span class=n>v_cache</span><span class=p>,</span>   <span class=c1>// [num_blocks, num_kv_heads, head_size, block_size]
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>...</span> <span class=c1>// Other side args.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>)</span>
</span></span></code></pre></div><p>模板参数说明:</p><ul><li><code>scalar_t</code> 元素类型 (实际代码中还有 <code>cache_t</code> 表示 KV cache 的元素类型).</li><li><code>HEAD_SIZE</code> 每个 head 中元素数量.</li><li><code>BLOCK_SIZE</code> 每个 PA block 中的 token 数量.<blockquote><ol><li>KV cache 被存储在不同 PA blocks. 每个 PA block 存储一个 head 中 <code>BLOCK_SIZE</code> 个 token.<br>例如, 若 <code>BLOCK_SIZE=16</code>, <code>HEAD_SIZE=128</code>, 则一个 PA block 能存储一个 head 的 <code>16 * 128 = 2048</code> 个元素.</li><li>每个 PA block 可能只包含一部分的 context tokens.</li><li>从 page 角度看, KV cache 是若干个 page 的集合;</li></ol></blockquote></li><li><code>NUM_THREADS</code> 每个 CUDA thread block 中 thread 的数量.</li><li><code>PARTITION_SIZE</code> 参与 TP 的 GPU 数量, 默认 0 表示单卡. (以下都以单卡为例说明)</li></ul><p>额外的一些参数:</p><ul><li><code>num_seqs</code>: 本次推理请求 sequence 数目.<blockquote><p>由于这个 kernel 只处理 decode 阶段单 query attention, 所以实际上每个 sequence 只有一个 query token.</p></blockquote></li><li><code>num_heads</code>: Q 的 head 数目</li><li><code>num_kv_heads</code>: KV 的 head 数目, 对于 MHA 其值和 <code>num_heads</code> 相同; 如果是 GQA, MQA 则 <code>num_kv_heads</code> 小于 <code>num_head</code>.</li><li><code>head_size</code>: 即 <code>HEAD_SIZE</code></li><li><code>k_cache: (num_blocks, num_kv_heads, head_size/x, block_size, x)</code>, 其中 <code>x</code> 表示 <code>THREAD_GROUP_SIZE * VEC_SIZE</code> 的大小 (后面会细说).</li></ul><p>下面结合 GPU architecture 初步分析一下参数.</p><p><img alt=gpu-archi.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/gpu-archi.png></p><p>🧐 <strong>为什么要分 thread group?</strong></p><ul><li>因为当一个 cuda block 要取的数据比较少的时候 (计算 QK), 一个 thread group 分别一次取 Q 和 K 中 16B; 当一个 cuda block 要取的数据比较多的时候 (计算 LV), 一个 thread 取 16B.</li></ul><h3 id=52shared-memory-q_vecs-的写入>5.2.Shared Memory: <code>q_vecs</code> 的写入<a hidden class=anchor aria-hidden=true href=#52shared-memory-q_vecs-的写入>#</a></h3><p>从 kernel 中的第一个申请的 shared memory 开始说.</p><blockquote><p>关于 shared memeory:</p><ol><li>在 kernel 中申请的 shared memory 被当前 cuda block 中的所有 thread 共享.</li><li>shared memory 的作用是为了减少 global memory 的访问次数，提高访存效率.</li></ol></blockquote><p>以下代码申请了一块 shared memroy 被整个 CUDA Block 中所有 kernel 共享:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>__shared__</span> <span class=n>Q_vec</span> <span class=n>q_vecs</span><span class=p>[</span><span class=n>THREAD_GROUP_SIZE</span><span class=p>][</span><span class=n>NUM_VECS_PER_THREAD</span><span class=p>];</span>
</span></span></code></pre></div><p>首先, <code>q_vecs</code> 覆盖了 Q 中 <code>head_size</code> 个元素 - 这也是一个 cuda block 需要处理的数据量.</p><p>接着再说两个维度的参数的意思:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>constexpr</span> <span class=kt>int</span> <span class=n>NUM_ELEMS_PER_THREAD</span> <span class=o>=</span> <span class=n>HEAD_SIZE</span> <span class=o>/</span> <span class=n>THREAD_GROUP_SIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>constexpr</span> <span class=kt>int</span> <span class=n>NUM_VECS_PER_THREAD</span> <span class=o>=</span> <span class=n>NUM_ELEMS_PER_THREAD</span> <span class=o>/</span> <span class=n>VEC_SIZE</span><span class=p>;</span>
</span></span></code></pre></div><ul><li><code>THREAD_GROUP_SIZE</code>: 每个 thread group 中的 thread 数量. 注意, 一个 cuda block 中有 <code>NUM_THREADS</code> 个 thread, <code>NUM_THREAD_GROUPS</code> 个 thread group. <code>THREAD_GROUP_SIZE = MAX(WARP_SIZE/BLOCK_SIZE, 1)</code>.</li><li><code>NUM_VECS_PER_THREAD</code>: <code>HEAD_SIZE</code> 能被分成多少个 16B. (这个变量这么命名的理由是后面读取 K 的时候每个 thread 会往自己的寄存器内读 <code>NUM_VECS_PER_THREAD</code> 个 k_vec.)</li></ul><blockquote><p>证明: <code>q_vecs</code> 覆盖 Q 的一个 head, 并且 <code>NUM_VECS_PER_THREAD</code> 表示 Q 的一个 head 被分成多少个 16B.<br>=> <code>THREAD_GROUP_SIZE</code> * <code>VEC_SIZE</code> = 16B / <code>sizeof(scalar_t)</code>;<br>=> <code>NUM_VECS_PER_THREAD</code> * 16B / <code>sizeof(scalar_t)</code> = <code>HEAD_SIZE</code>;</p></blockquote><p>然后看 load Q 的代码, 建议结合下面的图一起看:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl>  <span class=c1>// Load Q to shmem
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=n>thread_group_idx</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_VECS_PER_THREAD</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>i</span> <span class=o>+=</span> <span class=n>NUM_THREAD_GROUPS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>int</span> <span class=n>vec_idx</span> <span class=o>=</span> <span class=n>thread_group_offset</span> <span class=o>+</span> <span class=n>i</span> <span class=o>*</span> <span class=n>THREAD_GROUP_SIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>q_vecs</span><span class=p>[</span><span class=n>thread_group_offset</span><span class=p>][</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>        <span class=o>*</span><span class=k>reinterpret_cast</span><span class=o>&lt;</span><span class=k>const</span> <span class=n>Q_vec</span><span class=o>*&gt;</span><span class=p>(</span><span class=n>q_ptr</span> <span class=o>+</span> <span class=n>vec_idx</span> <span class=o>*</span> <span class=n>VEC_SIZE</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><ul><li><code>thread_group_idx</code> 表示当前 thread 属于当前 cuda block 中第几个 thread group.</li><li><code>thread_group_offset</code> 表示当前 thread 在当前 thread group 中是第几个 thread.</li></ul><p><img alt=pa-load-q.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/pa-load-q.png></p><p>上图展示了循环具体是怎么跑的.</p><ul><li>一个紫色箭头表示一个 thread group.</li><li><code>NUM_VECS_PER_THREAD</code> 表示 <code>HEAD_SIZE</code> 能被分成多少个 16B.</li><li>实际读取 Q 的内存时, 所有 thread group 从 Q 的起始位置紧密排列, 根据图上看的话一共有 <code>NUM_THREAD_GROUPS</code> 个紫色箭头.</li><li>所有 thread group 读取一次 Q 并存入 <code>q_vecs</code> 对应循环中的一次迭代; 因此下次迭代 thread group 需要向后偏移 <code>NUM_THREAD_GROUPS</code> 个位置 (例如 <code>i</code> 从 1 变为 7).</li><li>此外, 读一次 16B 对应一个 thread 来说自然也是取一个 VEC.</li><li>对应到 kernel 编写, 还需要计算当前 thread 具体读取哪个 vec; 因此得到 <code>vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE</code>.</li></ul><blockquote><p>🤔 这里会不会有 bank conflict?</p></blockquote><p>总之现在我们把 <code>(1, head_size)</code> 大小的元素读到了 cuda block 共享的 shared memory <code>q_vecs</code> 中.</p><h3 id=53-读取-k-cache-并计算-qk>5.3. 读取 K Cache 并计算 QK<a hidden class=anchor aria-hidden=true href=#53-读取-k-cache-并计算-qk>#</a></h3><p>现在从 cuda block 的角度看, 当前 block 已经获得了自己要算的 Q 中的一个 head (形状为 <code>(1, head_size)</code>), 接下来就是计算 Q 和 K 的点积.</p><p>点积过程是把当前 block 拥有的 Q head 和整个 K Cache (迭代地) 进行点积运算. 参考下图:</p><p><img alt=pa-cal-kq-01.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/pa-cal-kq-01.png></p><p>QK 乘积实际上被暂存在 <code>logits</code> (也是一块 shared memory) 中, 之后会被用来计算 softmax.</p><p>😇 看下循环的具体代码吧:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Loop 1
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>block_idx</span> <span class=o>=</span> <span class=n>start_block_idx</span> <span class=o>+</span> <span class=n>warp_idx</span><span class=p>;</span> <span class=n>block_idx</span> <span class=o>&lt;</span> <span class=n>end_block_idx</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>block_idx</span> <span class=o>+=</span> <span class=n>NUM_WARPS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Physical block calculation ...
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// Loop 2
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_TOKENS_PER_THREAD_GROUP</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Offset calculation ...
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>K_vec</span> <span class=n>k_vecs</span><span class=p>[</span><span class=n>NUM_VECS_PER_THREAD</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Loop 3
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>NUM_VECS_PER_THREAD</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=c1>// Load K to `k_vecs` ...
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=kt>float</span> <span class=n>qk</span> <span class=o>=</span> <span class=n>scale</span> <span class=o>*</span> <span class=n>Qk_dot</span><span class=o>&lt;</span><span class=n>scalar_t</span><span class=p>,</span> <span class=n>THREAD_GROUP_SIZE</span><span class=o>&gt;::</span><span class=n>dot</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                        <span class=n>q_vecs</span><span class=p>[</span><span class=n>thread_group_offset</span><span class=p>],</span> <span class=n>k_vecs</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Add the ALiBi bias if slopes are given.
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>qk</span> <span class=o>+=</span> <span class=p>(</span><span class=n>alibi_slope</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>)</span> <span class=o>?</span> <span class=n>alibi_slope</span> <span class=o>*</span> <span class=p>(</span><span class=n>token_idx</span> <span class=o>-</span> <span class=n>seq_len</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>:</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=n>thread_group_offset</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=c1>// Store the partial reductions to shared memory.
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=c1>// Mask
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=c1>// Update the max value.
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>先说第一个循环, 其中比较重要的几个参数定义如下:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// [start_block_idx, end_block_idx) is the range of blocks to process.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=kt>int</span> <span class=n>start_block_idx</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>    <span class=n>USE_PARTITIONING</span> <span class=o>?</span> <span class=n>partition_idx</span> <span class=o>*</span> <span class=nl>num_blocks_per_partition</span> <span class=p>:</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=c1>// If not using partitioning, `end_block_idx` should be equal to `num_seq_blocks`.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=kt>int</span> <span class=n>end_block_idx</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>    <span class=n>MIN</span><span class=p>(</span><span class=n>start_block_idx</span> <span class=o>+</span> <span class=n>num_blocks_per_partition</span><span class=p>,</span> <span class=n>num_seq_blocks</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=c1>// Number of blocks to process.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=kt>int</span> <span class=n>num_blocks</span> <span class=o>=</span> <span class=n>end_block_idx</span> <span class=o>-</span> <span class=n>start_block_idx</span><span class=p>;</span>
</span></span></code></pre></div><p>用文字描述就是:</p><ul><li><code>blk_idx</code> 表示当前 thread 所在 warp 需要处理的 PA block 的在 <code>block_table</code> 中索引 (逻辑上的索引).</li><li><code>start_block_idx</code> 和 <code>end_block_idx</code> 表示当前 cuda block 需要处理的 block 范围.</li><li><code>num_blocks</code> 表示当前 cuda block 需要处理的 block 数量.</li><li><code>NUM_WARPS</code> 表示当前 cuda block 中 warp 的数量. 一个 warp 包含 32 个 thread.</li><li><code>warp_idx</code> 表示当前 warp 在当前 cuda block 中的索引.</li></ul><p>说人话就是每个 warp 处理一个 PA block, 一开始 cuda block 中的所有 warp 紧密地指向最前面的 <code>NUM_WARPS</code> 个 PA block, 每次循环所有 warp 向后偏移 <code>NUM_WARPS</code> 个 PA block 的长度. 参考下图:</p><p><img alt=pa-cal-kq-02.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/pa-cal-kq-02.png></p><blockquote><p>🔔 这里再回顾一下, 一个 PA block 里存放了 <code>BLOCK_SIZE</code> 个 token 的 K 或 V cache.</p></blockquote><p>所以说这个循环和上面读取 Q 的循环一个尿性🤮, 不过是以 warp 的粒度处理数据;</p><p>进入了第一个循环内部, 第一步当然是计算当前 thread 对应的 warp 应该计算哪个 PA block (物理上的索引), 因此得到了 <code>physical_block_number</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>const</span> <span class=kt>int64_t</span> <span class=n>physical_block_number</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>    <span class=k>static_cast</span><span class=o>&lt;</span><span class=kt>int64_t</span><span class=o>&gt;</span><span class=p>(</span><span class=n>block_table</span><span class=p>[</span><span class=n>block_idx</span><span class=p>]);</span>
</span></span></code></pre></div><hr><p>然后解释第二个循环, 第二个循环的整体目标就是让当前 warp 计算好自己负责的 PA block 中 <code>BLOCK_SIZE</code> 个 token 的 QK 乘积.</p><p>先看一下 <code>i</code> 的上界:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>constexpr</span> <span class=kt>int</span> <span class=n>NUM_TOKENS_PER_THREAD_GROUP</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>    <span class=n>DIVIDE_ROUND_UP</span><span class=p>(</span><span class=n>BLOCK_SIZE</span><span class=p>,</span> <span class=n>WARP_SIZE</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=c1>// Loop 1
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>block_idx</span> <span class=o>=</span> <span class=n>start_block_idx</span> <span class=o>+</span> <span class=n>warp_idx</span><span class=p>;</span> <span class=n>block_idx</span> <span class=o>&lt;</span> <span class=n>end_block_idx</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>block_idx</span> <span class=o>+=</span> <span class=n>NUM_WARPS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Loop 2
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_TOKENS_PER_THREAD_GROUP</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></div><p>从 kernel 角度看, 每个 thread 需要辅助当前 warp 计算自己负责的一整个 PA block (包含 <code>BLOCK_SIZE</code> 个 token), 而我们把这个过程拆分为 Loop 2 中的 <code>NUM_TOKEN_PER_THREAD_GROUP</code> (也就是 <code>ceil(BLOCK_SIZE / 32)</code>) 次循环;</p><p>说人话就是<strong>一个 thread group 对应一个 token 中的一个 head</strong>, 如果 BLOCK SIZE 太大了后面每个 thread 向后偏移 <code>i * WARP_SIZE</code> 个 token 继续狠狠算🤣.</p><p>也因此第二个循环内部一上来先计算了几个偏移量, 并且申请了 thread 内部私有的 <code>k_vecs</code> 数组:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>const</span> <span class=kt>int</span> <span class=n>physical_block_offset</span> <span class=o>=</span> <span class=p>(</span><span class=n>thread_group_idx</span> <span class=o>+</span> <span class=n>i</span> <span class=o>*</span> <span class=n>WARP_SIZE</span><span class=p>)</span> <span class=o>%</span> <span class=n>BLOCK_SIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>const</span> <span class=kt>int</span> <span class=n>token_idx</span> <span class=o>=</span> <span class=n>block_idx</span> <span class=o>*</span> <span class=n>BLOCK_SIZE</span> <span class=o>+</span> <span class=n>physical_block_offset</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>K_vec</span> <span class=n>k_vecs</span><span class=p>[</span><span class=n>NUM_VECS_PER_THREAD</span><span class=p>];</span>
</span></span></code></pre></div><ul><li><code>thread_group_idx</code> 表示当前 thread group 在整个 cuda block 中的索引.</li><li>☢️ 一个 thread group 在一次循环中负责 fetch 一个 PA block 中 K cache 的一个 token 中<strong>自己负责的 head</strong>.</li><li>☢️ 一个 thread group 负责计算一个 qk 值; 这个值显然是由一个 Q head 和一个 K head 点积得到的.</li><li><code>physical_block_offset</code> 表示当前要算的 token 在当前 PA block 中的偏移量 (注意和前面的 <code>physical_block_number</code> 区分).</li><li>加 <code>i * WARP_SIZE</code> 的原因是如果 <code>BLOCK_SIZE</code> 大于 32, 那么一个 warp 要多次循环才能处理完一个 PA block 中的所有 token, 对应 <code>thread_group_idx</code> 需要做偏移.</li><li><code>token_idx</code> 表示当前要算的 token 在整个 seq 的 KV cache 中的索引.</li><li><code>k_vecs</code> 中能存放 <code>NUM_VECS_PER_THREAD</code> 个 VEC, 而一整个 thread group 中所有的 thread 的 <code>k_vecs</code> 合起来才能组成一个 K 的 head (推导参考上面 Q 的 😇). 这就是为什么后面算 QK 的时候要 reduce.</li></ul><p>🤔 <strong>看到这里读者可能有一个问题: 一个 token 的 K cache 应该对应多个 head, 为什么上面说一个 thread group 只负责一个 head?</strong><br>答: 因为实际计算的时候, 一个 cuda block 只负责计算一个 head, 对应到 K Cache 乃至后面 V Cache 的位置也是一样的.</p><blockquote><p>这里额外说一下, 读 K 的 head 的一个目标应该是在尽量少的 register 中装下一个 head 的所有元素, 这样后续和 shared memory 中的 Q 做点乘并规约的速度更快. 假设一个 head 有 128 个 float16, 则占用 256B, 而 A100 中一个 thread 最多能有 255 个 32-bit register (也就是 1020B), 此时可以认为一个 thread 能装下一个 head 的所有元素.<br>但是由于目前 PA kernel 在 <code>BLOCK_SIZE</code> 为 16 的情况下 <code>THREAD_GROUP_SIZE</code> 等于 2, 因此一个 thread 只会装一个 head 的一半元素, 这样可能会导致 register 的使用率不高.</p></blockquote><hr><p>接着进入第三个循环, 目的是让 thread group 从 K cache 中读一个 head, 并存入 <code>k_vecs</code> 中:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// x == THREAD_GROUP_SIZE * VEC_SIZE
</span></span></span><span class=line><span class=cl><span class=c1>// Each thread group fetches x elements from the key at a time.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>constexpr</span> <span class=kt>int</span> <span class=n>x</span> <span class=o>=</span> <span class=mi>16</span> <span class=o>/</span> <span class=k>sizeof</span><span class=p>(</span><span class=n>cache_t</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=c1>//...
</span></span></span><span class=line><span class=cl><span class=c1>// Loop 1
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>block_idx</span> <span class=o>=</span> <span class=n>start_block_idx</span> <span class=o>+</span> <span class=n>warp_idx</span><span class=p>;</span> <span class=n>block_idx</span> <span class=o>&lt;</span> <span class=n>end_block_idx</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>block_idx</span> <span class=o>+=</span> <span class=n>NUM_WARPS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Loop 2
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_TOKENS_PER_THREAD_GROUP</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>K_vec</span> <span class=n>k_vecs</span><span class=p>[</span><span class=n>NUM_VECS_PER_THREAD</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Loop 3
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>NUM_VECS_PER_THREAD</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=k>const</span> <span class=n>cache_t</span><span class=o>*</span> <span class=n>k_ptr</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>                <span class=n>k_cache</span> <span class=o>+</span> <span class=n>physical_block_number</span> <span class=o>*</span> <span class=n>kv_block_stride</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>                <span class=n>kv_head_idx</span> <span class=o>*</span> <span class=n>kv_head_stride</span> <span class=o>+</span> <span class=n>physical_block_offset</span> <span class=o>*</span> <span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=k>const</span> <span class=kt>int</span> <span class=n>vec_idx</span> <span class=o>=</span> <span class=n>thread_group_offset</span> <span class=o>+</span> <span class=n>j</span> <span class=o>*</span> <span class=n>THREAD_GROUP_SIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=k>const</span> <span class=kt>int</span> <span class=n>offset1</span> <span class=o>=</span> <span class=p>(</span><span class=n>vec_idx</span> <span class=o>*</span> <span class=n>VEC_SIZE</span><span class=p>)</span> <span class=o>/</span> <span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=k>const</span> <span class=kt>int</span> <span class=n>offset2</span> <span class=o>=</span> <span class=p>(</span><span class=n>vec_idx</span> <span class=o>*</span> <span class=n>VEC_SIZE</span><span class=p>)</span> <span class=o>%</span> <span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=c1>// if Fp8KVCacheDataType::kAuto
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=n>k_vecs</span><span class=p>[</span><span class=n>j</span><span class=p>]</span> <span class=o>=</span> <span class=o>*</span><span class=k>reinterpret_cast</span><span class=o>&lt;</span><span class=k>const</span> <span class=n>K_vec</span><span class=o>*&gt;</span><span class=p>(</span>
</span></span><span class=line><span class=cl>              <span class=n>k_ptr</span> <span class=o>+</span> <span class=n>offset1</span> <span class=o>*</span> <span class=n>BLOCK_SIZE</span> <span class=o>*</span> <span class=n>x</span> <span class=o>+</span> <span class=n>offset2</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></div><p>老规矩, 先看 <code>j</code>, 本质就是从 0 迭代到 <code>NUM_VECS_PER_THREAD</code>, 每次迭代当前 thread 读取一个 VEC 存入 <code>k_vecs</code> 中.</p><blockquote><p>🔔 回顾:</p><ol><li><code>NUM_VECS_PER_THREAD</code> 表示一个 head 被分成多少个 16B.</li><li><code>k_cache</code> 的 shape 为 <code>(num_blocks, num_kv_heads, head_size/x, block_size, x)</code>.</li></ol></blockquote><p>其中的 <code>x</code> 表示一个 thread group 需要读取的元素数量 (<code>VEC_SIZE</code> * <code>THREAD_GROUP_SIZE</code>); 因此作者将 K Cache 的 layout 的最后一维设置为 <code>x</code> 其实也是方便后续 thread group 对 K cache 的读取.</p><p>下图具体展示了寻址的过程:</p><p><img alt=pa-cal-kq-03.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/pa-cal-kq-03.png></p><p>其中:</p><ul><li>在 MHSA 中, <code>num_kv_heads</code> 等于 <code>num_heads</code>; 而在 GQA, MQA 中, <code>num_kv_heads</code> 小于 <code>num_heads</code>.</li><li>(1) 负责找到当前 thread 属于的 warp 要处理哪个 PA block.</li><li>(2) 负责找到当前 thread 要计算的 head 在 K cache 中的位置. 这个 head 的索引和 Q 中 head 的索引在 MHSA 中相同.</li><li>(3) 负责找到当前 thread group 要计算的 token 在当前 PA block 中的位置.</li><li>(5) 负责找到当前 thread 在需要读取的 head (蓝色长方体) 中 x 的偏移, 通过 <code>j</code> 进行迭代读取. <strong>每次循环 thread group 中的所有 thread 取一个 x.</strong></li><li>(6) 负责找到当前 thread 在 thread gruop 中读取的 x 中 VEC 的偏移; thread 一次读取一个 VEC.</li></ul><p>🤔 <strong>为什么 (5) 在实际寻址时需要 <code>* BLOCK_SIZE * x</code> ?</strong><br>答: 这是根据 <code>k_cache</code> 的 layout 得到的 stride. 同理 (3) <code>* x</code> 也是 stride.</p><p>第 3 个循环结束时当前 warp 负责的每个 token 中需要的 K cache head 已经全被加载入 thread 本地的 <code>k_vecs</code> 中了.</p><p>由于一个 thread group 的 <code>k_vecs</code> 才能真正组成一个 head, 在退回第二个循环进行 QK dot 的时候, 需要做个 reduction, 具体的范围就是 <code>THREAD_GROUP_SIZE</code> 个 thread:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Loop 1
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>block_idx</span> <span class=o>=</span> <span class=n>start_block_idx</span> <span class=o>+</span> <span class=n>warp_idx</span><span class=p>;</span> <span class=n>block_idx</span> <span class=o>&lt;</span> <span class=n>end_block_idx</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>block_idx</span> <span class=o>+=</span> <span class=n>NUM_WARPS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Loop 2
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_TOKENS_PER_THREAD_GROUP</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>K_vec</span> <span class=n>k_vecs</span><span class=p>[</span><span class=n>NUM_VECS_PER_THREAD</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Loop 3
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>NUM_VECS_PER_THREAD</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=kt>float</span> <span class=n>qk</span> <span class=o>=</span> <span class=n>scale</span> <span class=o>*</span> <span class=n>Qk_dot</span><span class=o>&lt;</span><span class=n>scalar_t</span><span class=p>,</span> <span class=n>THREAD_GROUP_SIZE</span><span class=o>&gt;::</span><span class=n>dot</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                             <span class=n>q_vecs</span><span class=p>[</span><span class=n>thread_group_offset</span><span class=p>],</span> <span class=n>k_vecs</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></div><p>计算完 <code>qk</code> 后, 由当前 thread group 中第一个 (offset 为 0) 的 thread 对自己刚才算出来的 <code>qk</code> 进行 mask, 顺便看看如果没有 mask 掉, 把 <code>qk_max</code> 赋值为 <code>qk</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>thread_group_offset</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Store the partial reductions to shared memory.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// NOTE(woosuk): It is required to zero out the masked logits.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>const</span> <span class=kt>bool</span> <span class=n>mask</span> <span class=o>=</span> <span class=n>token_idx</span> <span class=o>&gt;=</span> <span class=n>seq_len</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span><span class=p>[</span><span class=n>token_idx</span> <span class=o>-</span> <span class=n>start_token_idx</span><span class=p>]</span> <span class=o>=</span> <span class=n>mask</span> <span class=o>?</span> <span class=mf>0.f</span> <span class=o>:</span> <span class=n>qk</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Update the max value.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>qk_max</span> <span class=o>=</span> <span class=n>mask</span> <span class=o>?</span> <span class=nl>qk_max</span> <span class=p>:</span> <span class=n>fmaxf</span><span class=p>(</span><span class=n>qk_max</span><span class=p>,</span> <span class=n>qk</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>🧐 <strong>为什么要做 mask?</strong></p><ul><li>因为一个 seq 的最后一个 PA block 可能覆盖不满 <code>BLOCK_SIZE</code> 个 token. 这里的 mask 就是把那部分 qk 置零.</li></ul><h3 id=54-softmax>5.4. Softmax<a hidden class=anchor aria-hidden=true href=#54-softmax>#</a></h3><p>我勒个 QK 啊, 总算算完了, 锐克 five 都要被抽清仓了. 页意丁真, 鉴定为开算 softmax.</p><p>主要步骤就是广播然后算, 算 softmax 需要知道每个 head 对应的 qk 的最大值. 由于一个 cuda block 负责的就是一个 head, 对于这个 head 上面的计算步骤一共算了 <code>cache_len</code>个 token 的 qk, 因此需要做一个 cuda block 范围的规约, 找到其中最大的 qk 值.</p><p>先在 warp 层面规约.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>__shared__</span> <span class=kt>float</span> <span class=n>red_smem</span><span class=p>[</span><span class=mi>2</span> <span class=o>*</span> <span class=n>NUM_WARPS</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=c1>// Perform reduction across the threads in the same warp to get the
</span></span></span><span class=line><span class=cl><span class=c1>// max qk value for each &#34;warp&#34; (not across the thread block yet).
</span></span></span><span class=line><span class=cl><span class=c1>// The 0-th thread of each thread group already has its max qk value.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>mask</span> <span class=o>=</span> <span class=n>WARP_SIZE</span> <span class=o>/</span> <span class=mi>2</span><span class=p>;</span> <span class=n>mask</span> <span class=o>&gt;=</span> <span class=n>THREAD_GROUP_SIZE</span><span class=p>;</span> <span class=n>mask</span> <span class=o>/=</span> <span class=mi>2</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>qk_max</span> <span class=o>=</span> <span class=n>fmaxf</span><span class=p>(</span><span class=n>qk_max</span><span class=p>,</span> <span class=n>VLLM_SHFL_XOR_SYNC</span><span class=p>(</span><span class=n>qk_max</span><span class=p>,</span> <span class=n>mask</span><span class=p>));</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>lane</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>red_smem</span><span class=p>[</span><span class=n>warp_idx</span><span class=p>]</span> <span class=o>=</span> <span class=n>qk_max</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>__syncthreads</span><span class=p>();</span>
</span></span></code></pre></div><ul><li><code>red_smem</code> 是之前申请的 shared memory.</li><li><code>VLLM_SHFL_XOR_SYNC</code> 是一个 warp 内的 shuffle 操作, 具体来说, 在每次循环时, 每个 thread 和自己相距 <code>mask</code> 位置的线程交换数据 (交换来的数据通过 <code>fmaxf</code> 比较), 并且 <code>mask</code> 会逐渐减半, 直到 <code>THREAD_GROUP_SIZE</code> 为止.</li><li><code>lane</code> 表示当前 warp 中的线程索引.</li></ul><p>接着再对每个 warp 的最大值进行规约, 由于每个 warp 的最大值都被存入了 <code>red_smem</code> 中, 所以只需要再次进行 shuffle 操作即可.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// TODO(woosuk): Refactor this part.
</span></span></span><span class=line><span class=cl><span class=c1>// Get the max qk value for the sequence.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>qk_max</span> <span class=o>=</span> <span class=n>lane</span> <span class=o>&lt;</span> <span class=n>NUM_WARPS</span> <span class=o>?</span> <span class=n>red_smem</span><span class=p>[</span><span class=n>lane</span><span class=p>]</span> <span class=o>:</span> <span class=o>-</span><span class=n>FLT_MAX</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>mask</span> <span class=o>=</span> <span class=n>NUM_WARPS</span> <span class=o>/</span> <span class=mi>2</span><span class=p>;</span> <span class=n>mask</span> <span class=o>&gt;=</span> <span class=mi>1</span><span class=p>;</span> <span class=n>mask</span> <span class=o>/=</span> <span class=mi>2</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>qk_max</span> <span class=o>=</span> <span class=n>fmaxf</span><span class=p>(</span><span class=n>qk_max</span><span class=p>,</span> <span class=n>VLLM_SHFL_XOR_SYNC</span><span class=p>(</span><span class=n>qk_max</span><span class=p>,</span> <span class=n>mask</span><span class=p>));</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>此时, 第 1 个线程的 <code>qk_max</code> 就是当前 cuda block 中所有 warp 中最大的 qk 值. 将其广播给所有线程:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Broadcast the max qk value to all threads.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>qk_max</span> <span class=o>=</span> <span class=n>VLLM_SHFL_SYNC</span><span class=p>(</span><span class=n>qk_max</span><span class=p>,</span> <span class=mi>0</span><span class=p>);</span>
</span></span></code></pre></div><p>在获得了 <code>qk_max</code> 后, 就可以计算 softmax 了:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Get the sum of the exp values.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>float</span> <span class=n>exp_sum</span> <span class=o>=</span> <span class=mf>0.f</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=n>thread_idx</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>num_tokens</span><span class=p>;</span> <span class=n>i</span> <span class=o>+=</span> <span class=n>NUM_THREADS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>float</span> <span class=n>val</span> <span class=o>=</span> <span class=n>__expf</span><span class=p>(</span><span class=n>logits</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>-</span> <span class=n>qk_max</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>val</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>exp_sum</span> <span class=o>+=</span> <span class=n>val</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>exp_sum</span> <span class=o>=</span> <span class=n>block_sum</span><span class=o>&lt;</span><span class=n>NUM_WARPS</span><span class=o>&gt;</span><span class=p>(</span><span class=o>&amp;</span><span class=n>red_smem</span><span class=p>[</span><span class=n>NUM_WARPS</span><span class=p>],</span> <span class=n>exp_sum</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// Compute softmax.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=kt>float</span> <span class=n>inv_sum</span> <span class=o>=</span> <span class=n>__fdividef</span><span class=p>(</span><span class=mf>1.f</span><span class=p>,</span> <span class=n>exp_sum</span> <span class=o>+</span> <span class=mf>1e-6</span><span class=n>f</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=n>thread_idx</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>num_tokens</span><span class=p>;</span> <span class=n>i</span> <span class=o>+=</span> <span class=n>NUM_THREADS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>*=</span> <span class=n>inv_sum</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>__syncthreads</span><span class=p>();</span>
</span></span></code></pre></div><h3 id=55-lv-logits--value>5.5. LV (Logits * Value)<a hidden class=anchor aria-hidden=true href=#55-lv-logits--value>#</a></h3><p><img alt=pa-cal.png loading=lazy src=/imgs/blogs/dive-into-paged-attention/pa-cal.png></p><p>上图展示了 LV 的计算过程, 主要区别是由于要计算 Logits 的 shape 可以表示为 <code>(num_heads, num_seqs, cache_len)</code>, 而 V 的 shape 可以表示为 <code>(num_heads, cache_len, head_size)</code>, 因此 LV 的矩阵乘法中, 每计算一个元素需要读取 logits 的一行和 V 的一列进行计算.</p><p>此时, 一个 cuda block 的职责从 &ldquo;自 Q 中读取一个 head&rdquo; 转变为 &ldquo;计算 output 中的一个 head&rdquo;.</p><p>🧐 <strong>为什么在计算 LV 时, 去掉了 thread group 的概念, 每个 thread 都被设定为每次读取 16B?</strong></p><ul><li>因为现在每计算一个元素, 需要的访存量更大, 因此给每个 thread 分配了更多的数据读取量. 也就是说, <code>V_VEC_SIZE</code> 比 <code>VEC_SIZE</code> 更大.</li></ul><p>由于 cuda 访存模式按行读取更快, 所以实际的计算结果在遍历 PA block 时线程内部利用 <code>accs</code> 进行累计 (以实现与 V 的一列进行计算的行为):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>constexpr</span> <span class=kt>int</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>=</span> <span class=n>BLOCK_SIZE</span> <span class=o>/</span> <span class=n>V_VEC_SIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>constexpr</span> <span class=kt>int</span> <span class=n>NUM_ROWS_PER_ITER</span> <span class=o>=</span> <span class=n>WARP_SIZE</span> <span class=o>/</span> <span class=n>NUM_V_VECS_PER_ROW</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>constexpr</span> <span class=kt>int</span> <span class=n>NUM_ROWS_PER_THREAD</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>    <span class=n>DIVIDE_ROUND_UP</span><span class=p>(</span><span class=n>HEAD_SIZE</span><span class=p>,</span> <span class=n>NUM_ROWS_PER_ITER</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// NOTE(woosuk): We use FP32 for the accumulator for better accuracy.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>float</span> <span class=n>accs</span><span class=p>[</span><span class=n>NUM_ROWS_PER_THREAD</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>block_idx</span> <span class=o>=</span> <span class=n>start_block_idx</span> <span class=o>+</span> <span class=n>warp_idx</span><span class=p>;</span> <span class=n>block_idx</span> <span class=o>&lt;</span> <span class=n>end_block_idx</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>block_idx</span> <span class=o>+=</span> <span class=n>NUM_WARPS</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>V_vec</span> <span class=n>v_vec</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_ROWS_PER_THREAD</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>V_VEC_SIZE</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=c1>// Load V to `v_vec` ...
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=n>v_vec_ptr</span><span class=p>[</span><span class=n>j</span><span class=p>]</span> <span class=o>=</span> <span class=n>token_idx</span> <span class=o>+</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>seq_len</span> <span class=o>?</span> <span class=n>v_vec_ptr</span><span class=p>[</span><span class=n>j</span><span class=p>]</span> <span class=o>:</span> <span class=n>zero_value</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=c1>// Accumulate the dot product.
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>accs</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+=</span> <span class=n>dot</span><span class=p>(</span><span class=n>logits_vec</span><span class=p>,</span> <span class=n>v_vec</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>由于每个线程负责的累计部分不满一整行/列, 所以进行规约:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Perform reduction within each warp.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span>  <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_ROWS_PER_THREAD</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>float</span> <span class=n>acc</span> <span class=o>=</span> <span class=n>accs</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>mask</span> <span class=o>=</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>/</span> <span class=mi>2</span><span class=p>;</span> <span class=n>mask</span> <span class=o>&gt;=</span> <span class=mi>1</span><span class=p>;</span> <span class=n>mask</span> <span class=o>/=</span> <span class=mi>2</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=n>acc</span> <span class=o>+=</span> <span class=n>VLLM_SHFL_XOR_SYNC</span><span class=p>(</span><span class=n>acc</span><span class=p>,</span> <span class=n>mask</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>accs</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>acc</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>// NOTE(woosuk): A barrier is required because the shared memory space for
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>// logits is reused for the output.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>// Perform reduction across warps.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=kt>float</span><span class=o>*</span> <span class=n>out_smem</span> <span class=o>=</span> <span class=k>reinterpret_cast</span><span class=o>&lt;</span><span class=kt>float</span><span class=o>*&gt;</span><span class=p>(</span><span class=n>shared_mem</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span>  <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=n>NUM_WARPS</span><span class=p>;</span> <span class=n>i</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>;</span> <span class=n>i</span> <span class=o>/=</span> <span class=mi>2</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>mid</span> <span class=o>=</span> <span class=n>i</span> <span class=o>/</span> <span class=mi>2</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Upper warps write to shared memory.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>if</span> <span class=p>(</span><span class=n>warp_idx</span> <span class=o>&gt;=</span> <span class=n>mid</span> <span class=o>&amp;&amp;</span> <span class=n>warp_idx</span> <span class=o>&lt;</span> <span class=n>i</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=kt>float</span><span class=o>*</span> <span class=n>dst</span> <span class=o>=</span> <span class=o>&amp;</span><span class=n>out_smem</span><span class=p>[(</span><span class=n>warp_idx</span> <span class=o>-</span> <span class=n>mid</span><span class=p>)</span> <span class=o>*</span> <span class=n>HEAD_SIZE</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span>      <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_ROWS_PER_THREAD</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>const</span> <span class=kt>int</span> <span class=n>row_idx</span> <span class=o>=</span> <span class=n>lane</span> <span class=o>/</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>+</span> <span class=n>i</span> <span class=o>*</span> <span class=n>NUM_ROWS_PER_ITER</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=n>row_idx</span> <span class=o>&lt;</span> <span class=n>HEAD_SIZE</span> <span class=o>&amp;&amp;</span> <span class=n>lane</span> <span class=o>%</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>          <span class=n>dst</span><span class=p>[</span><span class=n>row_idx</span><span class=p>]</span> <span class=o>=</span> <span class=n>accs</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// Lower warps update the output.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>if</span> <span class=p>(</span><span class=n>warp_idx</span> <span class=o>&lt;</span> <span class=n>mid</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=k>const</span> <span class=kt>float</span><span class=o>*</span> <span class=n>src</span> <span class=o>=</span> <span class=o>&amp;</span><span class=n>out_smem</span><span class=p>[</span><span class=n>warp_idx</span> <span class=o>*</span> <span class=n>HEAD_SIZE</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span>      <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_ROWS_PER_THREAD</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>const</span> <span class=kt>int</span> <span class=n>row_idx</span> <span class=o>=</span> <span class=n>lane</span> <span class=o>/</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>+</span> <span class=n>i</span> <span class=o>*</span> <span class=n>NUM_ROWS_PER_ITER</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=n>row_idx</span> <span class=o>&lt;</span> <span class=n>HEAD_SIZE</span> <span class=o>&amp;&amp;</span> <span class=n>lane</span> <span class=o>%</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>          <span class=n>accs</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+=</span> <span class=n>src</span><span class=p>[</span><span class=n>row_idx</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span></code></pre></div><p>最后写入到输出中:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl>  <span class=c1>// Write the final output.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>if</span> <span class=p>(</span><span class=n>warp_idx</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>scalar_t</span><span class=o>*</span> <span class=n>out_ptr</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>+</span> <span class=n>seq_idx</span> <span class=o>*</span> <span class=n>num_heads</span> <span class=o>*</span> <span class=n>max_num_partitions</span> <span class=o>*</span> <span class=n>HEAD_SIZE</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>        <span class=n>head_idx</span> <span class=o>*</span> <span class=n>max_num_partitions</span> <span class=o>*</span> <span class=n>HEAD_SIZE</span> <span class=o>+</span> <span class=n>partition_idx</span> <span class=o>*</span> <span class=n>HEAD_SIZE</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=cp>#pragma unroll
</span></span></span><span class=line><span class=cl><span class=cp></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>NUM_ROWS_PER_THREAD</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=k>const</span> <span class=kt>int</span> <span class=n>row_idx</span> <span class=o>=</span> <span class=n>lane</span> <span class=o>/</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>+</span> <span class=n>i</span> <span class=o>*</span> <span class=n>NUM_ROWS_PER_ITER</span><span class=p>;</span>
</span></span><span class=line><span class=cl>      <span class=k>if</span> <span class=p>(</span><span class=n>row_idx</span> <span class=o>&lt;</span> <span class=n>HEAD_SIZE</span> <span class=o>&amp;&amp;</span> <span class=n>lane</span> <span class=o>%</span> <span class=n>NUM_V_VECS_PER_ROW</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>from_float</span><span class=p>(</span><span class=o>*</span><span class=p>(</span><span class=n>out_ptr</span> <span class=o>+</span> <span class=n>row_idx</span><span class=p>),</span> <span class=n>accs</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://jamesnulliu.github.io/tags/python/>Python</a></li><li><a href=https://jamesnulliu.github.io/tags/vllm/>Vllm</a></li><li><a href=https://jamesnulliu.github.io/tags/attention/>Attention</a></li></ul><nav class=paginav><a class=prev href=https://jamesnulliu.github.io/blogs/render-mathematics-in-hugo/><span class=title>« Prev</span><br><span>Render Mathematics in Hugo</span>
</a><a class=next href=https://jamesnulliu.github.io/blogs/learning-notes-mlir/mlir-tutorial-01-running-and-testing-a-lowering/><span class=title>Next »</span><br><span>MLIR Tutorial 01 | Running and Testing a Lowering</span></a></nav></footer><script src=https://giscus.app/client.js data-repo=jamesnulliu/jamesnulliu.github.io data-repo-id=R_kgDOMPCQIw data-category=Announcements data-category-id=DIC_kwDOMPCQI84Cgb2t data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async></script></article></main><footer class=footer><span>© 2024-2025 JamesNULLiu</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>