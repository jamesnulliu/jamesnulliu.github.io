<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Paper Review: The Deep Learning Compiler: A Comprehensive Survey | 秋水·JamesNULLiu</title><meta name=keywords content="mlir"><meta name=description content="My learning notes of MLIR."><meta name=author content="jamesnulliu"><link rel=canonical href=https://jamesnulliu.github.io/blogs/learning-notes-mlir/paper-review-the-deep-learning-compiler-a-comprehensive-survey/><link crossorigin=anonymous href=/assets/css/stylesheet.62cb9c488bb33c0e9a9d3c29b7f4259cbb0db25aaa19ba672188203d3d5bcaf9.css integrity="sha256-YsucSIuzPA6anTwpt/QlnLsNslqqGbpnIYggPT1byvk=" rel="preload stylesheet" as=style><link rel=icon href=https://jamesnulliu.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://jamesnulliu.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://jamesnulliu.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://jamesnulliu.github.io/apple-touch-icon.png><link rel=mask-icon href=https://jamesnulliu.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://jamesnulliu.github.io/blogs/learning-notes-mlir/paper-review-the-deep-learning-compiler-a-comprehensive-survey/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]]}}</script><meta property="og:url" content="https://jamesnulliu.github.io/blogs/learning-notes-mlir/paper-review-the-deep-learning-compiler-a-comprehensive-survey/"><meta property="og:site_name" content="秋水·JamesNULLiu"><meta property="og:title" content="Paper Review: The Deep Learning Compiler: A Comprehensive Survey"><meta property="og:description" content="My learning notes of MLIR."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2024-07-29T11:07:00+08:00"><meta property="article:modified_time" content="2025-09-12T22:14:12+00:00"><meta property="article:tag" content="Mlir"><meta name=twitter:card content="summary"><meta name=twitter:title content="Paper Review: The Deep Learning Compiler: A Comprehensive Survey"><meta name=twitter:description content="My learning notes of MLIR."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://jamesnulliu.github.io/blogs/"},{"@type":"ListItem","position":2,"name":"📁 Learning Notes: MLIR","item":"https://jamesnulliu.github.io/blogs/learning-notes-mlir/"},{"@type":"ListItem","position":3,"name":"Paper Review: The Deep Learning Compiler: A Comprehensive Survey","item":"https://jamesnulliu.github.io/blogs/learning-notes-mlir/paper-review-the-deep-learning-compiler-a-comprehensive-survey/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Paper Review: The Deep Learning Compiler: A Comprehensive Survey","name":"Paper Review: The Deep Learning Compiler: A Comprehensive Survey","description":"My learning notes of MLIR.","keywords":["mlir"],"articleBody":" Reference: The Deep Learning Compiler: A Comprehensive Survey\n1. ABSTRACT The DL compilers take the DL models described in different DL frameworks as input, and then generate optimized codes for diverse DL hardware as output.\nGenerally, the DL hardware can be divided into the following categories:\nGeneral-purpose hardware with software-hardware co-design; Dedicated hardware fully customized for DL models; Neuromorphic hardware inspired by biological brain science. However, the drawback of relying on the libraries is that they usually fall behind the rapid development of DL models, and thus fail to utilize the DL chips efficiently.\nTo address the drawback of DL libraries and tools, as well as alleviate the burden of optimizing the DL models on each DL hardware manually, the DL community has resorted to the domain specific compilers for rescue.\nThe DL compilers take the model definitions described in the DL frameworks as inputs, and generate efficient code implementations on various DL hardware as outputs.\n2. BACKGROUND 2.1. Deep Learning Frameworks …\n2.3. Hardware-specific DL Code Generator Field Programmable Gate Arrays (FPGAs) are reprogrammable integrated circuits that contain an array of programmable logic blocks. Programmers can configure them after manufacturing.\nThe FPGA can bridge the gap between CPUs/GPUs and ASICs, which causes the FPGA to be an attractive platform for deep learning.\nMapping DL models to FPGAs remains a complicated work even with HLS, because:\nDL models are usually described by the languages of DL frameworks rather than bare mental C/C++ code; DL-specific information and optimizations are hard to be leveraged. The hardware-specific code generator targeting FPGA take the DL models or their domain-specific languages (DSLs) as the input, conduct the domain-specific (about FPGA and DL) optimizations and mappings, then generate the HLS or Verilog/VHDL and finally generate the bitstream. They can be classified into two categories according to the generated architectures of FPGA-based accelerators: the processor architecture and the streaming architecture.\nThe processor architecture has similarities with general-purpose processors. An FPGA accelerator of this architecture usually comprises several Processing Units (PUs), which are comprised of on-chip buffers and multiple smaller Processing Engines (PEs).\nThe streaming architecture has similarities with pipelines. An FPGA accelerator of this architecture consists of multiple different hardware blocks, and it nearly has one hardware block for each layer of an input DL mode\n3. COMMON DESIGN ARCHITECTURE OF DL COMPILERS Figure 1. Common Design Architecture of DL Compilers.\n4.1. High-level IR DAG-based IR - DAG-based IR is one of the most traditional ways for the compilers to build a computation graph, with nodes and edges organized as a directed acyclic graph (DAG). In DL compilers, the nodes of a DAG represent the atomic DL operators (convolution, pooling, etc.), and the edges represent the tensors. And the graph is acyclic without loops, which differs from the data dependence graphs (DDG) of generic compilers.\nLet-binding-based IR - Let-binding is one method to solve the semantic ambiguity by offering let expression to certain functions with restricted scope used by many high-level programming languages such as Javascript, F#, and Scheme. When using the let keyword to define an expression, a let node is generated, and then it points to the operator and variable in the expression instead of just building computational relation between variables as a DAG.\nRepresenting Tensor Computation - Different graph IRs have different ways to represent the computation on tensors:\nFunction Based Lambda Based Einstein notation Data representation - The data in DL compilers (e.g., inputs, weights, and intermediate data) are usually organized in the form of tensors, which are also known as multi-dimensional arrays. The DL compilers can represent tensor data directly by memory pointers, or in a more flexible way by placeholders. A placeholder contains the size for each dimension of a tensor. Alternatively, the dimension sizes of the tensor can be marked as unknown. For optimizations, the DL compilers require the data layout information. In addition, the bound of iterators should be inferred according to the placeholders.\n4.2. Low-level IR Low-level IR describes the computation of a DL model in a more fine-grained representation than that in high-level IR, which enables the target-dependent optimizations by providing interfaces to tune the computation and memory access.\nHalide-based IR - Halide is firstly proposed to parallelize image processing, and it is proven to be extensible and efficient in DL compilers (e.g., TVM). The fundamental philosophy of Halide is the separation of computation and schedule.\nPolyhedral-based IR - The polyhedral model is an important technique adopted in DL compilers. It uses linear programming, affine transformations, and other mathematical methods to optimize loop-based codes with static control flow of bounds and branches.\n4.3. Frontend Optimizations After constructing the computation graph, the frontend applies graph-level optimizations.\nThe frontend optimizations are usually defined by passes, and can be applied by traversing the nodes of the computation graph and performing the graph transformations:\nCapture the specific features from the computation graph; Rewrite the graph for optimization. Figure 2. Example of computation graph optimizations, taken from the HLO graph of AlexNet on Volta GPU using TensorFlow XLA.\n4.3.1. Node-level optimizations The nodes of the computation graph are coarse enough to enable optimizations inside a single node. And the node-level optimizations include node elimination that eliminates unnecessary nodes and node replacement that replaces nodes with other lower-cost nodes.\n4.3.2. Block-level optimizations Algebraic simplification\nThe algebraic simplification opti- mizations consist of :\nalgebraic identification; strength reduction, with which we can replace more expensive operators by cheaper ones; constant folding, with which we can replace the constant expressions by their values. Such optimizations consider a sequence of nodes, then take advantage of commutativity, associativity, and distributivity of different kinds of nodes to simplify the computation.\nOperator fusion\nOperator fusion is indispensable optimization of DL compilers. It enables better sharing of computation, eliminates intermediate allocations, facilitates further optimization by combining loop nests, as well as reduces launch and synchronization overhead.\nOperator sinking\nThis optimization sinks the operations such as transposes below operations such as batch normalization, ReLU, sigmoid, and channel shuffle. By this optimization, many similar operations are moved closer to each other, creating more opportunities for algebraic simplification.\n4.3.3. Dataflow-level optimizations Common sub-expression elimination (CSE) Dead code elimination (DCE) Static memory planning - Static memory planning optimizations are performed to reuse the memory buffers as much as possible. Usually, there are two approaches: in-place memory sharing and standard memory sharing. Layout transformation - Layout transformation tries to find the best data layouts to store tensors in the computation graph and then inserts the layout transformation nodes to the graph. 4.4. Backend Optimizations The backends of DL compilers have commonly included various hardware-specific optimizations, auto-tuning techniques, and optimized kernel libraries. Hardware-specific optimizations enable efficient code generation for different hardware targets. Whereas, auto-tuning has been essential in the compiler backend to alleviate the manual efforts to derive the optimal parameter configurations. Besides, highly-optimized kernel libraries are also widely used on general-purpose processors and other customized DL accelerators.\nFigure 3. Overview of hardware-specific optimizations applied in DL compilers.\n5. FUTURE DIRECTIONS Dynamic shape and pre/post processing Advanced auto-tuning Polyhedral model Subgraph partitioning Quantization Unified optimizations Differentiable programming Privacy protection Training support ","wordCount":"1184","inLanguage":"en","datePublished":"2024-07-29T11:07:00+08:00","dateModified":"2025-09-12T22:14:12Z","author":[{"@type":"Person","name":"jamesnulliu"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://jamesnulliu.github.io/blogs/learning-notes-mlir/paper-review-the-deep-learning-compiler-a-comprehensive-survey/"},"publisher":{"@type":"Organization","name":"秋水·JamesNULLiu","logo":{"@type":"ImageObject","url":"https://jamesnulliu.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://jamesnulliu.github.io/ accesskey=h title="秋水·JamesNULLiu (Alt + H)">秋水·JamesNULLiu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://jamesnulliu.github.io/zh/ title=简体中文 aria-label=简体中文>简体中文</a></li></ul></div></div><ul id=menu><li><a href=https://jamesnulliu.github.io/ title=Home><span>Home</span></a></li><li><a href=https://jamesnulliu.github.io/about_me/ title="About Me"><span>About Me</span></a></li><li><a href=https://jamesnulliu.github.io/blogs/ title=Blogs><span>Blogs</span></a></li><li><a href=https://jamesnulliu.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://jamesnulliu.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://jamesnulliu.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://jamesnulliu.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://jamesnulliu.github.io/friends/ title=Friends><span>Friends</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://jamesnulliu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://jamesnulliu.github.io/blogs/>Blogs</a>&nbsp;»&nbsp;<a href=https://jamesnulliu.github.io/blogs/learning-notes-mlir/>📁 Learning Notes: MLIR</a></div><h1 class="post-title entry-hint-parent">Paper Review: The Deep Learning Compiler: A Comprehensive Survey</h1><div class=post-description>My learning notes of MLIR.</div><div class=post-meta><span title='2024-07-29 11:07:00 +0800 +0800'>Jul-29-2024</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1184 words&nbsp;·&nbsp;jamesnulliu</div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-abstract aria-label="1. ABSTRACT">1. ABSTRACT</a></li><li><a href=#2-background aria-label="2. BACKGROUND">2. BACKGROUND</a><ul><li><a href=#21-deep-learning-frameworks aria-label="2.1. Deep Learning Frameworks">2.1. Deep Learning Frameworks</a></li><li><a href=#23-hardware-specific-dl-code-generator aria-label="2.3. Hardware-specific DL Code Generator">2.3. Hardware-specific DL Code Generator</a></li></ul></li><li><a href=#3-common-design-architecture-of-dl-compilers aria-label="3. COMMON DESIGN ARCHITECTURE OF DL COMPILERS">3. COMMON DESIGN ARCHITECTURE OF DL COMPILERS</a><ul><li><a href=#41-high-level-ir aria-label="4.1. High-level IR">4.1. High-level IR</a></li><li><a href=#42-low-level-ir aria-label="4.2. Low-level IR">4.2. Low-level IR</a></li><li><a href=#43-frontend-optimizations aria-label="4.3. Frontend Optimizations">4.3. Frontend Optimizations</a><ul><li><a href=#431-node-level-optimizations aria-label="4.3.1. Node-level optimizations">4.3.1. Node-level optimizations</a></li><li><a href=#432-block-level-optimizations aria-label="4.3.2. Block-level optimizations">4.3.2. Block-level optimizations</a></li><li><a href=#433-dataflow-level-optimizations aria-label="4.3.3. Dataflow-level optimizations">4.3.3. Dataflow-level optimizations</a></li></ul></li><li><a href=#44-backend-optimizations aria-label="4.4. Backend Optimizations">4.4. Backend Optimizations</a></li></ul></li><li><a href=#5-future-directions aria-label="5. FUTURE DIRECTIONS">5. FUTURE DIRECTIONS</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{let e=null;const t=window.innerHeight+window.pageYOffset>=document.body.offsetHeight-100;if(t)e=elements[elements.length-1];else{let t=null,n=1/0;elements.forEach(e=>{const s=getOffsetTop(e)-window.pageYOffset;if(s<=window.innerHeight*.3){const o=Math.abs(s);o<n&&(n=o,t=e)}}),e=t||elements[0]}if(e&&e!==activeElement){if(activeElement){const t=encodeURI(activeElement.getAttribute("id")).toLowerCase(),e=document.querySelector(`.inner ul li a[href="#${t}"]`);e&&e.classList.remove("active")}activeElement=e;const n=encodeURI(activeElement.getAttribute("id")).toLowerCase(),t=document.querySelector(`.inner ul li a[href="#${n}"]`);t&&(t.classList.add("active"),document.getElementById("toc-container").classList.contains("wide")&&scrollTocToActiveItem(t))}},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}function scrollTocToActiveItem(e){const t=document.querySelector(".toc .inner");if(!t||!e)return;const n=t.getBoundingClientRect(),s=e.getBoundingClientRect(),o=n.height/2,i=s.top-n.top+t.scrollTop,a=i-o;t.scrollTo({top:Math.max(0,a),behavior:"smooth"})}</script><div class=post-content><blockquote><p>Reference: <a href=https://arxiv.org/pdf/2002.03794>The Deep Learning Compiler: A Comprehensive Survey</a></p></blockquote><h2 id=1-abstract>1. ABSTRACT<a hidden class=anchor aria-hidden=true href=#1-abstract>#</a></h2><p>The DL compilers take the DL models described in different DL frameworks as input, and then generate optimized codes for diverse DL hardware as output.</p><p>Generally, the DL hardware can be divided into the following categories:</p><ol><li>General-purpose hardware with software-hardware co-design;</li><li>Dedicated hardware fully customized for DL models;</li><li>Neuromorphic hardware inspired by biological brain science.</li></ol><p>However, the drawback of relying on the libraries is that they usually fall behind the rapid development of DL models, and thus fail to utilize the DL chips efficiently.</p><p>To address the drawback of DL libraries and tools, as well as alleviate the burden of optimizing the DL models on each DL hardware manually, the DL community has resorted to the domain specific compilers for rescue.</p><p>The DL compilers take the model definitions described in the DL frameworks as
inputs, and generate efficient code implementations on various DL hardware as outputs.</p><h2 id=2-background>2. BACKGROUND<a hidden class=anchor aria-hidden=true href=#2-background>#</a></h2><h3 id=21-deep-learning-frameworks>2.1. Deep Learning Frameworks<a hidden class=anchor aria-hidden=true href=#21-deep-learning-frameworks>#</a></h3><p>&mldr;</p><h3 id=23-hardware-specific-dl-code-generator>2.3. Hardware-specific DL Code Generator<a hidden class=anchor aria-hidden=true href=#23-hardware-specific-dl-code-generator>#</a></h3><p>Field Programmable Gate Arrays (FPGAs) are reprogrammable integrated circuits that contain an array of programmable logic blocks. Programmers can configure them after manufacturing.</p><p>The FPGA can bridge the gap between CPUs/GPUs and ASICs, which causes the FPGA to be an attractive platform for deep learning.</p><p>Mapping DL models to FPGAs remains a complicated work even with HLS, because:</p><ol><li>DL models are usually described by the languages of DL frameworks rather than bare mental C/C++ code;</li><li>DL-specific information and optimizations are hard to be leveraged.</li></ol><p>The hardware-specific code generator targeting FPGA take the DL models or their <strong>domain-specific languages</strong> (DSLs) as the input, conduct the domain-specific (about FPGA and DL) optimizations and mappings, then generate the HLS or Verilog/VHDL and finally generate the bitstream. They can be classified into two categories according to the generated architectures of FPGA-based accelerators: the processor architecture and the streaming architecture.</p><p><strong>The processor architecture</strong> has similarities with general-purpose processors. An FPGA accelerator of this architecture usually comprises several Processing Units (PUs), which are comprised of on-chip buffers and multiple smaller Processing Engines (PEs).</p><p><strong>The streaming architecture</strong> has similarities with pipelines. An FPGA accelerator of this architecture consists of multiple different hardware blocks, and it nearly has one hardware block for each layer of an input DL mode</p><h2 id=3-common-design-architecture-of-dl-compilers>3. COMMON DESIGN ARCHITECTURE OF DL COMPILERS<a hidden class=anchor aria-hidden=true href=#3-common-design-architecture-of-dl-compilers>#</a></h2><p><img alt=fig-1 loading=lazy src=/imgs/blogs/mlir/common-design-architecture-of-dl-compiler.png></p><p><em>Figure 1. Common Design Architecture of DL Compilers.</em></p><h3 id=41-high-level-ir>4.1. High-level IR<a hidden class=anchor aria-hidden=true href=#41-high-level-ir>#</a></h3><p><strong>DAG-based IR</strong> - DAG-based IR is one of the most traditional ways for the compilers to build a computation graph, with nodes and edges organized as a directed acyclic graph (DAG). In DL compilers, the nodes of a DAG represent the atomic DL operators (convolution, pooling, etc.), and the edges represent the tensors. And the graph is acyclic without loops, which differs from the data dependence graphs (DDG) of generic compilers.</p><p><strong>Let-binding-based IR</strong> - Let-binding is one method to solve the semantic ambiguity by offering let expression to certain functions with restricted scope used by many high-level programming languages such as Javascript, F#, and Scheme. When using the <code>let</code> keyword to define an expression, a let node is generated, and then it points to the operator and variable in the expression instead of just building computational relation between variables as a DAG.</p><p><strong>Representing Tensor Computation</strong> - Different graph IRs have different ways to represent the computation on tensors:</p><ul><li>Function Based</li><li>Lambda Based</li><li>Einstein notation</li></ul><p><strong>Data representation</strong> - The data in DL compilers (e.g., inputs, weights, and intermediate data) are usually organized in the form of tensors, which are also known as multi-dimensional arrays. The DL compilers can represent tensor data directly by memory pointers, or in a more flexible way by placeholders. A placeholder contains the size for each dimension of a tensor. Alternatively, the dimension sizes of the tensor can be marked as unknown. For optimizations, the DL compilers require the data layout information. In addition, the bound of iterators should be inferred according to the placeholders.</p><h3 id=42-low-level-ir>4.2. Low-level IR<a hidden class=anchor aria-hidden=true href=#42-low-level-ir>#</a></h3><p>Low-level IR describes the computation of a DL model in a more fine-grained representation than that in high-level IR, which enables the target-dependent optimizations by providing interfaces to tune the computation and memory access.</p><p><strong>Halide-based IR</strong> - Halide is firstly proposed to parallelize image processing, and it is proven to be extensible and efficient in DL compilers (e.g., TVM). The fundamental philosophy of Halide is the separation of computation and schedule.</p><p><strong>Polyhedral-based IR</strong> - The polyhedral model is an important technique adopted in DL compilers. It uses linear programming, affine transformations, and other mathematical methods to optimize loop-based codes with static control flow of bounds and branches.</p><h3 id=43-frontend-optimizations>4.3. Frontend Optimizations<a hidden class=anchor aria-hidden=true href=#43-frontend-optimizations>#</a></h3><p>After constructing the computation graph, the frontend applies graph-level optimizations.</p><p>The frontend optimizations are usually defined by <strong>passes</strong>, and can be applied by traversing the nodes of the computation graph and performing the graph transformations:</p><ol><li>Capture the specific features from the computation graph;</li><li>Rewrite the graph for optimization.</li></ol><p><img alt=fig-2 loading=lazy src=/imgs/blogs/mlir/computation-graph-optimization.png></p><p><em>Figure 2. Example of computation graph optimizations, taken from the HLO graph of AlexNet on Volta GPU using TensorFlow XLA.</em></p><h4 id=431-node-level-optimizations>4.3.1. Node-level optimizations<a hidden class=anchor aria-hidden=true href=#431-node-level-optimizations>#</a></h4><p>The nodes of the computation graph are coarse enough to enable optimizations inside a single node. And the node-level optimizations include node elimination that eliminates unnecessary nodes and node replacement that replaces nodes with other lower-cost nodes.</p><h4 id=432-block-level-optimizations>4.3.2. Block-level optimizations<a hidden class=anchor aria-hidden=true href=#432-block-level-optimizations>#</a></h4><p><strong>Algebraic simplification</strong></p><p>The algebraic simplification opti- mizations consist of :</p><ol><li>algebraic identification;</li><li>strength reduction, with which we can replace more expensive operators by cheaper ones;</li><li>constant folding, with which we can replace the constant expressions by their values.</li></ol><p>Such optimizations consider a sequence of nodes, then take advantage of commutativity, associativity, and distributivity of different kinds of nodes to simplify the computation.</p><p><strong>Operator fusion</strong></p><p>Operator fusion is indispensable optimization of DL compilers. It enables better sharing of computation, eliminates intermediate allocations, facilitates further optimization by combining loop nests, as well as reduces launch and synchronization overhead.</p><p><strong>Operator sinking</strong></p><p>This optimization sinks the operations such as transposes below operations such as batch normalization, ReLU, sigmoid, and channel shuffle. By this optimization, many similar operations are moved closer to each other, creating more opportunities for algebraic simplification.</p><h4 id=433-dataflow-level-optimizations>4.3.3. Dataflow-level optimizations<a hidden class=anchor aria-hidden=true href=#433-dataflow-level-optimizations>#</a></h4><ul><li>Common sub-expression elimination (CSE)</li><li>Dead code elimination (DCE)</li><li>Static memory planning - Static memory planning optimizations are performed to reuse the memory buffers as much as possible. Usually, there are two approaches: in-place memory sharing and standard memory sharing.</li><li>Layout transformation - Layout transformation tries to find the best data layouts to store tensors in the computation graph and then inserts the layout transformation nodes to the graph.</li></ul><h3 id=44-backend-optimizations>4.4. Backend Optimizations<a hidden class=anchor aria-hidden=true href=#44-backend-optimizations>#</a></h3><p>The backends of DL compilers have commonly included various hardware-specific optimizations, auto-tuning techniques, and optimized kernel libraries. Hardware-specific optimizations enable efficient code generation for different hardware targets. Whereas, auto-tuning has been essential in the compiler backend to alleviate the manual efforts to derive the optimal parameter configurations. Besides, highly-optimized kernel libraries are also widely used on general-purpose processors and other customized DL accelerators.</p><p><img alt=fig-3 loading=lazy src=/imgs/blogs/mlir/hardware-specific-optimization.png></p><p><em>Figure 3. Overview of hardware-specific optimizations applied in DL compilers.</em></p><h2 id=5-future-directions>5. FUTURE DIRECTIONS<a hidden class=anchor aria-hidden=true href=#5-future-directions>#</a></h2><ol><li>Dynamic shape and pre/post processing</li><li>Advanced auto-tuning</li><li>Polyhedral model</li><li>Subgraph partitioning</li><li>Quantization</li><li>Unified optimizations</li><li>Differentiable programming</li><li>Privacy protection</li><li>Training support</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://jamesnulliu.github.io/tags/mlir/>Mlir</a></li></ul><nav class=paginav><a class=prev href=https://jamesnulliu.github.io/blogs/learning-notes-mlir/setup-the-environment-of-mlir/><span class=title>« Prev</span><br><span>Setup the Environment of MLIR</span>
</a><a class=next href=https://jamesnulliu.github.io/blogs/user-management-on-linux/><span class=title>Next »</span><br><span>User Management on Linux</span></a></nav></footer><script src=https://giscus.app/client.js data-repo=jamesnulliu/jamesnulliu.github.io data-repo-id=R_kgDOMPCQIw data-category=Announcements data-category-id=DIC_kwDOMPCQI84Cgb2t data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async></script></article></main><footer class=footer><span>© 2024-2025 JamesNULLiu</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>