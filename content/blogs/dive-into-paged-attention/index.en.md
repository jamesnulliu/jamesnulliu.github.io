---
title: "Dive into Paged Attention"
date: 2024-10-07T12:00:00+08:00
lastmod: 2024-11-18T18:45:00+08:00
draft: false
author: ["jamesnulliu"]
keywords: 
    - vllm
    - continuous-batching
    - paged-attention
categories:
    - deeplearning
tags:
    - python
    - vllm
    - attention
description: Dive into the paged attention mechanism of vLLM.
summary: Dive into the paged attention mechanism of vLLM.
comments: true
images: 
cover:
    image: ""
    caption: ""
    alt: ""
    relative: true
    hidden: true
---

## 1. è¯æ˜ Attention çš„ $O_i$ åªä¸ $Q_i$ æœ‰å…³

Attention çš„å…¬å¼å¦‚ä¸‹:

$$
O=Attention(Q,K,V)=softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

å‡è®¾ $Q=\begin{bmatrix}Q_0\\Q_1\end{bmatrix}$, $K=\begin{bmatrix}K_0\\K_1\end{bmatrix}$

é‚£ä¹ˆ:

$$
O=softmax(\frac{\begin{bmatrix}Q_0K_0^T&Q_0K_1^T\\Q_1K_0^T&Q_1K_1^T\end{bmatrix}}{\sqrt{d_k}})V
$$

ä»¤:

$$
A=\begin{bmatrix}A_0\\A_1\end{bmatrix}=\begin{bmatrix}Q_0K_0^T&Q_0K_1^T\\Q_1K_0^T&Q_1K_1^T\end{bmatrix},f(x)=\frac{softmax(x)}{\sqrt{d_k}}
$$

æ­¤æ—¶, $A_1$ åªå’Œ $Q_1$ æœ‰å…³, å’Œ $Q_0$ æ— å…³, é‚£ä¹ˆ:

$$
\begin{bmatrix}O_0\\O_1\end{bmatrix}=O=\begin{bmatrix}f(A_0)\\f(A_1)\end{bmatrix}V=\begin{bmatrix}f(A_0)V\\f(A_1)V\end{bmatrix}
$$

å› æ­¤, $O_i$ åªå’Œ  $A_i$ ç›¸å…³, è€Œæ ¹æ® $A$ çš„è®¾å®š, $A_i$ åªå’Œ $Q_i$ ç›¸å…³, å³:

Attention çŸ©é˜µçš„ç¬¬ $i$ ä¸ªè¾“å‡ºåªå’Œç¬¬ $i$ ä¸ª $Q$ æœ‰å…³, å’Œä¹‹å‰çš„ $Q$ æ— å…³.

**æ€»ç»“**:

- åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶ï¼Œåªéœ€å¯¹æ–° token è®¡ç®—å¯¹åº”çš„ `Q_new`ï¼Œå¹¶ä¸ä¹‹å‰å·²ç»ç¼“å­˜çš„ `K_cache` å’Œ `V_cache` è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚
- æ–°çš„ `K_new` å’Œ `V_new` ä¼šè¢«åŠ å…¥åˆ°ç¼“å­˜ä¸­ï¼Œç»§ç»­ä¸ºä¸‹ä¸€ä¸ª token ç”Ÿæˆæä¾›åŸºç¡€ã€‚
- æ•´ä¸ªè¿‡ç¨‹é¿å…äº†å¯¹æ‰€æœ‰å†å² token çš„é‡å¤è®¡ç®—ï¼Œå¤§å¹…æé«˜äº†æ•ˆç‡ã€‚

## 2. KV Cache çš„å¢é‡è¿‡ç¨‹
### 2.1. åˆå§‹è¾“å…¥ï¼ˆå®Œæ•´åºåˆ—ï¼‰è®¡ç®—ï¼š

- å¯¹äºåˆå§‹çš„è¾“å…¥åºåˆ— `(seq_len, embed_dim)`ï¼Œæˆ‘ä»¬é€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ° `Q`ã€`K` å’Œ `V`ï¼Œå®ƒä»¬çš„å½¢çŠ¶éƒ½æ˜¯ `(seq_len, embed_dim)`ã€‚
- ä½¿ç”¨ `Q` å’Œ `K` è¿›è¡Œç‚¹ç§¯è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œç„¶åç»“åˆ `V` è®¡ç®—å¾—åˆ°è¾“å‡º `(seq_len, embed_dim)`ï¼Œè¿™æ˜¯ç¬¬ä¸€æ¬¡å¯¹åˆå§‹åºåˆ—çš„å®Œæ•´è®¡ç®—ã€‚

### 2.2. é¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶çš„å¢é‡è®¡ç®—ï¼š

åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶ï¼Œä¸éœ€è¦å¯¹æ•´ä¸ªåºåˆ—å†è¿›è¡Œå®Œæ•´çš„ `Q`ã€`K`ã€`V` è®¡ç®—ï¼Œè€Œæ˜¯åªéœ€å¯¹æ–°ç”Ÿæˆçš„ token è¿›è¡Œä¸€æ¬¡å¢é‡è®¡ç®—ã€‚è¿™æ—¶çš„æ“ä½œæµç¨‹å¦‚ä¸‹ï¼š

1. **è¾“å…¥æ–°çš„ token**ï¼šå°†å·²ç»ç”Ÿæˆçš„ tokenï¼ˆå…¶å½¢çŠ¶ä¸º `(embed_dim,)`ï¼‰ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ°è¯¥ token å¯¹åº”çš„ `Q_new`ï¼Œå½¢çŠ¶ä¸º `(embed_dim,)`ã€‚
2. **ä¸ä¹‹å‰ç¼“å­˜çš„ `K` å’Œ `V` è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—**ï¼šä½¿ç”¨ `Q_new` ä¸ä¹‹å‰å·²ç»è®¡ç®—å¹¶ç¼“å­˜çš„ `K_cache` å’Œ `V_cache` è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚è¿™é‡Œçš„ `K_cache` å’Œ `V_cache` åˆ†åˆ«æ˜¯ä¹‹å‰æ¯æ¬¡ç”Ÿæˆ token æ—¶å¾—åˆ°çš„ `K` å’Œ `V`ï¼Œå®ƒä»¬çš„å½¢çŠ¶æ˜¯ `(seq_len, embed_dim)`ï¼Œå³ç¼“å­˜äº†ä»æœ€åˆè¾“å…¥åºåˆ—åˆ°å½“å‰å·²ç»ç”Ÿæˆçš„æ‰€æœ‰ token çš„ `K` å’Œ `V`ã€‚`Q_new` å¯ä»¥ç›´æ¥ä¸ `K_cache` è¿›è¡Œç‚¹ç§¯ï¼Œå¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°ï¼Œç„¶åç»“åˆ `V_cache` å¾—åˆ°æ–°çš„è¾“å‡ºã€‚
3. **æ›´æ–° `KV Cache`**ï¼šæ–°çš„ `K_new` å’Œ `V_new` ä¼šé€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ°ï¼ˆå½¢çŠ¶ä¸º `(embed_dim,)`ï¼‰ï¼Œå¹¶å°†å®ƒä»¬æ·»åŠ åˆ° `K_cache` å’Œ `V_cache` çš„æœ«å°¾ï¼Œä½¿å¾—ç¼“å­˜çš„ `K_cache` å’Œ `V_cache` ä¸æ–­å¢å¤§ï¼Œä»¥å¤‡åç»­ä½¿ç”¨ã€‚
1. **è¾“å‡º**ï¼šé€šè¿‡æ³¨æ„åŠ›è®¡ç®—åçš„è¾“å‡ºå½¢çŠ¶ä¸º `(embed_dim,)`ï¼Œå³æ–°ç”Ÿæˆçš„ tokenã€‚

## 4. vllm ä¸­çš„ Paged Attention

### 4.1. åŠ¨æœº: Memory Wastes

![memory-wastes.png](/imgs/blogs/dive-into-paged-attention/memory-wastes.png)

ä¸Šå›¾å±•ç¤ºäº†å¯èƒ½çš„å†…å­˜æµªè´¹æƒ…å†µ, ä¸»è¦æ—¶è¾“å…¥ sequence ä¸çŸ¥é“ eos åœ¨å“ªé‡Œ, å¦‚æœéšæœºç”³è¯·å†…å­˜, å¯èƒ½å¯¼è‡´å¤§é‡å†…å­˜ç¢ç‰‡, å› æ­¤ååé‡ä¸‹é™.

### 4.2. è§£å†³æ–¹æ¡ˆ: ç”¨ Page ç®¡ç†å†…å­˜

![paged-attention-animation.webp](/imgs/blogs/dive-into-paged-attention/paged-attention-animation.webp)

ä¸Šå›¾å±•ç¤ºäº† vLLM ç”¨ Paged ç®¡ç†å†…å­˜å…·ä½“æ€ä¹ˆåšçš„.

ç®€å•æ¥è¯´, vLLM åœ¨å¼€å§‹æ¨ç†å‰ä¸ºæ¯ä¸ª Decoder Layer ç”³è¯·ä¸¤ä¸ªå·¨é•¿çš„ Tensor (`k_cache` å’Œ `v_cache`), æŠŠ Tensor åˆ†å‰²æˆè¿ç»­ç­‰é•¿çš„ PA blocks (å›¾ä¸­çš„ä¸€è¡Œä¸ºä¸€ä¸ª PA Block); æ¯ä¸ª PA Block èƒ½å¤Ÿå­˜æ”¾ `BLOCK_SIZE` ä¸ª token çš„ K æˆ– V cache (æ¯ä¸ª cache çš„å½¢çŠ¶å¯ä»¥ç†è§£ä¸º `(num_heads, head_size)`).

å› æ­¤, `k_cache` å’Œ `v_cache` çš„å½¢çŠ¶å¯ä»¥ç†è§£ä¸º `(num_blocks, num_heads, head_size)`.

å¯¹äºä¸€ä¸ªè¿ç»­çš„ sequnce, åœ¨ prefill é˜¶æ®µå‰å°±ä¼šåˆ†é…å¥½å®ƒçš„ PA blocks, ä¹‹åæ¨ç†æ—¶:

- è‹¥æ˜¯è®¡ç®— prompt çš„ Attention, åˆ™å…ˆæŠŠä¼ å…¥çš„ K å’Œ V æŒ‰ç…§ PA blocks å­˜å…¥ `k_cache` å’Œ `v_cache` ä¸­; ç„¶ååˆ©ç”¨æ•´æ®µçš„ QKV è®¡ç®— attention.
- è‹¥æ˜¯è®¡ç®—æ–° token, åˆ™åˆ©ç”¨ Q å’Œ block table è®¡ç®— decode é˜¶æ®µçš„ attntion; æ­¤æ—¶è®¿å­˜çš„å°±æ˜¯ `k_cache` å’Œ `v_cache` ä¸­çš„ PA blocks.

## 5. Paged Attention Kernel è¯¦è§£

> References:
>   - [vLLM Paged Attention](https://docs.vllm.ai/en/latest/dev/kernel/paged_attention.html)
>   - [vLLMçš‡å† ä¸Šçš„æ˜ç ï¼šæ·±å…¥æµ…å‡ºç†è§£PagedAttention CUDAå®ç°](https://zhuanlan.zhihu.com/p/673284781)

å…ˆçœ‹ä¸‹æ•´ä½“è®¡ç®—æµç¨‹å›¾ (è¿™ä¸ªå›¾åé¢ä¹Ÿä¼šå‡ºç°è¿™é‡Œå…ˆçœ‹ä¸€çœ¼):

![pa-cal.png](/imgs/blogs/dive-into-paged-attention/pa-cal.png)

### 5.1. è¾“å…¥è¾“å‡ºè¾“å‡ºåˆ†æå’Œå‚æ•°è¯´æ˜

```cpp
// Grid: (num_heads, num_seqs, 1).
template<
typename scalar_t,
int HEAD_SIZE,
int BLOCK_SIZE,
int NUM_THREADS,
int PARTITION_SIZE = 0>
__device__ void paged_attention_kernel(
... // Other side args.
const scalar_t* __restrict__ out,       // [num_seqs, num_heads, max_num_partitions, head_size]
const scalar_t* __restrict__ q,         // [num_seqs, num_heads, head_size]
const scalar_t* __restrict__ k_cache,   // [num_blocks, num_kv_heads, head_size/x, block_size, x]
const scalar_t* __restrict__ v_cache,   // [num_blocks, num_kv_heads, head_size, block_size]
... // Other side args.
)
```

æ¨¡æ¿å‚æ•°è¯´æ˜:

- `scalar_t` å…ƒç´ ç±»å‹ (å®é™…ä»£ç ä¸­è¿˜æœ‰ `cache_t` è¡¨ç¤º KV cache çš„å…ƒç´ ç±»å‹).
- `HEAD_SIZE` æ¯ä¸ª head ä¸­å…ƒç´ æ•°é‡.
- `BLOCK_SIZE` æ¯ä¸ª PA block ä¸­çš„ token æ•°é‡.
  >  1. KV cache è¢«å­˜å‚¨åœ¨ä¸åŒ PA blocks. æ¯ä¸ª PA block å­˜å‚¨ä¸€ä¸ª head ä¸­ `BLOCK_SIZE` ä¸ª token.  
  >     ä¾‹å¦‚, è‹¥ `BLOCK_SIZE=16`, `HEAD_SIZE=128`, åˆ™ä¸€ä¸ª  PA block èƒ½å­˜å‚¨ä¸€ä¸ª head çš„ `16 * 128 = 2048` ä¸ªå…ƒç´ . 
  >  2. æ¯ä¸ª PA block å¯èƒ½åªåŒ…å«ä¸€éƒ¨åˆ†çš„ context tokens.  
  >  3. ä» page è§’åº¦çœ‹, KV cache æ˜¯è‹¥å¹²ä¸ª page çš„é›†åˆ; 
- `NUM_THREADS` æ¯ä¸ª CUDA thread block ä¸­ thread çš„æ•°é‡.
- `PARTITION_SIZE` å‚ä¸ TP çš„ GPU æ•°é‡, é»˜è®¤ 0 è¡¨ç¤ºå•å¡. (ä»¥ä¸‹éƒ½ä»¥å•å¡ä¸ºä¾‹è¯´æ˜)

é¢å¤–çš„ä¸€äº›å‚æ•°:

- `num_seqs`: æœ¬æ¬¡æ¨ç†è¯·æ±‚ sequence æ•°ç›®.
  > ç”±äºè¿™ä¸ª kernel åªå¤„ç† decode é˜¶æ®µå• query attention, æ‰€ä»¥å®é™…ä¸Šæ¯ä¸ª sequence åªæœ‰ä¸€ä¸ª query token. 
- `num_heads`: Q çš„ head æ•°ç›®
- `num_kv_heads`: KV çš„ head æ•°ç›®, å¯¹äº MHA å…¶å€¼å’Œ `num_heads` ç›¸åŒ; å¦‚æœæ˜¯ GQA, MQA åˆ™ `num_kv_heads` å°äº `num_head`.
- `head_size`: å³ `HEAD_SIZE`
- `k_cache: (num_blocks, num_kv_heads, head_size/x, block_size, x)`, å…¶ä¸­ `x` è¡¨ç¤º `THREAD_GROUP_SIZE * VEC_SIZE` çš„å¤§å° (åé¢ä¼šç»†è¯´).

ä¸‹é¢ç»“åˆ GPU architecture åˆæ­¥åˆ†æä¸€ä¸‹å‚æ•°.

![gpu-archi.png](/imgs/blogs/dive-into-paged-attention/gpu-archi.png)

ğŸ§ **ä¸ºä»€ä¹ˆè¦åˆ† thread group?**  
- å› ä¸ºå½“ä¸€ä¸ª cuda block è¦å–çš„æ•°æ®æ¯”è¾ƒå°‘çš„æ—¶å€™ (è®¡ç®— QK), ä¸€ä¸ª thread group åˆ†åˆ«ä¸€æ¬¡å– Q å’Œ K ä¸­ 16B; å½“ä¸€ä¸ª cuda block è¦å–çš„æ•°æ®æ¯”è¾ƒå¤šçš„æ—¶å€™ (è®¡ç®— LV), ä¸€ä¸ª thread å– 16B.

### 5.2.Shared Memory: `q_vecs` çš„å†™å…¥

ä» kernel ä¸­çš„ç¬¬ä¸€ä¸ªç”³è¯·çš„ shared memory å¼€å§‹è¯´.

> å…³äº shared memeory:
> 1. åœ¨ kernel ä¸­ç”³è¯·çš„ shared memory è¢«å½“å‰ cuda block ä¸­çš„æ‰€æœ‰ thread å…±äº«.
> 2. shared memory çš„ä½œç”¨æ˜¯ä¸ºäº†å‡å°‘ global memory çš„è®¿é—®æ¬¡æ•°ï¼Œæé«˜è®¿å­˜æ•ˆç‡.

ä»¥ä¸‹ä»£ç ç”³è¯·äº†ä¸€å— shared memroy è¢«æ•´ä¸ª CUDA Block ä¸­æ‰€æœ‰ kernel å…±äº«:

```cpp
__shared__ Q_vec q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];
```

é¦–å…ˆ, `q_vecs` è¦†ç›–äº† Q ä¸­ `head_size` ä¸ªå…ƒç´  - è¿™ä¹Ÿæ˜¯ä¸€ä¸ª cuda block éœ€è¦å¤„ç†çš„æ•°æ®é‡.

æ¥ç€å†è¯´ä¸¤ä¸ªç»´åº¦çš„å‚æ•°çš„æ„æ€:

```cpp
constexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE;
constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;
```

- `THREAD_GROUP_SIZE`: æ¯ä¸ª thread group ä¸­çš„ thread æ•°é‡. æ³¨æ„, ä¸€ä¸ª cuda block ä¸­æœ‰ `NUM_THREADS` ä¸ª thread, `NUM_THREAD_GROUPS` ä¸ª thread group. `THREAD_GROUP_SIZE = MAX(WARP_SIZE/BLOCK_SIZE, 1)`.
- `NUM_VECS_PER_THREAD`: `HEAD_SIZE` èƒ½è¢«åˆ†æˆå¤šå°‘ä¸ª 16B. (è¿™ä¸ªå˜é‡è¿™ä¹ˆå‘½åçš„ç†ç”±æ˜¯åé¢è¯»å– K çš„æ—¶å€™æ¯ä¸ª thread ä¼šå¾€è‡ªå·±çš„å¯„å­˜å™¨å†…è¯» `NUM_VECS_PER_THREAD` ä¸ª k_vec.)

> è¯æ˜: `q_vecs` è¦†ç›– Q çš„ä¸€ä¸ª head, å¹¶ä¸” `NUM_VECS_PER_THREAD` è¡¨ç¤º Q çš„ä¸€ä¸ª head è¢«åˆ†æˆå¤šå°‘ä¸ª 16B.  
> => `THREAD_GROUP_SIZE` * `VEC_SIZE` = 16B / `sizeof(scalar_t)`;  
> => `NUM_VECS_PER_THREAD` * 16B / `sizeof(scalar_t)` = `HEAD_SIZE`;

ç„¶åçœ‹ load Q çš„ä»£ç , å»ºè®®ç»“åˆä¸‹é¢çš„å›¾ä¸€èµ·çœ‹:

```cpp
  // Load Q to shmem
#pragma unroll
for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;
        i += NUM_THREAD_GROUPS) {
    const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
    q_vecs[thread_group_offset][i] =
        *reinterpret_cast<const Q_vec*>(q_ptr + vec_idx * VEC_SIZE);
}
```

- `thread_group_idx` è¡¨ç¤ºå½“å‰ thread å±äºå½“å‰ cuda block ä¸­ç¬¬å‡ ä¸ª thread group.
- `thread_group_offset` è¡¨ç¤ºå½“å‰ thread åœ¨å½“å‰ thread group ä¸­æ˜¯ç¬¬å‡ ä¸ª thread.

![pa-load-q.png](/imgs/blogs/dive-into-paged-attention/pa-load-q.png)

ä¸Šå›¾å±•ç¤ºäº†å¾ªç¯å…·ä½“æ˜¯æ€ä¹ˆè·‘çš„.  

- ä¸€ä¸ªç´«è‰²ç®­å¤´è¡¨ç¤ºä¸€ä¸ª thread group.
- `NUM_VECS_PER_THREAD` è¡¨ç¤º `HEAD_SIZE` èƒ½è¢«åˆ†æˆå¤šå°‘ä¸ª 16B. 
- å®é™…è¯»å– Q çš„å†…å­˜æ—¶, æ‰€æœ‰ thread group ä» Q çš„èµ·å§‹ä½ç½®ç´§å¯†æ’åˆ—, æ ¹æ®å›¾ä¸Šçœ‹çš„è¯ä¸€å…±æœ‰ `NUM_THREAD_GROUPS` ä¸ªç´«è‰²ç®­å¤´.
- æ‰€æœ‰ thread group è¯»å–ä¸€æ¬¡ Q å¹¶å­˜å…¥ `q_vecs` å¯¹åº”å¾ªç¯ä¸­çš„ä¸€æ¬¡è¿­ä»£; å› æ­¤ä¸‹æ¬¡è¿­ä»£ thread group éœ€è¦å‘ååç§» `NUM_THREAD_GROUPS` ä¸ªä½ç½® (ä¾‹å¦‚ `i` ä» 1 å˜ä¸º 7).
- æ­¤å¤–, è¯»ä¸€æ¬¡ 16B å¯¹åº”ä¸€ä¸ª thread æ¥è¯´è‡ªç„¶ä¹Ÿæ˜¯å–ä¸€ä¸ª VEC.
- å¯¹åº”åˆ° kernel ç¼–å†™, è¿˜éœ€è¦è®¡ç®—å½“å‰ thread å…·ä½“è¯»å–å“ªä¸ª vec; å› æ­¤å¾—åˆ° `vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE`.

> ğŸ¤” è¿™é‡Œä¼šä¸ä¼šæœ‰ bank conflict?

æ€»ä¹‹ç°åœ¨æˆ‘ä»¬æŠŠ `(1, head_size)` å¤§å°çš„å…ƒç´ è¯»åˆ°äº† cuda block å…±äº«çš„ shared memory `q_vecs` ä¸­.


### 5.3. è¯»å– K Cache å¹¶è®¡ç®— QK

ç°åœ¨ä» cuda block çš„è§’åº¦çœ‹, å½“å‰ block å·²ç»è·å¾—äº†è‡ªå·±è¦ç®—çš„ Q ä¸­çš„ä¸€ä¸ª head (å½¢çŠ¶ä¸º `(1, head_size)`), æ¥ä¸‹æ¥å°±æ˜¯è®¡ç®— Q å’Œ K çš„ç‚¹ç§¯.

ç‚¹ç§¯è¿‡ç¨‹æ˜¯æŠŠå½“å‰ block æ‹¥æœ‰çš„ Q head å’Œæ•´ä¸ª K Cache (è¿­ä»£åœ°) è¿›è¡Œç‚¹ç§¯è¿ç®—. å‚è€ƒä¸‹å›¾:

![pa-cal-kq-01.png](/imgs/blogs/dive-into-paged-attention/pa-cal-kq-01.png)

QK ä¹˜ç§¯å®é™…ä¸Šè¢«æš‚å­˜åœ¨ `logits` (ä¹Ÿæ˜¯ä¸€å— shared memory) ä¸­, ä¹‹åä¼šè¢«ç”¨æ¥è®¡ç®— softmax.

ğŸ˜‡ çœ‹ä¸‹å¾ªç¯çš„å…·ä½“ä»£ç å§:

```cpp
// Loop 1
for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;
        block_idx += NUM_WARPS) {
    // Physical block calculation ...
    // Loop 2
    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
        // Offset calculation ...
        K_vec k_vecs[NUM_VECS_PER_THREAD];
        // Loop 3
        #pragma unroll
        for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
            // Load K to `k_vecs` ...
        }
        float qk = scale * Qk_dot<scalar_t, THREAD_GROUP_SIZE>::dot(
                        q_vecs[thread_group_offset], k_vecs);
        // Add the ALiBi bias if slopes are given.
        qk += (alibi_slope != 0) ? alibi_slope * (token_idx - seq_len + 1) : 0;
        if (thread_group_offset == 0) {
            // Store the partial reductions to shared memory.
            // Mask
            // Update the max value.
        }
    }
}
```    

å…ˆè¯´ç¬¬ä¸€ä¸ªå¾ªç¯, å…¶ä¸­æ¯”è¾ƒé‡è¦çš„å‡ ä¸ªå‚æ•°å®šä¹‰å¦‚ä¸‹:

```cpp
// [start_block_idx, end_block_idx) is the range of blocks to process.
const int start_block_idx =
    USE_PARTITIONING ? partition_idx * num_blocks_per_partition : 0;
// If not using partitioning, `end_block_idx` should be equal to `num_seq_blocks`.
const int end_block_idx =
    MIN(start_block_idx + num_blocks_per_partition, num_seq_blocks);
// Number of blocks to process.
const int num_blocks = end_block_idx - start_block_idx;
```
ç”¨æ–‡å­—æè¿°å°±æ˜¯:

- `blk_idx` è¡¨ç¤ºå½“å‰ thread æ‰€åœ¨ warp éœ€è¦å¤„ç†çš„ PA block çš„åœ¨ `block_table` ä¸­ç´¢å¼• (é€»è¾‘ä¸Šçš„ç´¢å¼•).
- `start_block_idx` å’Œ `end_block_idx` è¡¨ç¤ºå½“å‰ cuda block éœ€è¦å¤„ç†çš„ block èŒƒå›´.
- `num_blocks` è¡¨ç¤ºå½“å‰ cuda block éœ€è¦å¤„ç†çš„ block æ•°é‡.
- `NUM_WARPS` è¡¨ç¤ºå½“å‰ cuda block ä¸­ warp çš„æ•°é‡. ä¸€ä¸ª warp åŒ…å« 32 ä¸ª thread.
- `warp_idx` è¡¨ç¤ºå½“å‰ warp åœ¨å½“å‰ cuda block ä¸­çš„ç´¢å¼•.

è¯´äººè¯å°±æ˜¯æ¯ä¸ª warp å¤„ç†ä¸€ä¸ª PA block, ä¸€å¼€å§‹ cuda block ä¸­çš„æ‰€æœ‰ warp ç´§å¯†åœ°æŒ‡å‘æœ€å‰é¢çš„ `NUM_WARPS` ä¸ª PA block, æ¯æ¬¡å¾ªç¯æ‰€æœ‰ warp å‘ååç§» `NUM_WARPS` ä¸ª PA block çš„é•¿åº¦. å‚è€ƒä¸‹å›¾:

![pa-cal-kq-02.png](/imgs/blogs/dive-into-paged-attention/pa-cal-kq-02.png)

> ğŸ”” è¿™é‡Œå†å›é¡¾ä¸€ä¸‹, ä¸€ä¸ª PA block é‡Œå­˜æ”¾äº† `BLOCK_SIZE` ä¸ª token çš„ K æˆ– V cache.

æ‰€ä»¥è¯´è¿™ä¸ªå¾ªç¯å’Œä¸Šé¢è¯»å– Q çš„å¾ªç¯ä¸€ä¸ªå°¿æ€§ğŸ¤®, ä¸è¿‡æ˜¯ä»¥ warp çš„ç²’åº¦å¤„ç†æ•°æ®;  

è¿›å…¥äº†ç¬¬ä¸€ä¸ªå¾ªç¯å†…éƒ¨, ç¬¬ä¸€æ­¥å½“ç„¶æ˜¯è®¡ç®—å½“å‰ thread å¯¹åº”çš„ warp åº”è¯¥è®¡ç®—å“ªä¸ª PA block (ç‰©ç†ä¸Šçš„ç´¢å¼•), å› æ­¤å¾—åˆ°äº† `physical_block_number`:

```cpp
const int64_t physical_block_number =
    static_cast<int64_t>(block_table[block_idx]);
```

---

ç„¶åè§£é‡Šç¬¬äºŒä¸ªå¾ªç¯, ç¬¬äºŒä¸ªå¾ªç¯çš„æ•´ä½“ç›®æ ‡å°±æ˜¯è®©å½“å‰ warp è®¡ç®—å¥½è‡ªå·±è´Ÿè´£çš„ PA block ä¸­ `BLOCK_SIZE` ä¸ª token çš„ QK ä¹˜ç§¯. 

å…ˆçœ‹ä¸€ä¸‹ `i` çš„ä¸Šç•Œ:

```cpp
constexpr int NUM_TOKENS_PER_THREAD_GROUP =
    DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE);
// ...

// Loop 1
for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;
        block_idx += NUM_WARPS) {
    // Loop 2
    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
        // ...
    }
    // ...
}
```

ä» kernel è§’åº¦çœ‹, æ¯ä¸ª thread éœ€è¦è¾…åŠ©å½“å‰ warp è®¡ç®—è‡ªå·±è´Ÿè´£çš„ä¸€æ•´ä¸ª PA block (åŒ…å« `BLOCK_SIZE` ä¸ª token), è€Œæˆ‘ä»¬æŠŠè¿™ä¸ªè¿‡ç¨‹æ‹†åˆ†ä¸º Loop 2 ä¸­çš„ `NUM_TOKEN_PER_THREAD_GROUP` (ä¹Ÿå°±æ˜¯ `ceil(BLOCK_SIZE / 32)`) æ¬¡å¾ªç¯; 

è¯´äººè¯å°±æ˜¯**ä¸€ä¸ª thread group å¯¹åº”ä¸€ä¸ª token ä¸­çš„ä¸€ä¸ª head**, å¦‚æœ BLOCK SIZE å¤ªå¤§äº†åé¢æ¯ä¸ª thread å‘ååç§» `i * WARP_SIZE` ä¸ª token ç»§ç»­ç‹ ç‹ ç®—ğŸ¤£.

ä¹Ÿå› æ­¤ç¬¬äºŒä¸ªå¾ªç¯å†…éƒ¨ä¸€ä¸Šæ¥å…ˆè®¡ç®—äº†å‡ ä¸ªåç§»é‡, å¹¶ä¸”ç”³è¯·äº† thread å†…éƒ¨ç§æœ‰çš„ `k_vecs` æ•°ç»„:

```cpp
const int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
K_vec k_vecs[NUM_VECS_PER_THREAD];
```

- `thread_group_idx` è¡¨ç¤ºå½“å‰ thread group åœ¨æ•´ä¸ª cuda block ä¸­çš„ç´¢å¼•.
- â˜¢ï¸ ä¸€ä¸ª thread group åœ¨ä¸€æ¬¡å¾ªç¯ä¸­è´Ÿè´£ fetch ä¸€ä¸ª PA block ä¸­ K cache çš„ä¸€ä¸ª token ä¸­**è‡ªå·±è´Ÿè´£çš„ head**.
- â˜¢ï¸ ä¸€ä¸ª thread group è´Ÿè´£è®¡ç®—ä¸€ä¸ª qk å€¼; è¿™ä¸ªå€¼æ˜¾ç„¶æ˜¯ç”±ä¸€ä¸ª Q head å’Œä¸€ä¸ª K head ç‚¹ç§¯å¾—åˆ°çš„.
- `physical_block_offset` è¡¨ç¤ºå½“å‰è¦ç®—çš„ token åœ¨å½“å‰ PA block ä¸­çš„åç§»é‡ (æ³¨æ„å’Œå‰é¢çš„ `physical_block_number` åŒºåˆ†).
- åŠ  `i * WARP_SIZE` çš„åŸå› æ˜¯å¦‚æœ `BLOCK_SIZE` å¤§äº 32, é‚£ä¹ˆä¸€ä¸ª warp è¦å¤šæ¬¡å¾ªç¯æ‰èƒ½å¤„ç†å®Œä¸€ä¸ª PA block ä¸­çš„æ‰€æœ‰ token, å¯¹åº” `thread_group_idx` éœ€è¦åšåç§».
- `token_idx` è¡¨ç¤ºå½“å‰è¦ç®—çš„ token åœ¨æ•´ä¸ª seq çš„ KV cache ä¸­çš„ç´¢å¼•.
- `k_vecs` ä¸­èƒ½å­˜æ”¾ `NUM_VECS_PER_THREAD` ä¸ª VEC, è€Œä¸€æ•´ä¸ª thread group ä¸­æ‰€æœ‰çš„ thread çš„ `k_vecs` åˆèµ·æ¥æ‰èƒ½ç»„æˆä¸€ä¸ª K çš„ head (æ¨å¯¼å‚è€ƒä¸Šé¢ Q çš„ ğŸ˜‡). è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåé¢ç®— QK çš„æ—¶å€™è¦ reduce.

ğŸ¤” **çœ‹åˆ°è¿™é‡Œè¯»è€…å¯èƒ½æœ‰ä¸€ä¸ªé—®é¢˜: ä¸€ä¸ª token çš„ K cache åº”è¯¥å¯¹åº”å¤šä¸ª head, ä¸ºä»€ä¹ˆä¸Šé¢è¯´ä¸€ä¸ª thread group åªè´Ÿè´£ä¸€ä¸ª head?**  
ç­”: å› ä¸ºå®é™…è®¡ç®—çš„æ—¶å€™, ä¸€ä¸ª cuda block åªè´Ÿè´£è®¡ç®—ä¸€ä¸ª head, å¯¹åº”åˆ° K Cache ä¹ƒè‡³åé¢ V Cache çš„ä½ç½®ä¹Ÿæ˜¯ä¸€æ ·çš„.

> è¿™é‡Œé¢å¤–è¯´ä¸€ä¸‹, è¯» K çš„ head çš„ä¸€ä¸ªç›®æ ‡åº”è¯¥æ˜¯åœ¨å°½é‡å°‘çš„ register ä¸­è£…ä¸‹ä¸€ä¸ª head çš„æ‰€æœ‰å…ƒç´ , è¿™æ ·åç»­å’Œ shared memory ä¸­çš„ Q åšç‚¹ä¹˜å¹¶è§„çº¦çš„é€Ÿåº¦æ›´å¿«. å‡è®¾ä¸€ä¸ª head æœ‰ 128 ä¸ª float16, åˆ™å ç”¨ 256B, è€Œ A100 ä¸­ä¸€ä¸ª thread æœ€å¤šèƒ½æœ‰ 255 ä¸ª 32-bit register (ä¹Ÿå°±æ˜¯ 1020B), æ­¤æ—¶å¯ä»¥è®¤ä¸ºä¸€ä¸ª thread èƒ½è£…ä¸‹ä¸€ä¸ª head çš„æ‰€æœ‰å…ƒç´ .  
> ä½†æ˜¯ç”±äºç›®å‰ PA kernel åœ¨ `BLOCK_SIZE` ä¸º 16 çš„æƒ…å†µä¸‹ `THREAD_GROUP_SIZE` ç­‰äº 2, å› æ­¤ä¸€ä¸ª thread åªä¼šè£…ä¸€ä¸ª head çš„ä¸€åŠå…ƒç´ , è¿™æ ·å¯èƒ½ä¼šå¯¼è‡´ register çš„ä½¿ç”¨ç‡ä¸é«˜.

---

æ¥ç€è¿›å…¥ç¬¬ä¸‰ä¸ªå¾ªç¯, ç›®çš„æ˜¯è®© thread group ä» K cache ä¸­è¯»ä¸€ä¸ª head, å¹¶å­˜å…¥ `k_vecs` ä¸­:

```cpp
// x == THREAD_GROUP_SIZE * VEC_SIZE
// Each thread group fetches x elements from the key at a time.
constexpr int x = 16 / sizeof(cache_t);
//...
// Loop 1
for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;
        block_idx += NUM_WARPS) {
    // Loop 2
    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
        K_vec k_vecs[NUM_VECS_PER_THREAD];
        // Loop 3
        for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
            const cache_t* k_ptr =
                k_cache + physical_block_number * kv_block_stride +
                kv_head_idx * kv_head_stride + physical_block_offset * x;
            const int vec_idx = thread_group_offset + j * THREAD_GROUP_SIZE;
            const int offset1 = (vec_idx * VEC_SIZE) / x;
            const int offset2 = (vec_idx * VEC_SIZE) % x;
            // if Fp8KVCacheDataType::kAuto
            k_vecs[j] = *reinterpret_cast<const K_vec*>(
              k_ptr + offset1 * BLOCK_SIZE * x + offset2);
        }
        // ...
    }
    // ...
}
```

è€è§„çŸ©, å…ˆçœ‹ `j`, æœ¬è´¨å°±æ˜¯ä» 0 è¿­ä»£åˆ° `NUM_VECS_PER_THREAD`, æ¯æ¬¡è¿­ä»£å½“å‰ thread è¯»å–ä¸€ä¸ª VEC å­˜å…¥ `k_vecs` ä¸­.

> ğŸ”” å›é¡¾:  
> 1. `NUM_VECS_PER_THREAD` è¡¨ç¤ºä¸€ä¸ª head è¢«åˆ†æˆå¤šå°‘ä¸ª 16B.
> 2. `k_cache` çš„ shape ä¸º `(num_blocks, num_kv_heads, head_size/x, block_size, x)`.

å…¶ä¸­çš„ `x` è¡¨ç¤ºä¸€ä¸ª thread group éœ€è¦è¯»å–çš„å…ƒç´ æ•°é‡ (`VEC_SIZE` * `THREAD_GROUP_SIZE`); å› æ­¤ä½œè€…å°† K Cache çš„ layout çš„æœ€åä¸€ç»´è®¾ç½®ä¸º `x` å…¶å®ä¹Ÿæ˜¯æ–¹ä¾¿åç»­ thread group å¯¹ K cache çš„è¯»å–.

ä¸‹å›¾å…·ä½“å±•ç¤ºäº†å¯»å€çš„è¿‡ç¨‹:

![pa-cal-kq-03.png](/imgs/blogs/dive-into-paged-attention/pa-cal-kq-03.png)

å…¶ä¸­:

- åœ¨ MHSA ä¸­, `num_kv_heads` ç­‰äº `num_heads`; è€Œåœ¨ GQA, MQA ä¸­, `num_kv_heads` å°äº `num_heads`.
- (1) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread å±äºçš„ warp è¦å¤„ç†å“ªä¸ª PA block.
- (2) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread è¦è®¡ç®—çš„ head åœ¨ K cache ä¸­çš„ä½ç½®. è¿™ä¸ª head çš„ç´¢å¼•å’Œ Q ä¸­ head çš„ç´¢å¼•åœ¨ MHSA ä¸­ç›¸åŒ.
- (3) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread group è¦è®¡ç®—çš„ token åœ¨å½“å‰ PA block ä¸­çš„ä½ç½®.
- (5) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread åœ¨éœ€è¦è¯»å–çš„ head (è“è‰²é•¿æ–¹ä½“) ä¸­ x çš„åç§», é€šè¿‡ `j` è¿›è¡Œè¿­ä»£è¯»å–. **æ¯æ¬¡å¾ªç¯ thread group ä¸­çš„æ‰€æœ‰ thread å–ä¸€ä¸ª x.**
- (6) è´Ÿè´£æ‰¾åˆ°å½“å‰ thread åœ¨ thread gruop ä¸­è¯»å–çš„ x ä¸­ VEC çš„åç§»; thread ä¸€æ¬¡è¯»å–ä¸€ä¸ª VEC.

ğŸ¤” **ä¸ºä»€ä¹ˆ (5) åœ¨å®é™…å¯»å€æ—¶éœ€è¦ `* BLOCK_SIZE * x` ?**  
ç­”: è¿™æ˜¯æ ¹æ® `k_cache` çš„ layout å¾—åˆ°çš„ stride. åŒç† (3) `* x` ä¹Ÿæ˜¯ stride.

ç¬¬ 3 ä¸ªå¾ªç¯ç»“æŸæ—¶å½“å‰ warp è´Ÿè´£çš„æ¯ä¸ª token ä¸­éœ€è¦çš„ K cache head å·²ç»å…¨è¢«åŠ è½½å…¥ thread æœ¬åœ°çš„ `k_vecs` ä¸­äº†. 

ç”±äºä¸€ä¸ª thread group çš„ `k_vecs` æ‰èƒ½çœŸæ­£ç»„æˆä¸€ä¸ª head, åœ¨é€€å›ç¬¬äºŒä¸ªå¾ªç¯è¿›è¡Œ QK dot çš„æ—¶å€™, éœ€è¦åšä¸ª reduction, å…·ä½“çš„èŒƒå›´å°±æ˜¯ `THREAD_GROUP_SIZE` ä¸ª thread:

```cpp
// Loop 1
for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;
        block_idx += NUM_WARPS) {
    // Loop 2
    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
        K_vec k_vecs[NUM_VECS_PER_THREAD];
        // Loop 3
        for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
            // ...
        }
        float qk = scale * Qk_dot<scalar_t, THREAD_GROUP_SIZE>::dot(
                             q_vecs[thread_group_offset], k_vecs);
    }
    // ...
}
```

è®¡ç®—å®Œ `qk` å, ç”±å½“å‰ thread group ä¸­ç¬¬ä¸€ä¸ª (offset ä¸º 0) çš„ thread å¯¹è‡ªå·±åˆšæ‰ç®—å‡ºæ¥çš„ `qk` è¿›è¡Œ mask, é¡ºä¾¿çœ‹çœ‹å¦‚æœæ²¡æœ‰ mask æ‰, æŠŠ `qk_max` èµ‹å€¼ä¸º `qk`:

```cpp
if (thread_group_offset == 0) {
    // Store the partial reductions to shared memory.
    // NOTE(woosuk): It is required to zero out the masked logits.
    const bool mask = token_idx >= seq_len;
    logits[token_idx - start_token_idx] = mask ? 0.f : qk;
    // Update the max value.
    qk_max = mask ? qk_max : fmaxf(qk_max, qk);
}
```

ğŸ§ **ä¸ºä»€ä¹ˆè¦åš mask?**
- å› ä¸ºä¸€ä¸ª seq çš„æœ€åä¸€ä¸ª PA block å¯èƒ½è¦†ç›–ä¸æ»¡ `BLOCK_SIZE` ä¸ª token. è¿™é‡Œçš„ mask å°±æ˜¯æŠŠé‚£éƒ¨åˆ† qk ç½®é›¶.

### 5.4. Softmax

æˆ‘å‹’ä¸ª QK å•Š, æ€»ç®—ç®—å®Œäº†, é”å…‹ five éƒ½è¦è¢«æŠ½æ¸…ä»“äº†. é¡µæ„ä¸çœŸ, é‰´å®šä¸ºå¼€ç®— softmax.

ä¸»è¦æ­¥éª¤å°±æ˜¯å¹¿æ’­ç„¶åç®—, ç®— softmax éœ€è¦çŸ¥é“æ¯ä¸ª head å¯¹åº”çš„ qk çš„æœ€å¤§å€¼. ç”±äºä¸€ä¸ª cuda block è´Ÿè´£çš„å°±æ˜¯ä¸€ä¸ª head, å¯¹äºè¿™ä¸ª head ä¸Šé¢çš„è®¡ç®—æ­¥éª¤ä¸€å…±ç®—äº† `cache_len`ä¸ª token çš„ qk, å› æ­¤éœ€è¦åšä¸€ä¸ª cuda block èŒƒå›´çš„è§„çº¦, æ‰¾åˆ°å…¶ä¸­æœ€å¤§çš„ qk å€¼.

å…ˆåœ¨ warp å±‚é¢è§„çº¦.

```cpp
__shared__ float red_smem[2 * NUM_WARPS];

// ...

// Perform reduction across the threads in the same warp to get the
// max qk value for each "warp" (not across the thread block yet).
// The 0-th thread of each thread group already has its max qk value.
#pragma unroll
for (int mask = WARP_SIZE / 2; mask >= THREAD_GROUP_SIZE; mask /= 2) {
    qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask));
}
if (lane == 0) {
    red_smem[warp_idx] = qk_max;
}
__syncthreads();
```

- `red_smem` æ˜¯ä¹‹å‰ç”³è¯·çš„ shared memory. 
- `VLLM_SHFL_XOR_SYNC` æ˜¯ä¸€ä¸ª warp å†…çš„ shuffle æ“ä½œ, å…·ä½“æ¥è¯´, åœ¨æ¯æ¬¡å¾ªç¯æ—¶, æ¯ä¸ª thread å’Œè‡ªå·±ç›¸è· `mask` ä½ç½®çš„çº¿ç¨‹äº¤æ¢æ•°æ® (äº¤æ¢æ¥çš„æ•°æ®é€šè¿‡ `fmaxf` æ¯”è¾ƒ), å¹¶ä¸” `mask` ä¼šé€æ¸å‡åŠ, ç›´åˆ° `THREAD_GROUP_SIZE` ä¸ºæ­¢.
- `lane` è¡¨ç¤ºå½“å‰ warp ä¸­çš„çº¿ç¨‹ç´¢å¼•.

æ¥ç€å†å¯¹æ¯ä¸ª warp çš„æœ€å¤§å€¼è¿›è¡Œè§„çº¦, ç”±äºæ¯ä¸ª warp çš„æœ€å¤§å€¼éƒ½è¢«å­˜å…¥äº† `red_smem` ä¸­, æ‰€ä»¥åªéœ€è¦å†æ¬¡è¿›è¡Œ shuffle æ“ä½œå³å¯.

```cpp
// TODO(woosuk): Refactor this part.
// Get the max qk value for the sequence.
qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
#pragma unroll
for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
    qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask));
}
```

æ­¤æ—¶, ç¬¬ 1 ä¸ªçº¿ç¨‹çš„ `qk_max` å°±æ˜¯å½“å‰ cuda block ä¸­æ‰€æœ‰ warp ä¸­æœ€å¤§çš„ qk å€¼. å°†å…¶å¹¿æ’­ç»™æ‰€æœ‰çº¿ç¨‹:

```cpp
// Broadcast the max qk value to all threads.
qk_max = VLLM_SHFL_SYNC(qk_max, 0);
```

åœ¨è·å¾—äº† `qk_max` å, å°±å¯ä»¥è®¡ç®— softmax äº†:

```cpp
// Get the sum of the exp values.
float exp_sum = 0.f;
for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
    float val = __expf(logits[i] - qk_max);
    logits[i] = val;
    exp_sum += val;
}
exp_sum = block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum);

// Compute softmax.
const float inv_sum = __fdividef(1.f, exp_sum + 1e-6f);
for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
    logits[i] *= inv_sum;
}
__syncthreads();
```

### 5.5. LV (Logits * Value)

![pa-cal.png](/imgs/blogs/dive-into-paged-attention/pa-cal.png)

ä¸Šå›¾å±•ç¤ºäº† LV çš„è®¡ç®—è¿‡ç¨‹, ä¸»è¦åŒºåˆ«æ˜¯ç”±äºè¦è®¡ç®— Logits çš„ shape å¯ä»¥è¡¨ç¤ºä¸º `(num_heads, num_seqs, cache_len)`, è€Œ V çš„ shape å¯ä»¥è¡¨ç¤ºä¸º `(num_heads, cache_len, head_size)`, å› æ­¤ LV çš„çŸ©é˜µä¹˜æ³•ä¸­, æ¯è®¡ç®—ä¸€ä¸ªå…ƒç´ éœ€è¦è¯»å– logits çš„ä¸€è¡Œå’Œ V çš„ä¸€åˆ—è¿›è¡Œè®¡ç®—.

æ­¤æ—¶, ä¸€ä¸ª cuda block çš„èŒè´£ä» "è‡ª Q ä¸­è¯»å–ä¸€ä¸ª head" è½¬å˜ä¸º "è®¡ç®— output ä¸­çš„ä¸€ä¸ª head".

ğŸ§ **ä¸ºä»€ä¹ˆåœ¨è®¡ç®— LV æ—¶, å»æ‰äº† thread group çš„æ¦‚å¿µ, æ¯ä¸ª thread éƒ½è¢«è®¾å®šä¸ºæ¯æ¬¡è¯»å– 16B?**  
- å› ä¸ºç°åœ¨æ¯è®¡ç®—ä¸€ä¸ªå…ƒç´ , éœ€è¦çš„è®¿å­˜é‡æ›´å¤§, å› æ­¤ç»™æ¯ä¸ª thread åˆ†é…äº†æ›´å¤šçš„æ•°æ®è¯»å–é‡. ä¹Ÿå°±æ˜¯è¯´, `V_VEC_SIZE` æ¯” `VEC_SIZE` æ›´å¤§.

ç”±äº cuda è®¿å­˜æ¨¡å¼æŒ‰è¡Œè¯»å–æ›´å¿«, æ‰€ä»¥å®é™…çš„è®¡ç®—ç»“æœåœ¨éå† PA block æ—¶çº¿ç¨‹å†…éƒ¨åˆ©ç”¨ `accs` è¿›è¡Œç´¯è®¡ (ä»¥å®ç°ä¸ V çš„ä¸€åˆ—è¿›è¡Œè®¡ç®—çš„è¡Œä¸º):

```cpp
constexpr int NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE;
constexpr int NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW;
constexpr int NUM_ROWS_PER_THREAD =
    DIVIDE_ROUND_UP(HEAD_SIZE, NUM_ROWS_PER_ITER);

// NOTE(woosuk): We use FP32 for the accumulator for better accuracy.
float accs[NUM_ROWS_PER_THREAD];

for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;
        block_idx += NUM_WARPS) {
    V_vec v_vec;
    // ...
    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
        // ...
        for (int j = 0; j < V_VEC_SIZE; j++) {
            // Load V to `v_vec` ...
            v_vec_ptr[j] = token_idx + j < seq_len ? v_vec_ptr[j] : zero_value;
        }
        // Accumulate the dot product.
        accs[i] += dot(logits_vec, v_vec);
    }
}
```

ç”±äºæ¯ä¸ªçº¿ç¨‹è´Ÿè´£çš„ç´¯è®¡éƒ¨åˆ†ä¸æ»¡ä¸€æ•´è¡Œ/åˆ—, æ‰€ä»¥è¿›è¡Œè§„çº¦:

```cpp
// Perform reduction within each warp.
#pragma unroll
  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
    float acc = accs[i];
#pragma unroll
    for (int mask = NUM_V_VECS_PER_ROW / 2; mask >= 1; mask /= 2) {
      acc += VLLM_SHFL_XOR_SYNC(acc, mask);
    }
    accs[i] = acc;
  }

  // NOTE(woosuk): A barrier is required because the shared memory space for
  // logits is reused for the output.
  __syncthreads();

  // Perform reduction across warps.
  float* out_smem = reinterpret_cast<float*>(shared_mem);
#pragma unroll
  for (int i = NUM_WARPS; i > 1; i /= 2) {
    int mid = i / 2;
    // Upper warps write to shared memory.
    if (warp_idx >= mid && warp_idx < i) {
      float* dst = &out_smem[(warp_idx - mid) * HEAD_SIZE];
#pragma unroll
      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
          dst[row_idx] = accs[i];
        }
      }
    }
    __syncthreads();

    // Lower warps update the output.
    if (warp_idx < mid) {
      const float* src = &out_smem[warp_idx * HEAD_SIZE];
#pragma unroll
      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
          accs[i] += src[row_idx];
        }
      }
    }
    __syncthreads();
  }
```

æœ€åå†™å…¥åˆ°è¾“å‡ºä¸­:

```cpp
  // Write the final output.
  if (warp_idx == 0) {
    scalar_t* out_ptr =
        out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE +
        head_idx * max_num_partitions * HEAD_SIZE + partition_idx * HEAD_SIZE;
#pragma unroll
    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
      if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
        from_float(*(out_ptr + row_idx), accs[i]);
      }
    }
  }
```