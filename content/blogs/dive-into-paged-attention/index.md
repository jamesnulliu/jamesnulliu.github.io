---
title: "Dive into Paged Attention"
date: 2024-10-07T12:00:00+08:00
lastmod: 2024-10-07T23:00:00+08:00
draft: false
author: ["jamesnulliu"]
keywords: 
    - vllm
    - continuous-batching
    - paged-attention
categories:
    - deeplearning
tags:
    - python
    - vllm
    - attention
description: Dive into the paged attention mechanism of VLLM.
summary: Dive into the paged attention mechanism of VLLM.
comments: true
images: 
cover:
    image: ""
    caption: ""
    alt: ""
    relative: true
    hidden: true
---

## 1. è¯æ˜ Attention çš„ \(O_i\) åªä¸ \(Q_i\) æœ‰å…³

Attention çš„å…¬å¼å¦‚ä¸‹:

\[
O=Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V
\]

å‡è®¾ \(Q=\begin{bmatrix}Q_0\\Q_1\end{bmatrix}\), \(K=\begin{bmatrix}K_0\\K_1\end{bmatrix}\)

é‚£ä¹ˆ:

\[
O=softmax(\frac{\begin{bmatrix}Q_0K_0^T&Q_0K_1^T\\Q_1K_0^T&Q_1K_1^T\end{bmatrix}}{\sqrt{d_k}})V
\]

ä»¤:

\[
A=\begin{bmatrix}A_0\\A_1\end{bmatrix}=\begin{bmatrix}Q_0K_0^T&Q_0K_1^T\\Q_1K_0^T&Q_1K_1^T\end{bmatrix},f(x)=\frac{softmax(x)}{\sqrt{d_k}}
\]

æ­¤æ—¶, \(A_1\) åªå’Œ \(Q_1\) æœ‰å…³, å’Œ \(Q_0\) æ— å…³, é‚£ä¹ˆ:

\[
\begin{bmatrix}O_0\\O_1\end{bmatrix}=O=\begin{bmatrix}f(A_0)\\f(A_1)\end{bmatrix}V=\begin{bmatrix}f(A_0)V\\f(A_1)V\end{bmatrix}
\]

å› æ­¤, \(O_i\) åªå’Œ  \(A_i\) ç›¸å…³, è€Œæ ¹æ® \(A\) çš„è®¾å®š, \(A_i\) åªå’Œ \(Q_i\) ç›¸å…³, å³:

Attention çŸ©é˜µçš„ç¬¬ \(i\) ä¸ªè¾“å‡ºåªå’Œç¬¬ \(i\) ä¸ª \(Q\) æœ‰å…³, å’Œä¹‹å‰çš„ \(Q\) æ— å…³.

**æ€»ç»“**:

- åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶ï¼Œåªéœ€å¯¹æ–° token è®¡ç®—å¯¹åº”çš„ `Q_new`ï¼Œå¹¶ä¸ä¹‹å‰å·²ç»ç¼“å­˜çš„ `K_cache` å’Œ `V_cache` è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚
- æ–°çš„ `K_new` å’Œ `V_new` ä¼šè¢«åŠ å…¥åˆ°ç¼“å­˜ä¸­ï¼Œç»§ç»­ä¸ºä¸‹ä¸€ä¸ª token ç”Ÿæˆæä¾›åŸºç¡€ã€‚
- æ•´ä¸ªè¿‡ç¨‹é¿å…äº†å¯¹æ‰€æœ‰å†å² token çš„é‡å¤è®¡ç®—ï¼Œå¤§å¹…æé«˜äº†æ•ˆç‡ã€‚

## 2. KV Cache çš„å¢é‡è¿‡ç¨‹
### 2.1. åˆå§‹è¾“å…¥ï¼ˆå®Œæ•´åºåˆ—ï¼‰è®¡ç®—ï¼š

- å¯¹äºåˆå§‹çš„è¾“å…¥åºåˆ— `(seq_len, embed_dim)`ï¼Œæˆ‘ä»¬é€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ° `Q`ã€`K` å’Œ `V`ï¼Œå®ƒä»¬çš„å½¢çŠ¶éƒ½æ˜¯ `(seq_len, embed_dim)`ã€‚
- ä½¿ç”¨ `Q` å’Œ `K` è¿›è¡Œç‚¹ç§¯è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œç„¶åç»“åˆ `V` è®¡ç®—å¾—åˆ°è¾“å‡º `(seq_len, embed_dim)`ï¼Œè¿™æ˜¯ç¬¬ä¸€æ¬¡å¯¹åˆå§‹åºåˆ—çš„å®Œæ•´è®¡ç®—ã€‚

### 2.2. é¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶çš„å¢é‡è®¡ç®—ï¼š

åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ª token æ—¶ï¼Œä¸éœ€è¦å¯¹æ•´ä¸ªåºåˆ—å†è¿›è¡Œå®Œæ•´çš„ `Q`ã€`K`ã€`V` è®¡ç®—ï¼Œè€Œæ˜¯åªéœ€å¯¹æ–°ç”Ÿæˆçš„ token è¿›è¡Œä¸€æ¬¡å¢é‡è®¡ç®—ã€‚è¿™æ—¶çš„æ“ä½œæµç¨‹å¦‚ä¸‹ï¼š

1. **è¾“å…¥æ–°çš„ token**ï¼šå°†å·²ç»ç”Ÿæˆçš„ tokenï¼ˆå…¶å½¢çŠ¶ä¸º `(embed_dim,)`ï¼‰ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ°è¯¥ token å¯¹åº”çš„ `Q_new`ï¼Œå½¢çŠ¶ä¸º `(embed_dim,)`ã€‚
2. **ä¸ä¹‹å‰ç¼“å­˜çš„ `K` å’Œ `V` è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—**ï¼š
    - ä½¿ç”¨ `Q_new` ä¸ä¹‹å‰å·²ç»è®¡ç®—å¹¶ç¼“å­˜çš„ `K` å’Œ `V` è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚
    - è¿™é‡Œçš„ `K_cache` å’Œ `V_cache` åˆ†åˆ«æ˜¯ä¹‹å‰æ¯æ¬¡ç”Ÿæˆ token æ—¶å¾—åˆ°çš„ `K` å’Œ `V`ï¼Œå®ƒä»¬çš„å½¢çŠ¶æ˜¯ `(seq_len, embed_dim)`ï¼Œå³ç¼“å­˜äº†ä»æœ€åˆè¾“å…¥åºåˆ—åˆ°å½“å‰å·²ç»ç”Ÿæˆçš„æ‰€æœ‰ token çš„ `K` å’Œ `V`ã€‚
    - `Q_new` å¯ä»¥ç›´æ¥ä¸ `K_cache` è¿›è¡Œç‚¹ç§¯ï¼Œå¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°ï¼Œç„¶åç»“åˆ `V_cache` å¾—åˆ°æ–°çš„è¾“å‡ºã€‚
3. **æ›´æ–° `KV Cache`**ï¼š
    - æ–°çš„ `K_new` å’Œ `V_new` ä¼šé€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ°ï¼ˆå½¢çŠ¶ä¸º `(embed_dim,)`ï¼‰ï¼Œå¹¶å°†å®ƒä»¬æ·»åŠ åˆ° `K_cache` å’Œ `V_cache` çš„æœ«å°¾ï¼Œä½¿å¾—ç¼“å­˜çš„ `K_cache` å’Œ `V_cache` ä¸æ–­å¢å¤§ï¼Œä»¥å¤‡åç»­ä½¿ç”¨ã€‚
4. **è¾“å‡º**ï¼šé€šè¿‡æ³¨æ„åŠ›è®¡ç®—åçš„è¾“å‡ºå½¢çŠ¶ä¸º `(embed_dim,)`ï¼Œå³æ–°ç”Ÿæˆçš„ tokenã€‚

![vllm-prefill-and-decode](/imgs/blogs/dive-into-paged-attention/vllm-prefill-and-decode.png)

### 2.3. Python å®ç°

```python
class MultiHeadAttention(nn.Module):
    def __init__(
        self,
        embed_dim,
        num_heads,
        k_dim=None,
        k_cache: torch.Tensor = None,  # (cache_len, num_heads, head_size)
        v_cache: torch.Tensor = None,  # (cache_len, num_heads, head_size)
    ):
        super(MultiHeadAttention, self).__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_size = embed_dim // num_heads
        self.k_dim = embed_dim if k_dim is None else k_dim
        assert (
            self.head_size * num_heads == embed_dim
        ), "embed_dim should be divisible by n_heads"
        self.wq = nn.Parameter(torch.randn(self.embed_dim, self.k_dim))
        self.wk = nn.Parameter(torch.randn(self.embed_dim, self.k_dim))
        self.wv = nn.Parameter(torch.randn(self.embed_dim, self.k_dim))
        self.k_cache = k_cache
        self.v_cache = v_cache
        self.wout = nn.Parameter(torch.randn(self.embed_dim, self.k_dim))

    def forward(self, x):
        # x: (seq_len, embed_dim)
        seq_len, embed_dim = x.size()
        q = torch.matmul(x, self.wq)  # (seq_len, embed_dim)
        k = torch.matmul(x, self.wk)  # (seq_len, embed_dim)
        v = torch.matmul(x, self.wv)  # (seq_len, embed_dim)

        # q -> (n_heads, seq_len, head_size)
        q = q.view(seq_len, self.num_heads, self.head_size).transpose(0, 1)
        # k, v -> (seq_len, n_heads, head_size)
        k = k.view(seq_len, self.num_heads, self.head_size)
        v = v.view(seq_len, self.num_heads, self.head_size)
        if self.k_cache is not None and self.v_cache is not None:
            k = torch.cat([self.k_cache, k], dim=0)
            v = torch.cat([self.v_cache, v], dim=0)
            # New cache_len = seq_len + cache_len
        # k -> (n_heads, head_size, cache_len)
        k = k.transpose(0, 1)
        # v -> (n_heads, cache_len, head_size)
        v = v.transpose(0, 1)

        # scores: (n_heads, seq_len, cache_len)
        scores = (torch.matmul(q, k.transpose(-2, -1))) / (self.head_size**0.5)
        # attn_weights: (n_heads, seq_len, cache_len)
        attn_weights = torch.softmax(scores, dim=-1)
        # attn_output: (n_heads, seq_len, head_size)
        attn_output = torch.matmul(attn_weights, v)
        # attn_output: (seq_len, n_heads, head_size)
        attn_output = (
            attn_output.transpose(0, 1)  # (seq_len, n_heads, head_size)
            .contiguous()  # Make sure the memory is contiguous
            .view(seq_len, embed_dim)  # (seq_len, embed_dim)
        )
        # out: (seq_len, embed_dim)
        out = torch.matmul(attn_output, self.wout)
        return out

```

## 3. vllm çš„åœ¨çº¿æ¨ç† & ç¦»çº¿æ¨ç†

ç¦»çº¿æ¨ç†: `vllm.LLM`
åœ¨çº¿æ¨ç†: `vllm.AsyncLLMEngine`

### 3.1. ç¦»çº¿æ¨ç†

ç¤ºä¾‹:

```python
# "examples/offline_inference.py"
from vllm import LLM, SamplingParams

# Sample prompts.
prompts = [
    "Hello, my name is",
    "The president of the United States is",
    "The capital of France is",
     "The future of AI is",
]

# Create a sampling params object.
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)
# Create an LLM.
llm = LLM(model="facebook/opt-125m")
# Generate texts from the prompts. The output is a list of RequestOutput objects
# that contain the prompt, generated text, and other information.
outputs = llm.generate(prompts, sampling_params)
# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

`LLM.generate` è°ƒç”¨ `LLM._run_engine` ç”Ÿæˆ outputs.

`_run_engine` å†…éƒ¨:

```python
# ...
while self.llm_engine.has_unfinished_requests():
    step_outputs = self.llm_engine.step()
        for output in step_outputs:
            if output.finished:
                outputs.append(output)
            # ...
```


## 4. Continuous Batching

> å‚è€ƒ: https://blog.csdn.net/qq_27590277/article/details/135710435

### 4.1. BatchMaker: Low Latency RNN Inference with Cellular Batching

è®ºæ–‡: [Low Latency RNN Inference with Cellular Batching](https://madsys.cs.tsinghua.edu.cn/publication/low-latency-rnn-inference-with-cellular-batching/EUROSYS2018-gao.pdf)

BatchMaker æ˜¯ä¸€ä¸ªä¸º RNNs è®¾è®¡çš„ serving ç³»ç»Ÿï¼Œå®ƒä»¥ RNN Cell ä¸ºç²’åº¦è¿›è¡Œè°ƒåº¦å’Œ Batchingã€‚RNN ä½¿ç”¨ç›¸åŒæƒé‡å¯¹ä¸åŒè¾“å…¥è¿›è¡Œè®¡ç®—ã€‚å½“æ”¶åˆ°è¯·æ±‚æ—¶ï¼ŒBatchMaker å°†ç”¨äºå¤„ç†è¯·æ±‚çš„æ•°æ®æµå›¾åˆ†è§£ä¸º RNN Cellï¼ˆå³ä¸€ä¸ªiteration stepï¼‰ï¼Œå¹¶ä»¥ Cell çš„ç²’åº¦è¿›è¡Œæ‰§è¡Œè°ƒåº¦ï¼Œå¹¶æ‰¹å¤„ç†ç›¸åŒçš„å•å…ƒæ‰§è¡Œã€‚ç”±äºæ¯ä¸ª RNN Cell å§‹ç»ˆæ‰§è¡Œå®Œå…¨ç›¸åŒçš„è®¡ç®—ï¼ŒBatchMaker å¯ä»¥æ— è®ºå•å…ƒçš„ä½ç½®ï¼ˆå³æ ‡è®°ç´¢å¼•ï¼‰å¦‚ä½•ï¼Œéƒ½ä»¥ Batching æ–¹å¼æ‰§è¡Œå¤šä¸ª RNN Cellã€‚é€šè¿‡è¿™æ ·åšï¼ŒBatchMaker å…è®¸æ–°åˆ°è¾¾çš„ RNN è¯·æ±‚åŠ å…¥ï¼ˆæˆ–å·²å®Œæˆçš„è¯·æ±‚ç¦»å¼€ï¼‰å½“å‰æ‰§è¡Œçš„æ‰¹æ¬¡ï¼Œè€Œæ— éœ€ç­‰å¾…æ‰¹æ¬¡å®Œå…¨å®Œæˆã€‚

![celluar-batching](/imgs/blogs/dive-into-paged-attention/CellularBatching.png)

### 4.2. ORCAï¼šæ›´é€‚åˆ Transformer çš„ Batching æ–¹æ³•

è®ºæ–‡: [Orca: A Distributed Serving System for Transformer-Based Generative Models](https://www.usenix.org/system/files/osdi22-yu.pdf)

ORCA å€Ÿé‰´ BatchMaker æ–¹æ³•ï¼Œå°†å®ƒé€‚é…åˆ° Transformer Decoder ç”Ÿæˆè¿‡ç¨‹ã€‚è™½ç„¶ Transformer Decoder å’Œ RNN åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­éƒ½æ˜¯é€ä¸ª token åœ°è¿­ä»£ç”Ÿæˆï¼Œä½†å®ƒä»¬ä¹‹é—´å­˜åœ¨ä¸€äº›æœ¬è´¨åŒºåˆ«ã€‚

1. é¦–å…ˆï¼ŒTransformer Decoding é˜¶æ®µæ¯ä¸ªè¿­ä»£æ—¶ï¼Œå°†å½“å‰ token å’Œä¹‹å‰ç”Ÿæˆçš„ token åºåˆ—æ‹¼æ¥èµ·æ¥ä¼ å…¥æ¨¡å‹ã€‚å°½ç®¡æ¯æ¬¡åªç”Ÿæˆä¸€ä¸ª tokenï¼Œè®¡ç®—é‡è¿‘ä¼¼ï¼Œä½†æ¯ä¸ªè¿­ä»£çš„ KVCache çš„é•¿åº¦ä¼šé€æ¸å¢åŠ ã€‚
2. å…¶æ¬¡ï¼ŒDecoder åœ¨è¿›è¡Œè§£ç æ—¶éœ€è¦è¿›è¡Œ Prefill è¿‡ç¨‹ï¼Œè¿™æ˜¯ RNN æ²¡æœ‰çš„ã€‚Prefill è®¡ç®—æ˜¯ä¸€å † token ä¸€èµ·ç®—ï¼Œå’Œ Decoding é˜¶æ®µè®¡ç®—æ¨¡å¼æˆªç„¶ä¸åŒã€‚å‰è€…æ˜¯è®¡ç®—å¯†é›†ï¼Œåè€…æ˜¯è®¿å­˜å¯†é›†ã€‚

ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼ŒOCRA æå‡ºäº†ä¸¤ä¸ªè®¾è®¡æ€è·¯ï¼šIteration-level Batching å’Œ Selective Batchingã€‚ Iteration-level Batching å¯ä»¥çœ‹ä½œæ˜¯å¯¹ BatchMaker Cell ç²’åº¦å¤„ç†æ€æƒ³çš„ä¸€ç§è‡´æ•¬ï¼Œè€Œ Selective Batching åˆ™æ˜¯é’ˆå¯¹ Transformer çš„ç‹¬ç‰¹å¤„ç†ï¼Œä»¥æ”¯æŒåœ¨ batch size å’Œ input sequence è¿™ä¸¤ä¸ªç»´åº¦åŠ¨æ€å˜åŒ–å¯¹ Batching æ‰§è¡Œçš„å½±å“ã€‚

ç”±äº Attention æœºåˆ¶å’Œ FNN çš„ Batching æ–¹å¼ä¸åŒã€‚Linear å±‚å¯ä»¥å°† batch size å’Œ seq_len è¿™ä¸¤ä¸ªç»´åº¦èåˆä¸ºä¸€ä¸ªç»´åº¦ï¼Œç±»ä¼¼äº Efficient Transformer çš„æ€æƒ³ï¼Œè€Œ Attention åˆ™ä¸è¡Œã€‚å› æ­¤ï¼Œä¸€ä¸ª Transformer Layer å¯ä»¥åˆ’åˆ†ä¸º PreAttnã€Attn å’Œ PostAttn ä¸‰ä¸ªéƒ¨åˆ†ã€‚ä»è€Œæ”¯æŒ prefill é˜¶æ®µå’Œ decoding ä¸€ä¸ª step æ‰“æˆä¸€ä¸ª batch å¤„ç†ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒQKV Linear å’Œ Attn Out Linear æ‰“æˆä¸€ä¸ªbatch size=7ã€‚Attn çš„è®¡ç®—æ²¡æœ‰æ‰“ Batchï¼Œæ¯ä¸ª request å•ç‹¬å¤„ç†ã€‚æ‰€ä»¥åœ¨ Attn å‰åæœ‰ Split å’Œ Merge æ“ä½œã€‚

![orca-transformer-execution](/imgs/blogs/dive-into-paged-attention/ORCA-transformer-execution.png)

**OCRA è¿˜æ²¡è€ƒè™‘ KV Cache å†…å­˜ç®¡ç†ä¼˜åŒ–ï¼Œå®ƒæ¯ä¸ªåºåˆ—é¢„å…ˆåˆ†é… max token æ•°çš„ä½œä¸º KV Cache æ˜¾å­˜ç©ºé—´ã€‚OCRA çš„å®éªŒéƒ½æ˜¯æŒ‰ç…§ max token æ¥ç”Ÿæˆï¼Œä¸ä¼šè€ƒè™‘é‡åˆ° eos çš„æƒ…å†µã€‚**

### 4.3. 2023å¹´æ›´å¤š Continuous Batching çš„å˜ç§

2023å¹´ Continuous Batching è¿æ¥äº†å¤§å‘å±•ï¼Œåœ¨ vLLM æ¨åŠ¨ä¸‹å·²æˆä¸ºæ¨ç†æ¡†æ¶äº‹å®æ ‡å‡†ã€‚ä¸åŒæ¡†æ¶å®ç°æœ‰å·®åˆ«ï¼Œä¸»è¦ä½“ç°åœ¨å¯¹ prefill å¤„ç†çš„æ–¹å¼ä¸Šã€‚å°† prefill å•ç‹¬å¤„ç†è¿˜æ˜¯å’Œ decoding èåˆï¼Œä»¥ä»€ä¹ˆæ ·çš„ç²’åº¦èåˆï¼Œæœ‰ä¸€äº›è®²ç©¶ã€‚

#### 4.3.1. vLLM (UC Berkeley)

SOSP 2023 çš„è®ºæ–‡ vLLMï¼Œä¹Ÿæ˜¯çƒ­é—¨å¼€æºé¡¹ç›®ï¼Œå…¶åˆ›æ–°ç‚¹paged attnï¼ˆPAï¼‰ï¼Œå‡å°‘å†…å­˜ç¢ç‰‡ï¼Œå¢åŠ memory efficiencyï¼Œå¢å¤§batch sizeä»è€Œå¢åŠ ååã€‚Batchingç­–ç•¥æ˜¯ä¸ºPAè®¾è®¡æœåŠ¡çš„ï¼Œæ‰€ä»¥æ²¡æœ‰ç…§æ¬OCRAçš„å®ç°ã€‚

å’ŒORCAä¸åŒä¹‹å¤„åœ¨äºï¼ŒvLLM Batchingæ—¶å€™prefillå’Œdecodingæ˜¯åˆ†å¼€çš„ï¼Œä¸€ä¸ªBatching stepè¦ä¹ˆå¤„ç†decodingè¦ä¹ˆå¤„ç†prefillã€‚è¿™æ ·å®ç°æ¯”OCRAæ›´ç®€å•äº†ï¼Œprefillç›´æ¥è°ƒç”¨xformerså¤„ç†è®¡ç®—å¯†é›†çš„prefill attnè®¡ç®—ï¼›decodingæ‰‹å†™CUDA PAå¤„ç†è®¿å­˜å¯†é›†çš„attnè®¡ç®—ã€‚

vLLM ä¹‹æ‰€ä»¥æ²¡æœ‰é‡‡ç”¨ OCRA è®¾è®¡ï¼Œæ˜¯å› ä¸º vLLM çš„ PA æ˜¯æ‰‹å†™ CUDA Kernel å®ç°çš„ï¼Œå¯ä»¥å¤„ç† sequence é•¿åº¦ä¸åŒçš„è¾“å…¥ï¼ŒAttn çš„ Batching æ–¹å¼å¯ä»¥å’Œ Non-Attn éƒ¨åˆ†ç»Ÿä¸€ã€‚å› æ­¤ï¼Œä¸€ä¸ªç³™å¿«çŒ›æ–¹æ³•æ˜¯ä¸é‡‡ç”¨ Selective Batching çš„è®¾è®¡äº†ï¼Œæ‰€ Decoding æ•´ä½“ä¸€èµ·å¤„ç†ä¸€ä¸ª Batch çš„ step è®¡ç®—ï¼Œprefill ä¸å’Œ decoding step èåˆã€‚å¦‚æœæŠŠ prefill è®¡ç®—å’Œä¸€ä¸ª decoding step èåˆï¼Œåˆ™è¿˜éœ€è¦æ‹†åˆ† Attn å’Œ Non-Attn äº†ï¼ŒAttn å®ç°ä¹Ÿæ›´æ›´å¤æ‚äº†ï¼Œä¸åˆ©äºå±•ç¤º PA çš„æ€æƒ³ã€‚

ä¸è¿‡å› ä¸ºPrefillè¿‡ç¨‹ä¼šæŠ¢å decodingçš„stepå‰è¿›ï¼Œå¦‚æœè¾“å…¥prompt sequence lengthè¿‡é•¿ï¼Œæ‰€æœ‰decodingè¿‡ç¨‹éƒ½éœ€è¦ç­‰å¾…ï¼Œé€ æˆå¤§å®¶æ›´é•¿çš„å»¶è¿Ÿï¼Œå› æ­¤ç•™ä¸‹äº†ä¸€äº›ä¼˜åŒ–ç©ºé—´ï¼Œè¿™åæ¥è¿™ä¹Ÿé€ æˆäº†å’ŒDeepSpeedçš„ä¸€æ®µå­½ç¼˜ã€‚

#### 4.3.2. FastGen (DeepSpeed)

å¾®è½¯ DeepSpeed å›¢é˜Ÿ2023å¹´11æœˆåœ¨ MII é¡¹ç›®ä¸­æå‡ºäº†ä¸€ç§ Continous Batching å˜ç§ SplitFuseï¼Œåœ¨å‘å¸ƒæ—¶æŠŠ vLLM å½“é¶å­æ‰“ï¼ŒvLLM éšåè¿˜å‡»ï¼Œé€æ¸æ¼”åŒ–æˆæˆä¸ºä¸¤ä¸ªå¤§é—¨æ´¾çš„å£æ°´æˆ˜ã€‚

SplitFuse çš„æƒ³æ³•æ˜¯ï¼Œå¯¹é•¿ prompt request è¢«åˆ†è§£æˆæ›´å°çš„å—ï¼Œå¹¶åœ¨å¤šä¸ª forward step ä¸­è¿›è¡Œè°ƒåº¦ï¼Œåªæœ‰æœ€åä¸€å—çš„ forwar då®Œæˆåæ‰å¼€å§‹è¿™ä¸ª prompt request çš„ç”Ÿæˆã€‚å¯¹çŸ­ prompt request å°†è¢«ç»„åˆä»¥ç²¾ç¡®å¡«å…… step çš„ç©ºéš™ã€‚æ¯ä¸ª step çš„è®¡ç®—é‡åŸºæœ¬ç›¸ç­‰ï¼Œè¾¾åˆ°æ‰€æœ‰è¯·æ±‚å¹³å‡å»¶è¿Ÿæ›´ç¨³å®šçš„ç›®çš„

## 5. Paged Attention

> References:
>   - [vLLM Paged Attention](https://docs.vllm.ai/en/latest/dev/kernel/paged_attention.html)
>   - [vLLMçš‡å† ä¸Šçš„æ˜ç ï¼šæ·±å…¥æµ…å‡ºç†è§£PagedAttention CUDAå®ç°](https://zhuanlan.zhihu.com/p/673284781)

### 5.1. Paged Attention ä¸­ KV Cache çš„å˜åŒ–å½¢å¼

![paged-attention-animation](/imgs/blogs/dive-into-paged-attention/paged-attention-animation.webp)

1. **Page çš„æ¦‚å¿µ**ï¼š  
    - åœ¨ Paged Attention ä¸­ï¼Œé•¿åºåˆ—è¢«åˆ’åˆ†ä¸ºè‹¥å¹²å°å—ï¼ˆå³ "pages"ï¼‰ï¼Œæ¯ä¸ª page åŒ…å«å¤šä¸ª tokensã€‚æ¯æ¬¡ Attention æ“ä½œåªåœ¨å½“å‰ page å†…çš„ tokens ä¹‹é—´è®¡ç®—ï¼Œä¹Ÿå°±æ˜¯åœ¨æœ‰é™çš„ä¸Šä¸‹æ–‡çª—å£å†…è®¡ç®— Attentionï¼Œè€Œä¸æ˜¯åœ¨æ•´ä¸ªåºåˆ—ä¸Šè®¡ç®—ã€‚
    - å› æ­¤ï¼ŒKey å’Œ Value ç¼“å­˜ä¼šæŒ‰ page è¿›è¡Œç®¡ç†ï¼Œæ¯ä¸ª page å¯¹åº”ä¸€ç»„ K å’Œ V å€¼ã€‚
2. **ç”Ÿæˆæ–° token æ—¶çš„ KV Cache å˜åŒ–**ï¼š  
    - å½“ç”Ÿæˆæ–° token æ—¶ï¼Œæ–°çš„ Key å’Œ Value ä¼šæ ¹æ®è¿™ä¸ªæ–°çš„ token è®¡ç®—å‡ºæ¥ï¼Œå¹¶ä¸”ä¼šè¢«æ·»åŠ åˆ°å½“å‰ page çš„ **KCache** å’Œ **VCache** ä¸­ã€‚
    - **KCache å’Œ VCache** çš„å°ºå¯¸åªåœ¨å½“å‰ page ä¸­å˜åŒ–ã€‚åœ¨ç”Ÿæˆåˆ°æ–° page æ—¶ï¼Œæ‰ä¼šä¸ºæ–° page åˆ†é…æ–°çš„ç¼“å­˜ç©ºé—´ã€‚
3. **KV Cache çš„å˜åŒ–**ï¼š  
    - KV Cache å¯ä»¥ç†è§£ä¸º `(num_blocks, num_heads, head_size, block_size)`ï¼Œå…¶ä¸­:
        - `num_blocks` æ˜¯ page çš„æ•°é‡ (æˆ–è€…ç§°ä¸º block æ•°é‡ï¼Œæ¯ä¸ª block å¯¹åº”ä¸€ä¸ª page).
        - `num_heads` æ˜¯ Attention å¤´çš„æ•°é‡.
        - `head_size` æ˜¯æ¯ä¸ª Attention å¤´çš„ç»´åº¦å¤§å°.
    - éšç€æ–° token çš„ç”Ÿæˆï¼Œæ¯ä¸ª page ä¼šå¢åŠ æ–°çš„ Key å’Œ Valueï¼Œä½†å®ƒä»¬è¢«é™åˆ¶åœ¨å½“å‰ page å†…éƒ¨ã€‚
    - å½“å½“å‰ page å¡«æ»¡æ—¶ï¼Œç³»ç»Ÿä¼šå¼€å¯ä¸€ä¸ªæ–°çš„ pageï¼Œæ–°çš„ KV å€¼å°†è¢«å­˜å…¥æ–° page çš„ç¼“å­˜ä¸­ï¼Œå› æ­¤ä¼šåœ¨ `num_blocks` ç»´åº¦ä¸Šå¢åŠ ä¸€ä¸ªæ–°çš„ blockã€‚

### 5.2. Paged Attention Kernel è®¡ç®—æµç¨‹

é¦–å…ˆï¼ŒæŒ‰ç…§ CUDA ç¼–ç¨‹æ¨¡å‹å¯¹ä»»åŠ¡è¿›è¡Œå¹¶è¡Œåˆ’åˆ†: 

- Grid å¤§å° `(num_heads, num_seqs)` .
- Grid ä¸­æ¯ä¸ª CUDA thread block å¤§å° `(NUM_THREADS)`ï¼Œ`NUM_THREADS` æ˜¯å¸¸é‡é»˜è®¤ä¸º128ï¼Œä¹Ÿå°±è¯´æ¯ä¸ªthread blockåŒ…å«128ä¸ªçº¿ç¨‹ï¼Œè´Ÿè´£å®ŒæˆoutputçŸ©é˜µä¸€è¡Œï¼ˆåŒ…å«head_sizeä¸ªå…ƒç´ ï¼‰ç»“æœçš„ attention è®¡ç®—ä»»åŠ¡.
- Thread block ä¸­çš„çº¿ç¨‹è¿›ä¸€æ­¥åˆ’åˆ†è‹¥å¹²ä¸ª WARPã€‚ä¼—æ‰€å‘¨çŸ¥ï¼ŒWARP æ˜¯ GPU ä¸€ä¸ªåŸºæœ¬çš„æ‰§è¡Œå•å…ƒï¼Œç”±32ä¸ªçº¿ç¨‹ç»„æˆï¼Œè¿™äº›çº¿ç¨‹ä»¥ SMIT æ–¹å¼åœ¨ç¡¬ä»¶ä¸ŠåŒæ—¶æ‰§è¡Œç›¸åŒçš„æŒ‡ä»¤ï¼Œåœ¨ä¸åŒçš„æ•°æ®ä¸Šè¿›è¡Œæ“ä½œ.
- åœ¨ PA ä¸­æ¯”è¾ƒç‰¹æ®Šçš„æ˜¯ï¼ŒWARP å†… 32 ä¸ªçº¿ç¨‹è¿›ä¸€æ­¥åˆ’åˆ†ä¸º `blk_size` ä¸ª thread group ï¼Œè¿™å’Œ paged KVCache è®¾è®¡ x æ¯æ¯ç›¸å…³çš„ï¼Œé©¬ä¸Šä¼šç»†è®².

![paged-attention-compute-flow](/imgs/blogs/dive-into-paged-attention/paged-attention-compute-flow.webp)

åœ¨ä¸Šå›¾çš„å·¦ä¾§éƒ¨åˆ†ï¼Œæˆ‘ä»¬çœ‹åˆ°äº† Q çŸ©é˜µï¼Œè¿™éƒ¨åˆ†æè¿°äº†ä»æ˜¾å­˜è¯»å– Q æ•°æ®åˆ°å…±äº«å†…å­˜çš„è¿‡ç¨‹ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­: 

- ä¸€ä¸ª CUDA thread block ä¼šè¯»å–å›¾ä¸­ Q çŸ©é˜µçš„ä¸€è¡Œï¼ˆåŒ…å«head_sizeä¸ªå…ƒç´ ï¼‰å¹¶å°†å…¶å­˜å…¥å…±äº«å†…å­˜ã€‚(å› æ­¤ä¸€å…±éœ€è¦ `seqLen * numHeads` ä¸ª CUDA block)
- è¿™ä¸ªè¿‡ç¨‹æ˜¯é€šè¿‡ä¸€ä¸ªå¾ªç¯æ¥å®ç°çš„ï¼Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œ**æ¯ä¸ª thread group ä¼šè¯»å– 16 å­—èŠ‚çš„ Q æ•°æ®**ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœä½¿ç”¨ float16ï¼Œé‚£ä¹ˆå°±æ˜¯ 8 ä¸ªå…ƒç´ ï¼‰ã€‚
- ç”±äºä¸€ä¸ª WARP è¢«åˆ†ä¸º `BLOCK_SIZE` ä¸ª thread group, æ‰€ä»¥æ¯ä¸ª WARP ä¼šè¯»å– `16 * blk_size` å­—èŠ‚çš„Qæ•°æ®ï¼Œè¿™äº›æ•°æ®å¯¹åº”äºä¸€ä¸ª sequence çš„ä¸€ä¸ªheadï¼Œç”± CUDA GRID ç´¢å¼•æŒ‡å®šã€‚
- å½“å¾ªç¯è®¿é—®ç»“æŸåï¼Œå…±äº«å†…å­˜å­˜å‚¨ Q è¡Œçš„ä¸€éƒ¨åˆ†ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç»¿è‰²éƒ¨åˆ†è¡¨ç¤ºå­˜å‚¨åœ¨ä¸€ä¸ªçº¿ç¨‹è¯»å…¥å…±äº«å†…å­˜ä¸­çš„æ•°æ®ã€‚

![paged-attention-thread-group](/imgs/blogs/dive-into-paged-attention/paged-attention-thread-group.webp)

```cpp
// Load the query to registers.
// Each thread in a thread group has a different part of the query.
// For example, if the the thread group size is 4, then the first thread in
// the group has 0, 4, 8, ... th vectors of the query, and the second thread
// has 1, 5, 9, ... th vectors of the query, and so on. NOTE(woosuk): Because
// q is split from a qkv tensor, it may not be contiguous.
const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;
__shared__ Q_vec q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];
#pragma unroll
for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;
        i += NUM_THREAD_GROUPS) {
    const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
    q_vecs[thread_group_offset][i] =
        *reinterpret_cast<const Q_vec*>(q_ptr + vec_idx * VEC_SIZE);
}
__syncthreads();  // TODO(naed90): possible speedup if this is replaced with a
                    // memory wall right before we use q_vecs
```

å›¾ 1 ä¸­ä¸Šé¢éƒ¨åˆ† K çŸ©é˜µéƒ¨åˆ†æè¿°äº†ä»æ˜¾å­˜è¯»å– K Cache åˆ°å¯„å­˜å™¨çš„è¿‡ç¨‹ã€‚æ¯ä¸ªåºåˆ—çš„ K Cache åŒ…å« `cxt_length * num_kv_heads * head_size` ä¸ªå…ƒç´ ï¼Œ**ä½†ç”±äºé‡‡ç”¨äº†é¡µå¼å†…å­˜ç®¡ç†ï¼Œè¿™äº›å…ƒç´ åœ¨å†…å­˜ä¸­çš„å­˜å‚¨å¹¶ä¸è¿ç»­**ã€‚

æ¯ä¸ª CUDA thread block åªè´Ÿè´£è®¡ç®—ä¸€ä¸ª sequence ä¸€ä¸ª head çš„ \(QK^T\)ï¼Œå› æ­¤åªéœ€è¦`ctx_length * head_size` ä¸ª K Cache å…ƒç´ ã€‚ç„¶è€Œï¼Œç”±äº `ctx_length` ç»´åº¦çš„å­˜å‚¨æ˜¯ä¸è¿ç»­çš„ï¼Œå¹¶ä¸”ä»¥ `blk_size` ä¸ª token ä¸ºç²’åº¦åˆ†å¸ƒåœ¨ä¸åŒçš„å†…å­˜åœ°å€ï¼Œæˆ‘ä»¬éœ€è¦æ ¹æ® query çš„ `head_idx` å’Œ `seq_idx` è®¿é—® `block_table` ä»¥æ‰¾åˆ° K Cache çš„ `physical_block_num`ã€‚ä¸ºäº†æ–¹ä¾¿åç»­çš„æè¿°ï¼Œæˆ‘ä»¬å¯ä»¥å°† K Cache è§†ä¸º `(:, headSize)` çš„å½¢çŠ¶ï¼Œå…¶ä¸­ `head_size` ä¸ªå…ƒç´ ç»„æˆä¸€è¡Œã€‚

```cpp
// Iterate over the key blocks.
// Each warp fetches a block of keys for each iteration.
// Each thread group in a warp fetches a key from the block, and computes
// dot product with the query.
const int* block_table = block_tables + seq_idx * max_num_blocks_per_seq;
```

k_cache çš„å¸ƒå±€ä¸º `(num_blocks, num_kv_heads, head_size/x, block_size, x)`ï¼Œè¿™æ˜¯ä¸ºäº†ä¼˜åŒ–å†™å…¥ shared memory çš„æ“ä½œã€‚åœ¨ Q å’Œ K çŸ©é˜µçš„åŒä¸€è¡Œå…ƒç´ è¢«è¯»å…¥å¯„å­˜å™¨å¹¶è¿›è¡Œç‚¹ä¹˜è¿ç®—åï¼Œç»“æœéœ€è¦è¢«å­˜å…¥ shared memoryã€‚

```cpp
const int physical_block_offset =
    (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
K_vec k_vecs[NUM_VECS_PER_THREAD];
```

å¦‚æœä¸€ä¸ª WARP ä¸­æ‰€æœ‰çº¿ç¨‹éƒ½è®¡ç®— Qã€K åŒä¸€è¡Œæ•°æ®ï¼Œä¼šå¯¼è‡´å†™å…¥ shared memory çš„åŒä¸€ä¸ªä½ç½®ï¼Œè¿™å°†é€ æˆ WARP å†…ä¸åŒçº¿ç¨‹é¡ºåºåœ°å†™å…¥ã€‚å› æ­¤ï¼Œä¸ºäº†ä¼˜åŒ–ï¼ŒWARP çš„çº¿ç¨‹æœ€å¥½è®¡ç®— Q å’Œ K çš„ä¸åŒè¡Œæ•°æ®ã€‚å› æ­¤ï¼Œåœ¨è®¾è®¡ k_cache å¸ƒå±€æ—¶ï¼Œæˆ‘ä»¬å°† `block_size` æ”¾åœ¨æ¯” `head_size` æ›´ä½çš„ç»´åº¦ã€‚

ç”±äº `warp_size` å¤§äº `block_size`ï¼Œæˆ‘ä»¬éœ€è¦å°† `head_size` æ‹†åˆ†ä¸º `head_size/x` å’Œ `x` ä¸¤ä¸ªç»´åº¦ï¼Œå€Ÿ `x` åˆ°æœ€ä½ç»´åº¦ï¼Œä»¥ç¡®ä¿æ¯ä¸ªçº¿ç¨‹è¯»å…¥çš„æ•°æ®é‡å’Œè®¡ç®—é‡éƒ½è¶³å¤Ÿå¤§ã€‚æœ€åï¼Œæ¯ä¸ªçº¿ç¨‹ç»„æ´¾ä¸€ä¸ªçº¿ç¨‹å»å†™å…¥ shared memoryï¼Œè¿™æ ·ä¸€ä¸ª warp æœ‰ `block_size` ä¸ªçº¿ç¨‹å¹¶è¡Œå†™å…¥ shared memoryï¼Œä»è€Œå¢åŠ äº† shared memory çš„è®¿é—®å¸¦å®½ã€‚è¿™ç§è®¾è®¡ç­–ç•¥æ˜¯ä¸ºäº†å®ç°é«˜æ•ˆçš„å¹¶è¡Œè®¡ç®—å’Œå†…å­˜è®¿é—®ï¼Œä»¥æé«˜æ•´ä½“çš„è®¡ç®—æ€§èƒ½ã€‚

```cpp
if (thread_group_offset == 0) {
    // Store the partial reductions to shared memory.
    // NOTE(woosuk): It is required to zero out the masked logits.
    const bool mask = token_idx >= seq_len;
    logits[token_idx - start_token_idx] = mask ? 0.f : qk;
    // Update the max value.
    qk_max = mask ? qk_max : fmaxf(qk_max, qk);
}
```

åœ¨ä»£ç å®ç°ä¸­ï¼Œè®¿é—® K çŸ©é˜µéœ€è¦ä¸€ä¸ªå¾ªç¯ï¼Œè¯¥å¾ªç¯ä½¿å¾— CUDA çº¿ç¨‹å—ä¸­çš„æ‰€æœ‰ WARP ä¾æ¬¡è®¿é—® `num_block` ä¸ªé¡µé¢ã€‚åœ¨æ¯æ¬¡å¾ªç¯è¿­ä»£ä¸­ï¼Œæ¯ä¸ª WARP è´Ÿè´£è®¿é—®è¿ç»­çš„ `block_size` ä¸ª K Cache è¡Œï¼Œè¿™æ¶‰åŠåˆ°çš„æ•°æ®é‡ä¸º `blk_size * head_size` ä¸ªå…ƒç´ ã€‚åŒæ—¶ï¼Œæ¯ä¸ª thread group è´Ÿè´£è®¿é—® K Cache çš„ä¸€è¡Œï¼Œå°† `head_size` ä¸ªå…ƒç´ åŠ è½½åˆ°è‡ªå·±çš„å¯„å­˜å™¨ä¸­ã€‚æ¥ç€ï¼Œå¯„å­˜å™¨ä¸­çš„Qå’ŒKæ•°æ®å…ƒç´ ç«‹å³è¿›è¡Œç‚¹ä¹˜è¿ç®—ï¼Œè¿ç®—ç»“æœè¢«å†™å…¥ shared memory ä¸­ã€‚å› æ­¤ï¼Œçº¿ç¨‹å—çš„ shared memory å­˜å‚¨äº†ä¸€è¡Œ \(QK^T\) çš„ç»“æœï¼ŒåŒ…å« `ctx_length` ä¸ªå…ƒç´ ã€‚è¿™ç§å®ç°æ–¹å¼å……åˆ†åˆ©ç”¨äº† CUDA çš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›ï¼Œä»¥æé«˜æ•°æ®å¤„ç†çš„æ•ˆç‡ã€‚

ç„¶åï¼Œthread block å¯¹ shared memory ä¸­å…ƒç´ è¿›è¡Œ maxï¼Œsum æ–¹å¼ reductionï¼Œç„¶åè®¡ç®—å¾—åˆ° softmax ç»“æœã€‚

```cpp
// Get the sum of the exp values.
float exp_sum = 0.f;
for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
    float val = __expf(logits[i] - qk_max);
    logits[i] = val;
    exp_sum += val;
}
exp_sum = block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum);

// Compute softmax.
const float inv_sum = __fdividef(1.f, exp_sum + 1e-6f);
for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
    logits[i] *= inv_sum;
}
__syncthreads();
```
  
å›¾ 1 å³è¾¹ V çŸ©é˜µéƒ¨åˆ†æè¿°ä»æ˜¾å­˜è¯» V Cache åˆ°å¯„å­˜å™¨è¿‡ç¨‹ã€‚å’Œ K Cache ä¸€æ ·ï¼ŒCUDA thread block ä¾æ¬¡è®¿é—® `num_blk` ä¸ªç‰©ç†å—åˆ°å¯„å­˜å™¨ï¼Œæ¯ä¸ª warp è´Ÿè´£ `blk_size` ä¸ª token çš„ page å†…å­˜ï¼Œpage çš„çœŸå®ç‰©ç†åœ°å€åŒæ ·éœ€è¦è¿›è¡Œç´¢å¼•ã€‚ä¸è¿‡è¿™é‡Œä¸éœ€è¦ä»¥ thread group ä¸ºå•ä½è®¿é—® 16 å­—èŠ‚ï¼Œè€Œæ˜¯æ¯ä¸ª thread è®¿é—® 16 å­—èŠ‚çš„å…ƒç´ ã€‚è®¿é—®å®Œå°±å¯ä»¥ä¸ shared memory çš„ \(softmax(QK^T)\) ä¸­é—´ç»“æœå¯¹åº”ä½ç½® 16 å­—èŠ‚çš„æ•°æ®è¿›è¡Œç‚¹ä¹˜ï¼Œå¾—åˆ°ä¸€ä¸ª float ç»“æœï¼Œå†™åˆ° output å¯¹åº”ä½ç½®ä¸­ã€‚  
  
ä¸ºä»€ä¹ˆ V Cache çš„ layout æ˜¯ `[num_blocks, num_kv_heads, head_size, block_size]`ï¼Œå’Œ K Cache layout ä¸ä¸€æ ·ï¼Ÿ è¿™æ˜¯å› ä¸º V è¦å»åšç‚¹ä¹˜çš„å¯¹è±¡åœ¨ shared memoryï¼Œåªéœ€è¦è¯»ï¼Œä¸æ¶‰åŠå¹¶è¡Œå†™çš„é—®é¢˜ã€‚

å’Œ FlashAttentionï¼ˆFAï¼‰æœ‰ä»€ä¹ˆä¸åŒï¼Ÿç»“åˆæˆ‘çš„å›¾å’Œä¸­é—´ FAv2 çš„æµç¨‹å›¾å¯¹æ¯”å°±ä¸€ç›®äº†ç„¶äº†ã€‚FA ç”¨äº†ä¸¤å±‚å¾ªç¯ï¼Œæ¯æ¬¡å†™ä¸€ä¸ª Tile çš„ output tensorï¼Œè€Œ PA ä¸€ç›´åªæœ‰ä¸€å±‚å¾ªç¯ï¼Œæ¯æ¬¡å†™ä¸€è¡Œ output tensorã€‚å› ä¸ºæ¯æ¬¡éƒ½æœ‰æ•´è¡Œçš„ \(QK^T\) ä¸­é—´ç»“æœï¼Œä¸éœ€è¦ online softmax è¿™ç§èŠ±å“¨æŠ€å·§ã€‚

![flash-attention.webp](/imgs/blogs/dive-into-paged-attention/flash-attention.webp)

### 5.3. Paged Attention Kernel ä»£ç ç²¾è¯»

```cpp
// Grid: (num_heads, num_seqs, 1).
template<
typename scalar_t,
int HEAD_SIZE,
int BLOCK_SIZE,
int NUM_THREADS,
int PARTITION_SIZE = 0>
__device__ void paged_attention_kernel(
... // Other side args.
const scalar_t* __restrict__ out,       // [num_seqs, num_heads, max_num_partitions, head_size]
const scalar_t* __restrict__ q,         // [num_seqs, num_heads, head_size]
const scalar_t* __restrict__ k_cache,   // [num_blocks, num_kv_heads, head_size/x, block_size, x]
const scalar_t* __restrict__ v_cache,   // [num_blocks, num_kv_heads, head_size, block_size]
... // Other side args.
)
```

#### 5.2.1. æ¨¡æ¿å‚æ•°è¯´æ˜

- `scalar_t` å…ƒç´ ç±»å‹ (å®é™…ä»£ç ä¸­è¿˜æœ‰ `cache_t` è¡¨ç¤º KV cache çš„å…ƒç´ ç±»å‹).
- `HEAD_SIZE` æ¯ä¸ª head ä¸­å…ƒç´ æ•°é‡.
- `BLOCK_SIZE` æ¯ä¸ª PA block ä¸­çš„ token æ•°é‡.
>  KV cache è¢«å­˜å‚¨åœ¨ä¸åŒ Paged Attention blocks. æ¯ä¸ª PA block å­˜å‚¨ä¸€ä¸ª head ä¸­ `BLOCK_SIZE` ä¸ª token.   
>  ä¾‹å¦‚, è‹¥ `BLOCK_SIZE=16`, `HEAD_SIZE=128`, åˆ™ä¸€ä¸ª  PA block èƒ½å­˜å‚¨ä¸€ä¸ª head çš„ `16 * 128 = 2048` ä¸ªå…ƒç´ . (è¿™ä¸ªä¾‹å­æš‚æ—¶æ²¡çœ‹æ‡‚, token å’Œ `HEAD_SIZE` æœ‰å•¥å…³ç³»?)  
>  æ¯ä¸ª PA block å¯èƒ½åªåŒ…å«ä¸€éƒ¨åˆ†çš„ context tokens.  
>  ä» page è§’åº¦çœ‹, KV cache æ˜¯è‹¥å¹²ä¸ª page çš„é›†åˆ; 
- `NUM_THREADS` æ¯ä¸ª CUDA thread block ä¸­ thread çš„æ•°é‡.
- `PARTITION_SIZE` å‚ä¸ TP çš„ GPU æ•°é‡, é»˜è®¤ 0 è¡¨ç¤ºå•å¡. (ä»¥ä¸‹éƒ½ä»¥å•å¡ä¸ºä¾‹è¯´æ˜)

#### 5.2.2. é¢å¤–çš„ shape å‚æ•°è¯´æ˜

- `num_seqs`: æœ¬æ¬¡æ¨ç†è¯·æ±‚ sequence æ•°ç›®.
 > ç”±äºè¿™ä¸ª kernel åªå¤„ç† decode é˜¶æ®µå• query attention, æ‰€ä»¥å®é™…ä¸Šæ¯ä¸ª sequence åªæœ‰ä¸€ä¸ª query token. å› æ­¤ `num_seqs` ç­‰äºå½“å‰ batch ä¸­æ€»å…±æ­£åœ¨å¤„ç†çš„ token æ•°.
- `num_heads`: Q çš„ head æ•°ç›®
- `num_kv_heads`: KV çš„ head æ•°ç›®, å¯¹äº MHA å…¶å€¼å’Œ `num_heads` ç›¸åŒ; å¦‚æœæ˜¯ GQA, MQA åˆ™ `num_kv_heads` å°äº `num_head`.
- `head_size`: å³ `HEAD_SIZE`
- `k_cache: (num_blocks, num_kv_heads, head_size/x, block_size, x)`, å…¶ä¸­ `x` è¡¨ç¤ºä¸€ä¸ª **Vec** çš„å¤§å° (å³: `VEC_SIZE`)ï¼Œå¦‚ `float16 -> 16 / sizeof(float16) = 8`ã€‚
> **Vec**: The vec is a list of elements that are fetched and calculated together. For query and key data, the vec size (`VEC_SIZE`) is determined so that each thread group can fetch and calculate 16 bytes of data at a time. For value data, the vec size (`V_VEC_SIZE`) is determined so that each thread can fetch and calculate 16 bytes of data at a time. For example, if the `scalar_t` is FP16 (2 bytes) and `THREAD_GROUP_SIZE` is 2, the `VEC_SIZE` will be 4, while the `V_VEC_SIZE` will be 8.  

```cpp
constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(scalar_t)), 1);
using K_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
using Q_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
```

Paged å†…å­˜ç®¡ç†ç›¸å…³çš„è¾…åŠ©æ•°æ®ç»“æ„ï¼š
- `blk_size`ï¼šä¹Ÿå°±æ˜¯ `BLOCK_SIZE`ï¼Œæ˜¯ KVCache page çš„æœ€é«˜ç»´ï¼ŒKVCache æ˜¯è‹¥å¹²ä¸ª page çš„é›†åˆï¼Œæ¯ä¸ª page å­˜ `(blk_size, num_headï¼Œhead_size)` ä¸ª Kã€V çš„å…ƒç´ ã€‚
- `head_mapping`: `[num_heads]` ç”¨äº MQA, GQAï¼Œç¡®å®šç”¨çš„ KV_head
- `block_tables`: `[num_seqs, max_num_blocks_per_seq]`, æ˜ å°„è¡¨ï¼Œè¡¨ç¤ºæ¯ä¸ª sequence æ˜ å°„åˆ°å“ªå‡ ä¸ª block ä¸Š
- `context_lens`: `[num_seqs]`, ç”¨äºå˜é•¿


- **Sequence**: client çš„è¯·æ±‚. ç”±äº PA kernel åªåœ¨ decode é˜¶æ®µè¢«è°ƒç”¨, æ‰€ä»¥ä¸€æ¡è¯·æ±‚å¯¹åº”çš„è¾“å…¥ Q åªæœ‰ä¸€ä¸ª token. å¤šä¸ª sequence çš„è¾“å…¥ç»„æˆä¸€ä¸ª batch, æ‰€ä»¥è¾“å…¥ Q çš„ shape è¡¨ç¤ºä¸º: `[num_seqs, num_heads, head_size]`, å…¶ä¸­ `[num_heas, head_size]` å¯¹åº”ä¸€ä¸ª  token.
- **Vec**: è¢«çº¿ç¨‹ fectch å’Œ calculate çš„ä¸€ä¸² elements. 
- **CUDA Thread Block**
    - ğŸ¤”Processes the calculation between one query token and key tokens of a whole context.
    - CUDA grid shape is `(num_heads, num_seqs, max_num_partitions)`. Therefore, each thread block only handles the calculation for one head, one sequence, and one partition.
- **Thread Group** 
    - Consists of `THREAD_GROUP_SIZE` threads
    - Fetches one query token data (`HEAD_SIZE` elements), while each thread itself only handles a part of one query token data (`NUM_ELEMENTS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE` elements). 
    - Within each warp, every thread group will fetch the same query token data, but will multiply it with different key token data.
    - Fetches and calculates `VEC_SIZE = 16 / THREAD_GROUP_SIZE / SIZE_OF_ELEM` elements at a time for Q and K. -> Each thread in a thread group fetches and calculates `VEC_SIZE` elements at a time.


    
